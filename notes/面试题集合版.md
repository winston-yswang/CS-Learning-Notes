# 计算机基础

## 操作系统

### 常见问题

#### **问：什么是系统调用呢？**

根据进程访问资源的特点，我们可以把进程在系统上的运行分为两个级别：

1. 用户态(user mode) : 用户态运行的进程可以直接读取用户程序的数据。
2. 系统态(kernel mode):可以简单的理解系统态运行的进程或程序几乎可以访问计算机的任何资源，不受限制。

在我们运行的用户程序中，凡是与系统态级别的资源有关的操作（如文件管理、进程控制、内存管理等)，都必须通过系统调用方式向操作系统提出服务请求，并由操作系统代为完成。



#### 问：进程和线程的区别

进程：进程是程序的一次执行过程，是一个动态概念，是程序在执行过程中分配和管理资源的基本单位，每一个进程都有一个自己的地址空间。

线程：线程是CPU调度和分派的基本单位，它可与同属一个进程的其他的线程共享进程所拥有的全部资源。

联系：线程是进程的一部分，一个线程只能属于一个进程，而一个进程可以有多个线程，但至少有一个线程。

为什么引入线程概念：由于创建或撤销进程时，系统都要为之分配或回收资源，如内存空间、I/O 设备等，需要较大的时空开销，限制了并发程度的进一步提高。为减少进程切换的开销，把进程作为资源分配单位和调度单位这两个属性分开处理，即进程还是作为资源分配的基本单位，但是不作为调度的基本单位（很少调度或切换），把调度执行与切换的责任交给线程，即线程成为独立调度的基本单位，它比进程更容易（更快）创建，也更容易撤销。

参考文章：[进程和线程的区别，你该如何回答？](https://juejin.cn/post/6844904079550857223)



#### 问：进程切换

从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化：

1. 保存处理机上下文，包括程序计数器和其他寄存器。
2. 更新PCB信息。
3. 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。
4. 选择另一个进程执行，并更新其PCB。
5. 更新内存管理的数据结构。
6. 恢复处理机上下文。



#### **问：说说进程有哪几种状态?**

- 创建状态(new) ：进程正在被创建，尚未到就绪状态。
- 就绪状态(ready) ：进程已处于准备运行状态，即进程获得了除了处理器之外的一切所需资源，一旦得到处理器资源(处理器分配的时间片)即可运行。
- 运行状态(running) ：进程正在处理器上上运行(单核 CPU 下任意时刻只有一个进程处于运行状态)。
- 阻塞状态(waiting) ：又称为等待状态，进程正在等待某一事件而暂停运行如等待某资源为可用或等待 IO 操作完成。即使处理器空闲，该进程也不能运行。
- 结束状态(terminated) ：进程正在从系统中消失。可能是进程正常结束或其他原因中断退出运行。



#### **问：进程间的通信常见的的有哪几种方式呢?**

1. 管道/匿名管道(Pipes) ：用于具有亲缘关系的父子进程间或者兄弟进程之间的通信。
2. 有名管道(Names Pipes) : 匿名管道由于没有名字，只能用于亲缘关系的进程间通信。为了克服这个缺点，提出了有名管道。有名管道严格遵循**先进先出(first in first out)**。有名管道以磁盘文件的方式存在，可以实现本机任意两个进程通信。
3. 信号(Signal) ：信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生；
4. 消息队列(Message Queuing) ：消息队列是消息的链表,具有特定的格式,存放在内存中并由消息队列标识符标识。管道和消息队列的通信数据都是先进先出的原则。与管道（无名管道：只存在于内存中的文件；命名管道：存在于实际的磁盘介质或者文件系统）不同的是消息队列存放在内核中，只有在内核重启(即，操作系统重启)或者显式地删除一个消息队列时，该消息队列才会被真正的删除。消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取.比 FIFO 更有优势。消息队列克服了信号承载信息量少，管道只能承载无格式字 节流以及缓冲区大小受限等缺点。
5. 信号量(Semaphores) ：信号量是一个计数器，用于多进程对共享数据的访问，信号量的意图在于进程间同步。这种通信方式主要用于解决与同步相关的问题并避免竞争条件。
6. 共享内存(Shared memory) ：使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据的更新。这种方式需要依靠某种同步操作，如互斥锁和信号量等。可以说这是最有用的进程间通信方式。
7. 套接字(Sockets) : 此方法主要用于在客户端和服务器之间通过网络进行通信。套接字是支持 TCP/IP 的网络通信的基本操作单元，可以看做是不同主机之间的进程进行双向通信的端点，简单的说就是通信的两方的一种约定，用套接字中的相关函数来完成通信过程。



#### **问：那线程间的同步的方式有哪些呢?**

答：线程同步是两个或多个共享关键资源的线程的并发执行。应该同步线程以避免关键的资源使用冲突。操作系统一般有下面三种线程同步的方式：

1. 互斥量(Mutex)：采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问。比如 Java 中的 synchronized 关键词和各种 Lock 都是这种机制。
2. 信号量(Semphares**)** ：它允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量。
3. 事件(Event) :Wait/Notify：通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操作。



#### **问：操作系统中进程的调度算法有哪些?**

- 先到先服务(FCFS)调度算法 : 从就绪队列中选择一个最先进入该队列的进程为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度。
- 短作业优先(SJF)的调度算法 : 从就绪队列中选出一个估计运行时间最短的进程为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度。
- 时间片轮转调度算法 : 时间片轮转调度是一种最古老，最简单，最公平且使用最广的算法，又称 RR(Round robin)调度。每个进程被分配一个时间段，称作它的时间片，即该进程允许运行的时间。
- 高响应比优先的调度算法：这种算法是在短作业优先调度算法的基础上，加上一个随着时间累计而叠加的权重机制。响应比=(等待时间 + 要求服务时间) / 要求服务时间，这种算法既可以优先完成短作业，又能确保长作业不至于长期饥饿，是一个折中的算法。
- 多级反馈队列调度算法 ：前面介绍的几种进程调度的算法都有一定的局限性。如短进程优先的调度算法，仅照顾了短进程而忽略了长进程 。多级反馈队列调度算法既能使高优先级的作业得到响应又能使短作业（进程）迅速完成。，因而它是目前被公认的一种较好的进程调度算法，UNIX 操作系统采取的便是这种调度算法。
- 优先级调度 ： 为每个流程分配优先级，首先执行具有最高优先级的进程，依此类推。具有相同优先级的进程以 FCFS 方式执行。可以根据内存要求，时间要求或任何其他资源要求来确定优先级。



#### **问：何为死锁，产生条件，预防措施**

答：死锁是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象。

系统中以下四个条件同时成立，那么就能引起死锁：互斥、不可剥夺、占有且等待和循环等待。

死锁的预防方法：构成死锁的四个条件只要破坏其中一个就构不成死锁，死锁一旦形成就无法消除，因此最好的方法就是避免产生死锁。

1. 破坏互斥条件，让资源能够共享，但缺点是不通过，因为有些资源不能共享，如打印机。
2. 破坏请求并保持条件，采用预先分配的方法，在进行运行前一次性申请好它所需要的所有资源，但缺点是浪费资源。
3. 破坏不可剥夺的条件，对已经占用资源的线程发送取消请求，但是实现比较复杂，而且还有破坏业务逻辑。
4. 破坏循环等待条件，为每个资源进行编号，采用顺序的的资源分配方法，规定每个线程必须按照递增的顺序请求资源，缺点是编号必须相对稳定，增加新资源时会比较麻烦，而且有些特殊的业务逻辑不能按完全按照指定的顺序分配资源。

避免产生死锁的算法：银行家算法

核心思想：分配资源之前，判断系统是否安全，如果安全才会进行资源分配。即每当进程提出资源请求且系统的资源能够满足该请求时，系统将判断满足此次资源请求后系统状态是否安全，如果判断结果为安全，则给该进程分配资源，否则不分配资源，申请资源的进程将阻塞。



#### **问：操作系统的内存管理机制了解吗？内存管理有哪几种方式?**

答：简单分为连续分配管理方式和非连续分配管理方式这两种。连续分配管理方式是指为一个用户程序分配一个连续的内存空间，常见的如 块式管理 。同样地，非连续分配管理方式允许一个程序使用的内存分布在离散或者说不相邻的内存中，常见的如页式管理 和 段式管理。

（1）连续分配管理方式

单一连续分配：内存中只能有一道用户程序，用户程序独占整个用户区空间；

固定分区分配：将整个用户空间划分为若干个固定大小的分区，在每个分区中只装入一道作业；

动态分区分配：使用特殊的数据结构记录内存的使用情况，根据进程的大小动态地建立分区。

存在的问题：由于其要求把作业（进程）放在内存的一片连续区域中，很容易出现大段的连续内存空间因为不足够容纳作业或进程而不可用。

（2）非连续分配管理方式

基本分页管理：将内存空间分为一个个大小相等的分区，每个分区就称为一个 “页框（page frame）”。每个页框有一个编号，即“页框号”（也成为物理页框号、内存块号），页框号从 0 开 始 。

进程的页面与内存的页框有一一对应的关系。 各个页面不必连续存放，可以放到不相邻（离散）的各个页框中。

一个进程对应一张页表，进程的每个页面对应一个页表项，每个页表项由页号和块号（页框号）组成，记录着进程页面和实际存放的内存块之间的映射关系。

在任何分页式系统中，都不可避免地要考虑下面这两个问题：

问题 1：如何保证虚拟地址到物理地址的转换足够快 — 使用快表解决

问题 2：如何解决虚拟地址空间大，页表也会很大的问题（页表项多了，页表自然也就大了）— 使用多级页表解决

基本分段管理：段式系统是按照用户作业（进程）中的自然段来划分逻辑空间的。段与段之间可以不连续存储，但是段的内部仍然是连续的。和基本分页管理一样，基本分段管理也需要一个数据结构来记录虚拟地址和物理地址之间的映射，这个数据结构就是段表。

基本段页管理：对虚拟地址空间先进行段的划分，然后在每一段内再进行页的划分。



#### **问：介绍一下快表**

答：==为了解决虚拟地址到物理地址的转换速度，操作系统在 **页表方案** 基础之上引入了 **快表** 来加速虚拟地址到物理地址的转换==。我们可以把快表理解为一种特殊的高速缓冲存储器（Cache），其中的内容是页表的一部分或者全部内容。作为页表的 Cache，它的作用与页表相似，但是提高了访问速率。由于采用页表做地址转换，读写内存数据时 CPU 要访问两次主存。有了快表，有时只要访问一次高速缓冲存储器，一次主存，这样可加速查找并提高指令执行速度。

使用快表之后的地址转换流程是这样的：

1. 根据虚拟地址中的页号查快表；
2. 如果该页在快表中，直接从快表中读取相应的物理地址；
3. 如果该页不在快表中，就访问内存中的页表，再从页表中得到物理地址，同时将页表中的该映射表项添加到快表中；
4. 当快表填满后，又要登记新页时，就按照一定的淘汰策略淘汰掉快表中的一个页。



#### **问：分页机制和分段机制有哪些共同点和区别呢？**

答：共同点：

- 分页机制和分段机制都是为了提高内存利用率，减少内存碎片。
- 页内和段内的内存是连续的，页与页和段与段之间是离散的。

区别：

- 页的大小是固定的，由操作系统决定；而段的大小不固定，取决于我们当前运行的程序。
- 分页仅仅是为了满足操作系统内存管理的需求，而段是逻辑信息的单位，在程序中可以体现为代码段，数据段，能够更好满足用户的需要。



**问：CPU 寻址了解吗?为什么需要虚拟地址空间?**

答：现代处理器使用的是一种称为 虚拟寻址(Virtual Addressing) 的寻址方式。使用虚拟寻址，CPU 需要将虚拟地址翻译成物理地址，这样才能访问到真实的物理内存。 实际上完成虚拟地址转换为物理地址转换的硬件是 CPU 中含有一个被称为 内存管理单元（Memory Management Unit, MMU） 的硬件。



#### **问：什么是虚拟内存(Virtual Memory)?**

答：虚拟内存是计算机系统==内存管理==的一种技术。通过 虚拟内存 ==可以让程序可以拥有超过系统物理内存大小的可用内存空间==。另外，虚拟内存为每个进程提供了一个一致的、==私有的地址空间==，它让每个进程产生了一种自己在独享主存的错觉（每个进程拥有一片连续完整的内存空间）。这样会更加有效地管理内存并减少出错。



#### **问：局部性原理**

答：局部性原理是虚拟内存技术的基础，正是因为程序运行具有局部性原理，才可以只装入部分程序到内存就开始运行。

局部性原理表现在以下两个方面：

1. 时间局部性 ：如果程序中的某条指令一旦执行，不久以后该指令可能再次执行；如果某数据被访问过，不久以后该数据可能再次被访问。产生时间局部性的典型原因，是由于在程序中存在着大量的循环操作。
2. 空间局部性 ：一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也将被访问，即程序在一段时间内所访问的地址，可能集中在一定的范围之内，这是因为指令通常是顺序存放、顺序执行的，数据也一般是以向量、数组、表等形式簇聚存储的。

时间局部性是通过将近来使用的指令和数据保存到高速缓存存储器中，并使用高速缓存的层次结构实现。空间局部性通常是使用较大的高速缓存，并将预取机制集成到高速缓存控制逻辑中实现。虚拟内存技术实际上就是建立了 “内存一外存”的两级存储器的结构，利用局部性原理实现髙速缓存。



#### **问：什么是虚拟存储器？**

答：基于局部性原理，在程序装入时，可以将程序的一部分装入内存，而将其他部分留在外存，就可以启动程序执行。由于外存往往比内存大很多，所以我们运行的软件的内存大小实际上是可以比计算机系统实际的内存大小大的。在程序执行过程中，当所访问的信息不在内存时，由操作系统将所需要的部分调入内存，然后继续执行程序。另一方面，操作系统将内存中暂时不使用的内容换到外存上，从而腾出空间存放将要调入内存的信息。这样，计算机好像为用户提供了一个比实际内存大的多的存储器——虚拟存储器。

实际上，我觉得虚拟内存同样是一种时间换空间的策略，你用 CPU 的计算时间，页的调入调出花费的时间，换来了一个虚拟的更大的空间来支持程序的运行。



#### **问：虚拟内存技术的实现呢？**

答：虚拟内存的实现需要建立在离散分配的内存管理方式的基础上。 虚拟内存的实现有以下三种方式：

1. 请求分页存储管理 ：建立在分页管理之上，为了支持虚拟存储器功能而增加了请求调页功能和页面置换功能。请求分页是目前最常用的一种实现虚拟存储器的方法。请求分页存储管理系统中，在作业开始运行之前，仅装入当前要执行的部分段即可运行。假如在作业运行的过程中发现要访问的页面不在内存，则由处理器通知操作系统按照对应的页面置换算法将相应的页面调入到主存，同时操作系统也可以将暂时不用的页面置换到外存中。
2. 请求分段存储管理 ：建立在分段存储管理之上，增加了请求调段功能、分段置换功能。请求分段储存管理方式就如同请求分页储存管理方式一样，在作业开始运行之前，仅装入当前要执行的部分段即可运行；在执行过程中，可使用请求调入中断动态装入要访问但又不在内存的程序段；当内存空间已满，而又需要装入新的段时，根据置换功能适当调出某个段，以便腾出空间而装入新的段。
3. 请求段页式存储管理

请求分页与分页存储管理的不同：

请求分页存储管理建立在分页管理之上。他们的根本区别是是否将程序全部所需的全部地址空间都装入主存，这也是请求分页存储管理可以提供虚拟内存的原因，我们在上面已经分析过了。

它们之间的根本区别在于是否将一作业的全部地址空间同时装入主存。请求分页存储管理不要求将作业全部地址空间同时装入主存。基于这一点，请求分页存储管理可以提供虚存，而分页存储管理却不能提供虚存。

不管是上面那种实现方式，我们一般都需要：

1. 一定容量的内存和外存：在载入程序的时候，只需要将程序的一部分装入内存，而将其他部分留在外存，然后程序就可以执行了；
2. 缺页中断：如果需执行的指令或访问的数据尚未在内存（称为缺页或缺段），则由处理器通知操作系统将相应的页面或段调入到内存，然后继续执行程序；
3. 虚拟地址空间：逻辑地址到物理地址的变换。



#### **问：页面置换算法的作用?常见的页面置换算法有哪些?**

答：地址映射过程中，若在页面中发现所要访问的页面不在内存中，则发生缺页中断 。

当发生缺页中断时，如果当前内存中并没有空闲的页面，操作系统就必须在内存选择一个页面将其移出内存，以便为即将调入的页面让出空间。用来选择淘汰哪一页的规则叫做页面置换算法，我们可以把页面置换算法看成是淘汰页面的规则。

- **OPT 页面置换算法（最佳页面置换算法）** ：最佳(Optimal, OPT)置换算法所选择的被淘汰页面将是以后永不使用的，或者是在最长时间内不再被访问的页面,这样可以保证获得最低的缺页率。但由于人们目前无法预知进程在内存下的若千页面中哪个是未来最长时间内不再被访问的，因而该算法无法实现。一般作为衡量其他置换算法的方法。
- **FIFO（First In First Out） 页面置换算法（先进先出页面置换算法）** : 总是淘汰最先进入内存的页面，即选择在内存中驻留时间最久的页面进行淘汰。
- **LRU （Least Recently Used）页面置换算法（最近最久未使用页面置换算法）** ：LRU算法赋予每个页面一个访问字段，用来记录一个页面自上次被访问以来所经历的时间 T，当须淘汰一个页面时，选择现有页面中其 T 值最大的，即最近最久未使用的页面予以淘汰。
- **LFU （Least Frequently Used）页面置换算法（最少使用页面置换算法）** : 该置换算法选择在之前时期使用最少的页面作为淘汰页。



#### **问：IO 复用模型了解哪些，select/poll/epoll 分别讲讲**

对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，它会经历两个阶段：

1. 等待数据准备 (Waiting for the data to be ready)
2. 将数据从内核拷贝到进程中 (Copying the data from the kernel to the process)

正式因为这两个阶段，linux系统产生了下面五种网络模式的方案。

-  阻塞 I/O（blocking IO）
- 非阻塞 I/O（nonblocking IO）
- I/O 多路复用（ IO multiplexing）
- 信号驱动 I/O（ signal driven IO）
- 异步 I/O（asynchronous IO）

IO多路复用其实就是指一个线程可以监视多个网络IO连接，它的基本原理就是系统轮询所服务的所有socket，当某个socket有数据到达了，就通知用户进程。具体实现有select、poll、epoll。

就是select，poll，epoll这些系统调用会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。

（1）select(…)工作原理？

当操作系统的select函数被调用以后，首先会按照fd集合，去检查内存中的socket套接字状态，这个复杂度是O(N)的。然后检查完一遍之后，发现有就绪状态的socket那么直接返回，不会阻塞当前调用线程，否则就说明当前指定fd集合对应的socket没有就绪状态的。

那么就需要阻塞当前调用线程了，直到有某个socket有数据之后，才会唤醒线程。

（2）监听socket数量有没有限制？

它默认最大可以监听1024个socket。因为fd_set这个结构它是一个bitmap位图结构，默认长度是1024bit。之所以是1024bit，是考虑系统调用涉及到参数的数据拷贝， 如果bitmap数据太庞大，也不利于系统调用速度。

（3）poll(…) 和select(…)主要区别是什么？

select缺点：

1. 1024 bitmap
2. fd_set不可重用
3. 用户态<->内核态 开销
4. O(n)再次遍历

poll和select最大的区别是两者的传参不同，poll使用的是链表结构，解决了socket监听数量的问题。

（4）epoll产生的背景是什么呢

1. select和poll这两系统函数每次都是需要我们提供它所有需要监听的socket文件描述符集合，而且咱们的程序主线程都是死循环调用 select/poll函数的，这里涉及到用户空间数据到内核克空间拷贝的过程，这个相对来讲，还是比较消耗新性能的。
2. select和poll函数它的返回值是个int整形值，只能代表有几个socket就绪或者是有错误了，它没办法表示具体是哪个socket就绪了。这就导致程序唤醒以后，还需要新的一轮系统调用去检查哪个socket是就绪状态的，然后再进行socket数据处理逻辑，在这已经走了不少弯路了。

（5）epoll函数的工作原理是什么？

面试者：解决上面说的问题，就需要epoll函数再内核空间内，创建一个对应的数据结构去存储一些数据，这个数据结构其实就是epoll_event对象，它是通过系统函数epoll_create()去创建，就会得到epfd文件号，相当于我们再内核开辟了一小块空间，并且我们也知道这块空间的位置。

先说一下 epoll_event的结构，它主要是两块重要的区域，其中一块是存放需要监听的socket_fd描述符列表，另一块就是就绪列表，存放就绪状态的socket信息。

还另外两个重要的函数epoll_ctl()和epoll_wait()。
epoll_ctl它可以根据epfd号去增删改内核空间上的eventpoll对象列表；epoll_wait()它主要的参数就是epfd，表示此次系统调用需要监测的socket_fd集合，是eventpoll中已经指定好的那些socket信息。epoll_wait函数默认情况下会阻塞调用线程，直到某个socket就绪以后epoll_wait才会返回。
（6）eventpoll对象的就绪列表数据是如何维护的呢？

前面已经说了socket对象，它有三块区域嘛，读缓冲区、写缓冲区和等待队列。epoll跟select调用流程非常相似，当我们调用epoll_ctl添加一个需要关注的socket，其实内核程序就会把当前eventpoll对象追加到这个socket#等待队列里，当socket对应的客户端发送完数据，写入内存之后触发中断程序，最后检查这个socket的等待队列，发现这个socket#不是进程，是一个eventpoll对象引用，它会根据这个引用讲当前socket引用追加到eventpoll的就绪链表的末尾。

还有一个是eventpoll 有一块空间是eventpoll#等待队列，这个等待队列它保存的就是调用了epoll_wait的进程了。
（7）epollevent对象中存放需要检查的socket信息是采用的什么数据结构？为什么？

调用epoll_wait函数的时候会传入一个epoll_event事件数组指针，epoll_wait函数正常返回之前，会把就绪的socket事件信息拷贝到这个数组里，返回到上层程序，这样就可以通过这个数组拿到就绪列表了。

epoll_wait默认是阻塞的，也可以设置成非阻塞。

存放的集合信息是采用 红黑树数据结构。


参考文章：[Linux IO模式及 select、poll、epoll详解](https://juejin.cn/post/6844903488170786824) 、[看这一篇IO多路复用面试专题就够了！最全面最详细的解答！](https://blog.csdn.net/Oooo_mumuxi/article/details/108164013)



#### 问：epoll的优势

1、支持一个进程的打开socket描述符不受限制

2、I/O效率不会随着FD数目的增加而线性下降

3、使用mmap加速内核与用户空间的消息传递

4、epoll的API更加简单







## 计算机网络

#### **问：TCP/IP 各层的作用**

**应用层：**通过应用进程间的交互来完成特定功能的网络应用。对于不同的网络应用需要不同的应用层协议。

**表示层：**提供各种用于应用层数据的编码和转换功能

**会话层**：负责建立、管理和终止表示层实体之间的通信会话

**运输层**：传输层建立了主机端到端的链接，提供可靠传输和端口复用。

**网络层**：在计算机网络中进行通信的两个计算机之间可能会经过很多个数据链路，也可能还要经过很多通信子网。网络层的任务就是选择合适的网间路由和交换结点， 确保数据及时传送。

**数据链路层**：两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层的协议。 在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息（如同步信息，地址信息，差错控制等）。

**物理层**：实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异， 使其上面的数据链路层不必考虑网络的具体传输介质是什么

参考文章：[TCP/IP 各层的结构与功能](TCP/IP 各层的结构与功能)



#### **问：TCP为什么是三次握手？**

#### **问：TCP为什么是四次挥手？**

#### **问：TCP 初始序列号为什么是随机值？**

#### **问：为什么TCP4次挥手时等待为2MSL？**

1）要防止ACK包丢失，服务端重发FIN+ACK包，此时客户端要能处理该包，所以要有TIME-WAIT
2）要确保这一次连接中的所有包在网络中都已失效。这是因为如果没有等所有包失效，此时该端口又重新建立了连接，则上个连接的包会造成干扰。



#### **问：TCP 中有哪些计时器？**

#### **问：DNS工作过程？**

#### **问：TCP粘包、拆包及解决办法？**

#### **问：说一说 HTTP 请求的过程**

#### **问：HTTP 的缓存机制**

参考文章：[计算机网络面试题集锦](http://www.yswang.tech/archives/ji-suan-ji-wang-luo-mian-shi-ti-ji-jing#EzhGjjGb)



#### 问：HTTPS和HTTP的区别

HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全。

HTTPS和HTTP的区别主要如下：

1、https协议需要到ca申请证书，一般免费证书较少，因而需要一定费用。

2、http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议。

3、http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。

4、http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。





#### **问：UDP首部格式和TCP首部格式**

#### **问：TCP三次握手过程**

<img src="http://www.yswang.tech/upload/2022/01/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B-0db876c4fad441898b4579cad32c816e.png" alt="TCP三次握手.png" style="zoom:50%;" />

**第一次握手**

客户端向服务端发送连接请求报文段。该报文段中包含自身的数据通讯初始序号。请求发送后，客户端便进入 SYN-SENT 状态。

**第二次握手**

服务端收到连接请求报文段后，如果同意连接，则会发送一个应答，该应答中也会包含自身的数据通讯初始序号，发送完成后便进入 SYN-RECEIVED 状态（半连接状态）。

**第三次握手**

当客户端收到连接同意的应答后，还要向服务端发送一个确认报文。客户端发完这个报文段后便进入 ESTABLISHED 状态，服务端收到这个应答后也进入 ESTABLISHED 状态，此时连接建立成功。



#### **问：TCP四次挥手过程**

**第一次挥手**

若客户端 A 认为数据发送完成，则它需要向服务端 B 发送**连接释放**请求。

**第二次挥手**

B 收到连接释放请求后，会告诉应用层要释放 TCP 链接。然后会发送 ACK 包，并进入 CLOSE_WAIT 状态，此时表明 A 到 B 的连接已经释放，不再接收 A 发的数据了。但是因为 **TCP 连接是双向的，所以 B 仍旧可以发送数据给 A**。

**第三次挥手**

B 如果此时还有没发完的数据会继续发送，完毕后会向 A 发送连接释放请求，然后 B 便进入 LAST-ACK 状态。

> PS：通过延迟确认的技术（通常有时间限制，否则对方会误认为需要重传），可以将第二次和第三次握手合并，延迟 ACK 包的发送。

**第四次挥手**

A 收到释放请求后，向 B 发送确认应答，此时 A 进入 TIME-WAIT 状态。该状态会持续 2MSL（最长报文段寿命，指报文段在网络中生存的时间，超时会被抛弃） 时间，若该时间段内没有 B 的重发请求的话，就进入 CLOSED 状态。当 B 收到确认应答后，也便进入 CLOSED 状态。



#### **问：ARQ协议**

答：ARQ协议，即自动重传请求（Automatic Repeat-reQuest），是OSI模型中的错误纠正协议之一。

它通过使用确认和重传这两个机制，在不可靠服务的基础上实现可靠的信息传输；

如果发送方在发送后一段时间之内没有收到确认帧，它通常会重新发送；

重传的请求是自动进行的，接收方不需要请求发送方重传某个出错的分组；

ARQ包括停止等待ARQ协议和 连续ARQ协议。

（1）停止等待ARQ协议

基本思想就是每发完一个分组就停止发送，等待对方的确认（回复ACK）。如果过了一段时间（超时时间后），还是没有收到ACK确认，说明没有发送成功，需要重新发送，直到收到确认后再发送下一个分组。

在停止等待协议中，若接收方收到重复分组，那就丢弃该分组，但同时还要发送确认。

（2）连续ARQ协议

为了提高信道利用率，连续ARQ协议的发送方维持一个发送窗口，凡是位于发送窗口内的分组可以连续发送出去，而不需要等待对方确认。接收方一般采用累计确认，对按序到达的最后一个分组发送确认，表明到这个分组为止的所有分组都已经正确收到了。

为了解决这种流水线的差错恢复，TCP提供了两种滑动窗口：**回退N(Go-Back- N，GBN)** 和**选择重传(Selective Repeat，SR)** 。

**回退N帧**：发送窗口大于1，接收窗口等于1。发送方可连续发送多个数据帧 而不需等待对方确认。接收方只允许按顺序接收帧。当一个分组确认时间超时，发送方就需要重传所有待确认的分组。

**选择重传**：发送窗口大于1，接收窗口大于1。只重传真正出错或者丢失的分组。具体就是发送方维持着一个窗口，包含可发送或已发送但未被确认的序号。接收方维持着一个窗口，==包含可接收的序号，每个序号还保留一个缓存区，用来指明是否被填充==。每个接收方接到分组后会判断是否在窗口内，在的话就先缓存并返回确认。而发送到这边每个发送缓存都设置一个计时器，超时没有确认就重发。



ARQ到底运行在那一层？

ARQ是一种可以在不可靠的数据通道上可靠地传输数据的方案，所以其实链路层和传输层都用了ARQ，并不专属某一层。

并不是一条连接只要有一层用了ARQ，它的上层的通信就是可靠的。因为ARQ只保证使用它的点到点是可靠的，比如数据链路层只保证你和你的路由器通信可靠，你的路由器到小区的路由器通信也可靠， 但是路由器本身会故障，会拥塞丢包，也就是点本身会产生问题。

所以需要在传输层或者应用层再加一层ARQ保障整条数据通道的可靠性。比如你自己写程序要在应用层通信，但传输层不用tcp想用udp，也可以在你程序里用ARQ协来实现可靠性。

参考文章：[TCP可靠传输：ARQ协议（停止等待、超时重传、滑动窗口、回退N帧、选择重传）](https://juejin.cn/post/7059671699926548511)







#### **问：流量控制**

#### **问：拥塞控制**

参考文章：[TCP基础知识](http://www.yswang.tech/archives/2022-01-10-21-48-24#WHxnnpDm)



#### 问：TCP 怎么保证可靠传输

什么是可靠传输：可靠传输就是保证接收方收到的字节流和发送方发出的字节流是完全一样的，无差错、不丢失、不重复、按序到达。

TCP 保证可靠传输的机制有如下几种：校验和、序列号和确认应答机制、重传机制、滑动窗口机制、流量控制、拥塞控制。

参考文章：[TCP 怎么保证可靠传输](https://flying-veal.notion.site/TCP-08b94140ee224f0c826bc7b81477d57e) 



#### 问：说一说 HTTP 请求的过程（从输入域名到浏览器看见页面经历了什么过程）

参考文章：[说一说 HTTP 请求的过程](https://flying-veal.notion.site/HTTP-8e300601270d4542a6f5a2077c91b9c8)



#### **问：HTPP1.0/HTTP1.1/HTTP2.0 区别**

#### **问：介绍HTTPS协议**

#### **问：HTTP的请求与响应**

#### **问：密钥加密技术**

#### **问：HTTPS的数字数字证书验证原理**

参考文章：[HTTP基础知识](http://www.yswang.tech/archives/http-ji-chu-zhi-shi#RnerTJRt)



#### **问：知道哪些密钥算法？**

对称加密算法：指在加密和解密时使用的是同一个秘钥，常用算法：AES、DES（Data Encryption Standard）

DES算法把64位的明文输入块变为64位的密文输出块，它所使用的密钥也是64位。

非对称加密算法：非对称加密算法需要公钥和私钥。 公钥和私钥是一对,如果用公钥对数据进行加密,只有用对应的私钥才能解密。常用算法：RSA，DSA（Digital Signature Algorithm，数字签名算法）

RSA 算法可以生成公钥和私钥的。公钥用于加密信息，私钥用来解密。算法特点是由公钥和信息作为参数进行运算，得到密文，这个过程要很容易。而逆向运算，由密文和公钥想要获得信息，是很难做到的。当然，这个算法还必须有另外一个特点。就是逆向操作虽然默认很难做到的，但是如果拥有了特定的提示信息，也就是私钥，操作就变得非常容易了。

具体实现涉及取模运算和整数分解，实际生成公钥和私钥的过程是，我们选出 p1 和 p2 两个大素数，让 N = p1 * p2 。随机选择一个指数 e ，这样公钥就有了。而在知道 p1 和 p2 的前提下，从公钥算出私钥，也就是算出 d ，是非常容易的。而外人，因为不知道 p1 和 p2 ，而只知道 N ，所以不可能从 e 算出 d ，也就是不可能用公钥算出私钥。

参考文章：[RSA 算法--基本原理篇](https://blog.csdn.net/yexudengzhidao/article/details/102759795)



#### 问：泛洪攻击

答：泛洪攻击是一种针对TCP三次握手进行攻击的手段：

攻击者在第一次握手阶段向服务器发送大量的SYN包，在服务器回应第二次握手成功后，不向服务器端发送ACK包导致服务器在第二次握手后存在大量的半开连接，消耗服务器资源，最后使得服务器无法再响应TCP连接，达到攻击目的。

使用SYN cookie可以有效抵御泛洪攻击：

服务器在第二次握手时不会为第一次握手的SYN创建半开连接，而是生成一个cookie一起发送给客户端，只有客户端在第三次握手发送ACK报文并且验证cookie成功服务器才会创建TCP连接，分配资源。





## 数据库MySQL

#### 问：数据库三范式

1NF(第一范式)

属性（对应于表中的字段）不能再被分割，也就是这个字段只能是一个值，不能再分为多个其他的字段了。1NF 是所有关系型数据库的最基本要求 ，也就是说关系型数据库中创建的表一定满足第一范式。

2NF(第二范式)

2NF 在 1NF 的基础之上，消除了非主属性对于码的部分函数依赖。如下图所示，展示了第一范式到第二范式的过渡。第二范式在第一范式的基础上增加了一个列，这个列称为主键，非主属性都依赖于主键。

一些重要的概念：

- 函数依赖（functional dependency） ：若在一张表中，在属性（或属性组）X 的值确定的情况下，必定能确定属性 Y 的值，那么就可以说 Y 函数依赖于 X，写作 X → Y。
- 部分函数依赖（partial functional dependency） ：如果 X→Y，并且存在 X 的一个真子集 X0，使得 X0→Y，则称 Y 对 X 部分函数依赖。比如学生基本信息表 R 中（学号，身份证号，姓名）当然学号属性取值是唯一的，在 R 关系中，（学号，身份证号）->（姓名），（学号）->（姓名），（身份证号）->（姓名）；所以姓名部分函数依赖与（学号，身份证号）；
- 完全函数依赖(Full functional dependency) ：在一个关系中，若某个非主属性数据项依赖于全部关键字称之为完全函数依赖。比如学生基本信息表 R（学号，班级，姓名）假设不同的班级学号有相同的，班级内学号不能相同，在 R 关系中，（学号，班级）->（姓名），但是（学号）->(姓名)不成立，（班级）->(姓名)不成立，所以姓名完全函数依赖与（学号，班级）；
- 传递函数依赖 ： 在关系模式 R(U)中，设 X，Y，Z 是 U 的不同的属性子集，如果 X 确定 Y、Y 确定 Z，且有 X 不包含 Y，Y 不确定 X，（X∪Y）∩Z=空集合，则称 Z 传递函数依赖(transitive functional dependency) 于 X。传递函数依赖会导致数据冗余和异常。传递函数依赖的 Y 和 Z 子集往往同属于某一个事物，因此可将其合并放到一个表中。比如在关系 R(学号 ,姓名, 系名，系主任)中，学号 → 系名，系名 → 系主任，所以存在非主属性系主任对于学号的传递函数依赖。。

3NF(第三范式)

3NF 在 2NF 的基础之上，消除了非主属性对于码的传递函数依赖 。符合 3NF 要求的数据库设计，基本上解决了数据冗余过大，插入异常，修改异常，删除异常的问题。比如在关系 R(学号 ,姓名, 系名，系主任)中，学号 → 系名，系名 → 系主任，所以存在非主属性系主任对于学号的传递函数依赖，所以该表的设计，不符合 3NF 的要求。

总结

- 1NF：属性不可再分。
- 2NF：1NF 的基础之上，消除了非主属性对于码的部分函数依赖。
- 3NF：3NF 在 2NF 的基础之上，消除了非主属性对于码的传递函数依赖 。



#### 问：MySQL 组成部分，一条 SQL 查询语句怎么执行的

答：把 MySQL 分成三层，跟客户端对接的连接层，真正执行操作的服务层，和跟硬件打交道的存储引擎层。

![image-20211007102305222](pics\bfa420414fe84966b4d04a9fdbc6d907~tplv-k3u1fbpfcp-watermark.awebp)

- 连接层：我们的客户端要连接到 MySQL 服务器 3306 端口，必须要跟服务端建立连接，那么管理所有的连接，验证客户端的身份和权限，这些功能就在连接层完成。

- 服务层：连接层会把 SQL 语句交给服务层，这里面又包含一系列的流程：

  比如查询缓存的判断、根据 SQL 调用相应的接口，对我们的 SQL 语句进行词法和语法的解析（比如关键字怎么识别，别名怎么识别，语法有没有错误等等）。

  然后就是优化器，MySQL 底层会根据一定的规则对我们的 SQL 语句进行优化，最后再交给执行器去执行。

- 存储引擎：存储引擎就是我们的数据真正存放的地方，在 MySQL 里面支持不同的存储引擎。再往下就是内存或者磁盘。

SQL的执行流程：

以sql语句为例，梳理一下整个sql执行流程

```sql
select name from user where id = 1 and age >20;
```

通过连接器查询当前执行者的角色是否有权限，进行查询。如果有的话，就继续往下走，如果没有的话，就会被拒绝掉，同时报出 `Access denied for user` 的错误信息；

接下来就是去查询缓存，首先看缓存里面有没有，如果有呢，那就没有必要向下走，直接返回给客户端结果就可以了；如果缓存中没有的话，那就去执行语法解析器和预处理模块。（ MySQL 8.0 版本直接将查询缓存的整块功能都给删掉了）

语法解析器和预处理主要是分析sql语句的词法和语法是否正确，没啥问题就会进行下一步，来到查询优化器；

查询优化器就会对sql语句进行一些优化，看哪种方式是最节省开销，就会执行哪种sql语句，上面的sql有两种优化方案：

- 先查询表 user 中 id 为 1 的人的姓名，然后再从里面找年龄大于 20 岁的。
- 先查询表 user 中年龄大于 20 岁的所有人，然后再从里面找 id 为 1 的。

优化器决定选择哪个方案之后，执行引擎就去执行了。然后返回给客户端结果。

参考文章：[MySQL 是如何执行一条查询语句的？](https://juejin.cn/post/7017608820054556679)



#### 问：说说MySQL有哪些存储引擎？都有哪些区别？

MySQL的存储引擎主要有MyISAM和InnoDB。默认存储引擎是InnoDB。

- **InnoDB 支持事务，MyISAM 不支持事务**。这是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一；
- **InnoDB 最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁**。一个更新语句会锁住整张表，导致其他查询和更新都会被阻塞，因此并发访问受限。这也是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一；
- **InnoDB 采用聚簇索引，MyISAM 是非聚簇索引**，也就是说InnoDB的B+树主键索引的叶子节点就是数据文件，辅助索引的叶子节点是主键的值；而MyISAM的B+树主键索引和辅助索引的叶子节点都是数据文件的地址指针
- InnoDB 支持外键，而 MyISAM 不支持。对一个包含外键的 InnoDB 表转为 MYISAM 会失败；


- InnoDB 不保存表的具体行数，执行` select count(*) from table` 时需要全表扫描。而 MyISAM 用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快；


> 在 MySQL 中，MyISAM 引擎和 InnoDB 引擎都是使用 B+Tree 作为索引结构，但是，两者的实现方式不太一样。
>
> **MyISAM 引擎中，B+Tree叶节点的 data 域存放的是数据记录的地址**。在索引检索的时候，首先按照 B+Tree 搜索算法搜索索引，如果指定的 Key 存在，则取出其 data 域的值，然后以 data 域的值为地址读取相应的数据记录。这被称为“非聚簇索引”。
>
> **InnoDB 引擎中，其数据文件本身就是索引文件**。相比 MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按 B+Tree 组织的一个索引结构，**树的叶节点 data 域保存了完整的数据记录**。这个索引的 key 是数据表的主键，因此 InnoDB 表数据文件本身就是主索引。这被称为“聚簇索引（或聚集索引）”。而**其余的索引都作为辅助索引，辅助索引的 data 域存储相应记录主键的值而不是地址**，这也是和 MyISAM 不同的地方。在根据主索引搜索时，直接找到 key 所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，在走一遍主索引。因此，在设计表的时候，不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂。





#### **问：MySQL 都有哪些索引，为什么要用索引**

答：==索引是一种类似目录的用于快速查询和检索数据的数据结构==。常见的索引结构有：B树、B+树和Hash。优点是使用索引可以大大加快 数据的检索速度；缺点是创建索引和维护索引需要耗费许多时间，降低SQL执行效率；而且索引需要使用物理文件存储，也会耗费一定空间。

**从数据结构的角度：**

- B+树索引
- Hash索引

**从物理存储的角度：**

- 聚集索引（clustered index）

- 非聚集索引（non-clustered index），也叫辅助索引（secondary index）

  聚集索引和非聚集索引都是B+树结构

**从逻辑的角度：**

主键索引（Primary Key）

数据表的主键列使用的就是主键索引。一张数据表有只能有一个主键，并且主键不能为 null，不能重复。

二级索引（辅助索引）

二级索引又称为辅助索引，是因为二级索引的叶子节点存储的数据是主键。也就是说，通过二级索引，可以定位主键的位置。包括如下类型：

普通索引：MySQL中基本索引类型，没有什么限制，允许在定义索引的列中插入重复值和空值。唯一作用就是为了快速查询数据，一张表允许创建多个普通索引。

唯一索引：索引列中的值必须是唯一的，但是允许为空值。这类索引主要是为了该属性列的数据的唯一性，而不是为了查询效率。**唯一索引，它会去确保列的唯一，所以会多一次判断的过程，但是它判断过程的开销是很小的。真正的开销的话是在它buffer区的。**

前缀索引：前缀索引只适用于字符串类型的数据。前缀索引是对文本的前几个字符创建索引，相比普通索引建立的数据更小， 因为只取前几个字符。

全文索引：全文索引主要是为了检索大文本数据中的关键字的信息，是目前搜索引擎数据库使用的一种技术。Mysql5.6 之前只有 MYISAM 引擎支持全文索引，5.6 之后 InnoDB 也支持了全文索引。

空间索引：MySQL在5.7之后的版本支持了空间索引，而且支持OpenGIS几何数据模型。MySQL在空间索引这方面遵循OpenGIS几何数据模型规则。

其他（按照索引列数量分类）

1. 单列索引
2. 组合索引：组合索引的使用，需要遵循最左前缀匹配原则（最左匹配原则）。一般情况下在条件允许的情况下使用组合索引替代多个单列索引使用。

参考文章：[一口气搞懂MySQL索引所有知识点](https://mp.weixin.qq.com/s/faOaXRQM8p0kwseSHaMCbg)



#### **问：MySQL 聚集索引和非聚集索引的区别**

答：（1）==聚集索引即索引结构和数据一起存放的索引==。主键索引属于聚集索引。

优点是聚集索引的查询速度非常的快，因为整个 B+树本身就是一颗多叉平衡树，叶子节点也都是有序的，定位到索引的节点，就相当于定位到了数据。

缺点：

1. 依赖于有序的数据
2. 更新代价大

（2）==非聚集索引即索引结构和数据分开存放的索引==。

优点是更新代价比聚集索引要小 。非聚集索引的更新代价就没有聚集索引那么大了，非聚集索引的叶子节点是不存放数据的。

缺点：

1. 跟聚集索引一样，非聚集索引也依赖于有序的数据
2. 可能会二次查询(回表)

> 非聚集索引的叶子节点并不一定存放数据的指针， 因为二级索引的叶子节点就存放的是主键，根据主键再回表查数据。

参考文章：[JavaGuide 索引](https://snailclimb.gitee.io/javaguide/#/docs/database/mysql/mysql-index?id=%e8%81%9a%e9%9b%86%e7%b4%a2%e5%bc%95%e4%b8%8e%e9%9d%9e%e8%81%9a%e9%9b%86%e7%b4%a2%e5%bc%95)、[MySQL 聚集索引 和非聚集索引 的区别](https://flying-veal.notion.site/MySQL-f2f74a2d0b30456dac006fc7e897be96)



#### 问：讲讲回表查询和覆盖索引，为什么需要覆盖索引 ，可以使用覆盖索引优化的场景

答：==覆盖索引指一个查询语句的执行结果只用从索引中就能够取得，不必再去数据表中读取，主要是为了避免回表查询==。也可以称之为实现了索引覆盖。 当一条查询语句符合覆盖索引条件时，MySQL只需要通过索引就可以返回查询所需要的数据，这样避免了查到索引后再返回表操作，减少I/O提高效率。

回表查询，是因为非聚集索引的叶子节点存放的是主键值。当查到索引对应的主键后，可能还需要根据主键再到表中查询记录值。因为需要扫描两次索引 B+ 树，所以很显然它的性能较扫一遍索引树更低。

覆盖索引的目的就是避免发生回表查询，也就是说，通过覆盖索引，只需要扫描一次 B+ 树即可获得所需的行记录。

覆盖索引的常见使用场景：

（1）列查询回表优化（将单列索引升级为联合索引）

（2）全表count查询

```sql
 select count(age) from user;
```

（3）分页查询

参考文章：[回表查询和覆盖索引](https://flying-veal.notion.site/8f2c2a40d15c40cb8e239c2dbdfd8d4e)



#### **问：联合索引的最左匹配原则：为什么得最左匹配，不按照这个来为什么失效？**

答：多个普通字段组合在一起创建的索引就叫做联合索引。

==联合索引的最左匹配原则是指联合索引的最左前缀原则，只要查询的是联合索引的最左 N 个字段，就可以利用该联合索引来加速查询==。

不按照最左匹配来为什么失效，其原因就在于联合索引的 B+ 树中的键值是排好序的。不过，这里指的排好序，其实是相对的，举个例子，有 (a, b, c) 联合索引，a 首先是排序好的，而 b 列是在 a 列排序的基础上做的排序，同样的 c 是在 a,b 有序的基础上做的排序。

在创建联合索引的时候，如何安排索引内的字段顺序：

第一，优先选择使用频率高的字段放在索引的更左列。

第二，考虑空间开销，比如 name 和age建立联合索引，有两种方案：

1. 联合索引 (age, name) + 单字段索引 (name)
2. 联合索引 (name, age) + 单字段索引 (age)

因为name 字段是比age 字段消耗空间大，所以从空间消耗角度方案二更好些。

如果有对（a，b，c）的联合条件查询的话，并且 a 是模糊匹配或者说是范围查询的话，其实并不能完全踩中联合索引（a，b，c），a 列右边的所有列都无法使用索引进行快速定位了。所以这个时候就需要进行回表判断。也就是说数据库会首先根据索引来查找记录，然后再根据 where 条件来过滤记录。

而 MySQL 5.6 开始，==数据库在取出索引的同时，会根据 where 条件直接过滤掉不满足条件的记录，减少回表次数==。这就是 索引下推 (Index Condition Pushdown,ICP) ，一种根据索引进行查询的优化方式

参考文章：[联合索引的最左匹配原则](https://flying-veal.notion.site/bb78dbdb70004893b6b7d6bcc88a209d)



#### **问：有哪些索引失效的场景？**

（1）不满足最左前缀原则

（2）索引字段做运算：对索引字段做运算，比较时发送了字符串类型转化等都会导致索引失效。subString函数

（3）以%开头的like左模糊查询：因为索引 B+ 树是按照「索引值」有序排列存储的，只能根据前缀进行比较。

（4）用or分割开的条件， 如果or前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到。因为有一个不走索引，又是or条件，两个都要判断一下，相当于不管如何，都还是得去走全表查询，没有利用到索引。

（5）使用不等式 `!= 或 <>`也会导致索引失效

（6）MySQL认为全表更快时，不走索引。比如如果表中is null的比较多，那自然就直接全表扫描，如果is null的很少，会走索引。

查询语句的条件都用到了索引列，所以在查询过程都用上了索引。但是并不意味着，查询条件用上了索引列，就查询过程就一定都用上索引，有些情况会导致索引失效，而发生全表扫描。

> 联合索引最左匹配原则、索引字段运算、like左模糊查询、or运算

参考文章：[explain分析SQL，索引失效&&常见优化场景](https://juejin.cn/post/7061517454433845256)



#### **问：MySQL调优**

答：**我们在项目中的MySQL调优重点一般是在 开发规范 、数据库索引上**。

基本上只要有查询需求，都应该考虑建索引。

当需要创建索引时，会考虑索引能否使用 覆盖索引，减少 回表 所消耗的时间。

其次在创建联合索引时，会考虑综合考虑联合索引字段的使用频率和空间消耗。

为了让查询尽可能走索引，使用sql 语句时会遵循最左匹配的原则、select时指明具体字段、不建议使用函数、超过3张表不建议join

至于MySQL参数调优，这个一般是专门的DBA来搞，我们项目的话，因为那台服务器就是专门用来跑项目的，所以会设置`innodb_buffer_pool_size` 来配置缓冲池，缓冲池是数据和索引缓存的地方：这个值越大越好，这能保证你在大多数的读取操作时使用的是内存而不是硬盘。

当创建了索引，查询依然很慢的时候，会先凭经验或者慢查询确定具体的语句。

接着会使用 explain 进行分析，看看自己写的SQL是否走了索引，走了什么索引。

![img](C:\Users\Lenovo\Desktop\笔记\CS-Learning-Notes\notes\pics\8b06016197b441a99e239cdc620bd109~tplv-k3u1fbpfcp-watermark.awebp)

**即便走对了索引，线上查询还是慢，是如何解决的？**

一般而言，走对了索引但查询还是慢，多是因为数据量实在太大了。如果可以删除，那么删除是比较好的。

如不行的话，利用空间换时间的思想。用户数据来之不易，所以我们应对这个问题的方法是走一层Redis缓存。如果业务不能忍受非实时的话，那么可以考虑要不要根据查询条件的维度，做相对应的聚合表，线上的请求就查询聚合表的数据，不走原表。

**除了读之外，写性能同样有瓶颈，怎么办？**

如果是单库的话，那么可以考虑升级至主从架构，实现读写分离。简单理解就是主库接收写请求，从库接收读请求。从库的数据由主库发送的binlog进行更新，实现主从一致性（在一般情况下，主从的数据是通过异步来保证最终的一致性的）。

如果在主从架构下，读写仍然存在瓶颈，那就要考虑分库分表了。业务是有区分的，可以将一些区分度大的业务区分开来。

参考文章：[MySQL索引和SQL调优](https://juejin.cn/post/6844903555141206030#heading-0) 、 [面试官问我MySQL调优，我真的是](https://juejin.cn/post/7017969370206830623) 、[*MySQL优化](https://juejin.cn/post/6844903555141206030#heading-21)



#### 问：explain命令参数

- id：select查询的序列号，id值越大，优先级越高；

- select_type：表示 SELECT 的类型
- type：表示表的连接类型，一般至少要达到range级别，最好达到ref （非唯一性索引扫描）。
- possible_keys：表示查询时，可能使用到的索引
- **key**：表示查询时，实际使用的索引
- key_len：索引字段的长度
- **ref**：显示索引的哪一列被使用了，如果可能的话，是一个常数
- **rows**：扫描行数
- filtered：表里符合条件的记录数所占的百分比
- **extra**：执行情况的说明和描述
  - using filesort：在索引之外，需要额外进行外部的排序动作；一般是order by有关
  - using index：查询使用了覆盖索引，无需回表
  - using index condition：使用索引下推
  - using where：没用索引，需要全表扫描，再按where过滤



#### **问：MySQL事务中binLog和 redoLog怎么保证一致性？**

![全网最牛X的！！！MySQL两阶段提交串讲](C:\Users\Lenovo\Desktop\笔记\CS-Learning-Notes\notes\pics\37f6272ba10d43cba51c6405a4f4c1ad~tplv-k3u1fbpfcp-watermark.awebp)

两段式提交：就是我们先把这次更新写入到redolog中，并设redolog为prepare状态，然后再写入binlog,写完binlog之后再提交事务，并设redolog为commit状态。也就是把relolog拆成了prepare和commit两段！

引入原因：首先我们要知道如果binLog和redoLog不能同时记录成功，会有什么影响？

1. binLog记录成功redoLog记录失败，我们知道==binLog其中一个功能是做数据库间的主从同步的==， 如果binLog记录成功后自然就会同步到从库中，此时如果主库宕机redoLog没有记录导致数据无法持久化。（redoLog是保证MYSQL事务中持久性的）
2. binLog记录失败redoLog记录成功，将会导致主从数据不一致情况。

MYSQL为了解决以上两种情况提供了两段提交方式，两段提交分别为prepare阶段、commit阶段。二阶段是为了保证两份日志（redolog和binlog）的的逻辑一致性。

情景1：binlog有写入，最后往redolog写commit失败了

binlog中有当前事务的Xid，则提交事务（复制redolog disk中的数据页到磁盘数据页）

情景2：写binlog时失败了

没有当前事务Xid，则回滚事务（使用undolog来删除redolog中的对应事务）

参考文章：[面试官：为什么Mysql innoDB是两段式提交？](https://juejin.cn/post/6844903831566811144) 、[mysql-事务的两阶段提交](https://juejin.cn/post/7053756422399459341)



#### 问：binlog、redolog 和undolog

（1）binlog 归档日志

==	==。

在实际应用中， `binlog `的主要使用场景有两个，分别是 主从复制 和 数据恢复 。

- 主从复制 ：在 `Master `端开启 `binlog `，然后将 `binlog `发送到各个 `Slave `端， `Slave `端重放 `binlog `从而达到主从数据一致。
- 数据恢复 ：通过使用 `mysqlbinlog `工具来恢复数据。

对于 `InnoDB `存储引擎而言，只有在事务提交时才会记录 `biglog `，此时记录还在内存中。需要再调用fsync()将binlog文件系统缓存日志数据永久写入磁盘。

（2）undo log 回滚日志

==原子性 底层就是通过 undo log实现的。 undo log主要记录了数据的逻辑变化==，比如一条 ` INSERT`语句，对应一条 `DELETE `的 `undo log `，对于每个 `UPDATE `语句，对应一条相反的 `UPDATE `的`undo log `，这样在发生错误时，就能回滚到事务之前的数据状态。同时， `undo log `也是 `MVCC `(多版本并发控制)实现的关键。

（3）redo log 重做日志

==redo log 保证持久化，用于在崩溃恢复期间纠正失败事务写入的数据。==

redo log是InnoDB引擎特有的，之所以出现是因为如果每一个事务提交后都写入到磁盘的话，mysql的压力会压力山大。

redo log组成由redolog buffer 和redologfile 组成。当我们要修改数据时，MySQL会先把这条记录所在的页找到，加载进内存中，将对应的记录修改。为了防止内存修改完了，MySQL就挂掉了。MySQL引入了 redo log，内存写完了，然后就会写一份redo log，记载这次在某个页做了什么修改。



redo log和undo log是如何写入磁盘的：	

==先写缓存 再写磁盘 也被简写为  WAL(Write-Ahead Logging) 即每一条DML都会先写入缓存 然后再由fsync()函数写入磁盘== ps:很多地方都用到了该技术 如 mysql的buffer pool 的(flush链表) nio中使用的 mmap filechannel等 我个人理解这些东西的共性就是离不开两个字 --- ==刷盘==

<img src="C:\Users\Lenovo\Desktop\笔记\CS-Learning-Notes\notes\pics\ab2da64bf45349d8b227a9ea8a93bcbf~tplv-k3u1fbpfcp-watermark.awebp" alt="image.png" style="zoom:50%;" />

![image.png](C:\Users\Lenovo\Desktop\笔记\CS-Learning-Notes\notes\pics\eaa999815d2048b894202b16bfff95fb~tplv-k3u1fbpfcp-watermark.awebp)

insert update delete 操作 在mysql内部是怎么个执行过程???成功会怎样？失败会怎样?

![image.png](C:\Users\Lenovo\Desktop\笔记\CS-Learning-Notes\notes\pics\f867f36027e144708f23810ab1062141~tplv-k3u1fbpfcp-watermark.awebp)

参考文章：https://juejin.cn/post/6997602633292726303



#### 问：MySQL索引为什么使用B+树？

答：常见的MySQL主要有两种结构：Hash索引和B+Tree索引，我们使用的是InnoDB引擎，默认的是B+树。

因为Hash索引底层是哈希表，==哈希表是一种以key-value存储数据的结构==，所以多个数据在存储关系上是完全没有任何顺序关系的，所以，对于区间查询是无法直接通过索引查询的，就需要全表扫描。所以，哈希索引只适用于等值查询的场景。其次哈希索引不支持多列联合索引的最左匹配规则和模糊查询，并且存在哈希碰撞的问题，影响效率。

二叉查找树，比如红黑树，红黑树是二叉树搜索树的变种，一个Node节点只能存储一个key和一个Value。

而B树是一种多路平衡查找树，它的每一个节点最多包含m个孩子，m被称为B树的阶。m的大小取决于磁盘页的大小。B树的定义如下：

- 每个节点最多有m-1个关键字（可以存有的键值对）。
- 根节点最少可以只有1个关键字。 非根节点至少有m/2个关键字。
- 每个节点中的关键字都按照从小到大的顺序排列，每个关键字的左子树中的所有
- 关键字都小于它，而右子树中的所有关键字都大于它。
- 所有叶子节点都位于同一层，或者说根节点到每个叶子节点的长度都相同。

MySQL的数据是存储在硬盘的，在查询时一般是不能「一次性」把全部数据加载到内存中。



B和B+树是多路搜索树，相较于与红黑树而言。一个Node节点可以存储的信息会更多，「多路搜索树」的高度会比「二叉搜索树」更低。

B+树相对于B树而言，它又有两种特性。

- B+树非叶子节点不存储数据，在相同的数据量下，B+树更加矮壮。（这个应该不用多解释了，数据都存储在叶子节点上，非叶子节点的存储能存储更多的索引，所以整棵树就更加矮壮）

- B+树叶子节点之间组成一个链表，方便于遍历查询（遍历操作在MySQL中比较常见）

> B+树是多路搜索树，树的层级更低；只有叶子节点存储数据会更矮；叶子节点具有双向链表，便于遍历数据。

参考文章：[面试官问我MySQL索引](https://juejin.cn/post/7003527396427038733)


#### 问：MySQL的事务

事务就是逻辑上的一组操作，要么都执行，要么都不执行。其目的是为了 保证数据最终的一致性。

> 保证事务的最终一致性

事务的四大特性，分别是原子性、一致性、隔离性和持久性。

其中原子性是指事务中所有的操作 要么全部完成，要么全部不完成。原子性由 undo log 日志来保证，因为undo log 记载着数据修改前的信息。举例，发生时机。

> 原子性：要么同时成功，要么同时失败。底层依赖 undo log 来实现

隔离性是指在事务并发执行时，他们内部的操作互不干扰。如果多个事务可以同时操作一个数据，那么就可能会产生脏读、重复读、幻读等问题。于是，事务与事务之间需要一定的隔离。在InnoDB中，定义了四种隔离级别。

分别是read uncommit(读未提交)、read commit (读已提交)、repeatable read (可重复复读)、serializable (串行)。不同的隔离级别对事务之间的隔离性是不一样的，隔离级别越高隔离性越好，但性能就越低。而隔离性是由MySQL的各种锁来实现的，只是它屏蔽了加锁的细节。

> 隔离性：事务之间需要隔离性，互不影响。MySQL提供了四种隔离级别，隔离级别的底层实现是锁

持久性是指一旦提交了事务，它对数据库的改变应该是永久的。也就是会被持久化到硬盘中。持久性由 redo log 日志来保证，当我们要修改数据时，MySQL会先把这条记录所在的页找到，加载进内存中，将对应的记录修改。为了防止内存修改完了，MySQL就挂掉了。MySQL引入了 redo log，内存写完了，然后就会写一份redo log，记载这次在某个页做了什么修改。

> 持久性：一旦提交了事务，数据是永久记录的。MySQL底层使用redo log 来持久化数据

一致性则可以理解成我们使用事务的目的，而隔离性、原则性和持久性都是为了保障一致性的手段，保证一致性需要由程序代码来保证。比如事务在发生的过程中，出现了异常情况，此时就得回滚事务，而不是强行提交事务来导致数据不一致。

> 一致性：一致性是事务的目的，需要通过应用程序来保证一致性

在InnoDB引擎中，按锁的粒度分类，可以简单的分成行锁和表锁。锁是作用在索引上的，当我们的SQL命中了索引，那锁中的就是索引节点，也就是行锁，如果没有命中索引，那么锁的就是整个索引树，也就是表锁。

而行锁又可以分成读锁（共享锁、S锁）和写锁（排他锁、X锁）。读锁是共享的，多个事务可以同时读取同一个资源，但不允许其他事务修改。写锁是排他的，写锁也会阻塞其他的写锁和读锁。

> InnoDB引擎下，命中了索引是行锁，没命中则是表锁。行锁又可以分成读锁和写锁

read uncommitted（读未提交）如果事务B读到了事务A还没提交的数据，就也是脏读，那么这种隔离级别就叫read uncommitted。

对于锁的维度而言，其实就是在read uncommitted隔离级别下，读不会加任何锁，而写会加排他锁。读什么锁都不加，这就让排他锁无法排它了。

> read uncommited 原理：读不加锁，修改加写锁

脏读在生产环境下肯定是没法被接受的，那如果读加锁的话，那意味着：当更新数据的时候，就没办法读取了，这会极大地降低数据库性能。在MySQL InnoDB引擎层面，解决方案（解决加锁后读写性能问题），叫做MVCC（Multi-Version Concurrency Control)多版本并发控制。

> MVCC：提高读写性能，多版本并发控制

在MVCC下，就可以做到读写不阻塞，且避免了类似脏读这样的问题。做法就是MVCC通过生成数据快照（Snapshot)，并用这个快照来提供一定级别（语句级或事务级）的一致性读取。

read commit (读已提交) 隔离级别解决了脏读。思想其实很简单：在读取的时候生成一个”版本号”，等到其他事务提交了之后，才会读取最新已提交的”版本号”数据。

而repeatable read (可重复复读)则是每次读取的都是「当前事务的版本」，即使当前数据被其他事务修改了(commit)，也只会读取当前事务版本的数据。

参考文章：[面试官一口气问了MySQL事务、锁和MVCC，我](https://juejin.cn/post/7016486567485112356)、[Mysql 事务中Update 会锁表吗？](https://andyoung.blog.csdn.net/article/details/109022700?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ELandingCtr%7ERate-1.queryctrv4&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ELandingCtr%7ERate-1.queryctrv4&utm_relevant_index=2)



#### 问：SQL注入讲一下，怎么避免

答：SQL注入是指未经检查或者未经充分检查的用户输入数据，意外变成了代码被执行。

避免方法的核心思想就是不相信用户的输入。

1. 代码层防止sql注入攻击的最佳方案就是sql预编译（sql模板化或者说参数化）

   ```sql
   select*from tablename where username=? and password=?
   ```

   参数化能防注入的原因在于，语句是语句，参数是参数，参数的值并不是语句的一部分，数据库只按语句的语义跑

2. 数据类型检查、关键字过滤



#### 问：当前读和快照读

（1）快照读：就是单纯的 SELECT 语句，执行方式是生成 ReadView，直接利用 MVCC 机制来进行读取，并不会对记录进行加锁。

（2）当前读：读取的是最新版本，并且需要先获取对应记录的锁，如以下这些 SQL 类型：

select ... lock in share mode 、

select ... for update、

update 、delete 、insert

要 update 一条记录，在事务执行过程中，如果不加锁，那么另一个事务可以 delete 这条数据并且能成功 commit，就会产生冲突了。所以 update 的时候肯定要是当前读，得到最新的信息并且锁定相应的记录。

当前读是通过 next-key 锁(行记录锁+间隙锁)来是实现的。



Innodb支持三种行锁定方式

**行锁**：锁的是索引，如果SQL没有走索引，那么会全表扫描，从而升级为表锁

**Gap锁（间隙锁）**：==当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时（增删改查操作，Mysql会默认加锁），InnoDB会给符合条件的已有数据记录的索引项加行锁。对于键值在条件范围内但并不存在的记录加上间隙锁。Innodb 为了解决幻读问题时引入的锁机制，所以只有在 Read Repeatable 、Serializable 隔离级别才有。要取消间隙锁的话，切换隔离级别为读已提交即可。

**Next-Key Lock**：行锁与间隙锁组合起来用就叫做Next-Key Lock。

参考连接：https://juejin.cn/post/6988887904651051044



#### 问：分库分表

分库分表就是要将==大量数据分散到多个数据库中==，使==每个数据库中数据量小响应速度快==，以此来提升数据库`整体性能`。

针对数据切分类型，大致可以分为：垂直（纵向）切分和水平（横向）切分两种。

**垂直切分**：

垂直切分又细分为垂直分库和垂直分表。

垂直分库是按照业务分类进行划分，每个业务有独立数据库。

垂直分表是基于数据表的列为依据切分的，是一种==大表拆小表==的模式

优点：

1. 业务间解耦，不同业务的数据进行独立的维护、监控、扩展 
2. 在高并发场景下，一定程度上缓解了数据库的压力

缺点：

1. 提升了开发的复杂度，由于业务的隔离性，很多表无法直接访问，必须通过接口方式聚合数据 
2. 分布式事务管理难度增加 
3. 数据库还是存在单表数据量过大的问题，并未根本上解决，需要配合水平切分

**水平切分**：

水平切分将一张大数据量的表，切分成多个表结构相同，而每个表只占原表一部分数据，然后按不同的条件分散到多个数据库中。

水平切分又分有==库内分表和分库分表==

库内分表虽然将表拆分，但子表都还是在同一个数据库实例中，只是解决了单一表数据量过大的问题，并没有将拆分后的表分布到不同机器的库上，还在竞争同一个物理机的CPU、内存、网络IO。

分库分表则是将切分出来的子表，分散到不同的数据库中，从而使得单个表的数据量变小，达到分布式的效果。

优点：

1. 解决高并发时单库数据量过大的问题，提升系统稳定性和负载能力 
2. 业务系统改造的工作量不是很大

缺点：

1. 跨分片的事务一致性难以保证 
2. 跨库的join关联查询性能较差 
3. 扩容的难度和维护量较大，（拆分成几千张子表想想都恐怖）



**数据该往哪个库的表存？**

分库分表以后会出现一个问题，一张表会出现在多个数据库里，到底该往哪个库的表里存呢？

**根据取值范围** ：按照时间区间或ID区间来切分

**hash取模**：hash取模mod（`对hash结果取余数 (hash() mod N`)）的切分方式也比较常见，这样同一个用户的数据都会存在同一个库里，用userId作为条件查询就很好定位了

参考连接：https://blog.csdn.net/qq_40813329/article/details/121405366





# Java

## Java基础

#### **问：重载与重写的区别**

答：重载就是同样的一个方法能够根据输入数据的不同，做出不同的处理。

重写就是当子类继承自父类的相同方法，输入数据一样，但要做出有别于父类的响应时，就要覆盖父类方法。

参考文章：





#### 问：什么时候会用到静态内部类？

静态内部类的主要特点：1 不持有外部类的引用（普通内部类持有）；2 可以直接创建实例，不需要先创建外部类（普通内部类需要）；3 可以有静态成员变量、方法（普通内部类不行）和非静态成员变量、方法；4 只可以直接访问外部类静态成员，不可以直接访问外部类的非静态成员（普通内部类可以），需要通过传入外部类引用的方式才能访问。使用场景：外部类与内部类有很强的联系，需要通过内部类的方式维持嵌套的可读性。内部类可以单独创建。内部类不依赖于外部类，外部类需要使用内部类，而内部类不需使用外部类（或者不合适持有外部类的强引用）。





#### **问：== 和equals() 的区别**

答：`==` 对于基本类型和引用类型的作用效果是不同的：

- 对于基本数据类型来说，`==` 比较的是值。
- 对于引用数据类型来说，`==` 比较的是对象的内存地址。

`equals()` 不能用于判断基本数据类型的变量，只能用来判断两个对象是否相等。



#### **问：你重写过 `hashCode()` 和 `equals()`么?为什么重写 `equals()` 时必须重写 `hashCode()` 方法？**

答：`hashCode()` 的作用是获取哈希码（`int` 整数），也称为散列码。这个哈希码的作用是确定该对象在哈希表中的索引位置。

- 如果两个对象的`hashCode` 值相等，那这两个对象不一定相等（哈希碰撞）。
- 如果两个对象的`hashCode` 值相等并且`equals()`方法返回 `true`，我们才认为这两个对象相等。
- 如果两个对象的`hashCode` 值不相等，我们就可以直接认为这两个对象不相等。

因为两个相等的对象的 `hashCode` 值必须是相等。也就是说如果 `equals` 方法判断两个对象是相等的，那这两个对象的 `hashCode` 值也要相等。

如果重写 `equals()` 时没有重写 `hashCode()` 方法的话就可能会导致 `equals` 方法判断是相等的两个对象，`hashCode` 值却不相等。



#### **问：自动装箱与拆箱了解吗？原理是什么？**

答：装箱是指将基本类型用它们对应的引用类型包装起来；拆箱是指将包装类型转换为基本数据类型。

从字节码中，我们发现装箱其实就是调用了 包装类的`valueOf()`方法，拆箱其实就是调用了 `xxxValue()`方法。



#### **问：面向过程和面向对象的区别**

- 面向过程把解决问题的过程拆成一个个方法，通过一个个方法的执行解决问题。
- 面向对象会先抽象出对象，然后用对象执行方法的方式解决问题。



#### **问：面向对象的三大特征**

答：封装是指把一个对象的状态信息（也就是属性）隐藏在对象内部，不允许外部对象直接访问对象的内部信息。

继承是使用已存在的类的定义作为基础建立新类的技术。

多态是指表示一个对象具有多种的状态，具体表现为父类的引用指向子类的实例。



#### **问：深拷贝和浅拷贝区别**

- 浅拷贝：浅拷贝会在堆上创建一个新的对象（区别于引用拷贝的一点），不过，如果原对象内部的属性是引用类型的话，浅拷贝会直接复制内部对象的引用地址，也就是说拷贝对象和原对象共用同一个内部对象。
- 深拷贝 ：深拷贝会完全复制整个对象，包括这个对象所包含的内部对象。



#### **问：String、StringBuffer、StringBuilder 的区别？String 为什么是不可变的?**

String不可变的原因：

1. 保存字符串的数组被 final 修饰且为私有的，并且String 类没有提供/暴露修改这个字符串的方法。
2. String 类被 final 修饰导致其不能被继承，进而避免了子类破坏 String 不可变。

三者的性能区别：

String 中的对象是不可变的，也就可以理解为常量，线程安全。AbstractStringBuilder 是 StringBuilder 与 StringBuffer 的公共父类，定义了一些字符串的基本操作，如 expandCapacity、append、insert、indexOf 等公共方法。StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。

三者使用总结：

1. 操作少量的数据: 适用 String
2. 单线程操作字符串缓冲区下操作大量数据: 适用 StringBuilder
3. 多线程操作字符串缓冲区下操作大量数据: 适用 StringBuffer



#### **问：字符串常量池的作用了解吗？**

答：字符串常量池 是 JVM 为了提升性能和减少内存消耗针对字符串（String 类）专门开辟的一块区域，主要目的是为了避免字符串的重复创建。



#### **问：介绍泛型的使用**

泛型的本质是参数化类型，也就是说所操作的数据类型被指定为一个参数。

Java 的泛型是伪泛型，这是因为 Java 在运行期间，所有的泛型信息都会被擦掉，这也就是通常所说类型擦除 。

泛型一般有三种使用方式: 泛型类、泛型接口、泛型方法。

常用的通配符有：

- ？ 表示不确定的 Java 类型
- T (type) 表示具体的一个 Java 类型
- K V (key value) 分别代表 Java 键值中的 Key Value
- E (element) 代表 Element



#### 问：泛型擦除

泛型擦除是指Java在运行时，不会保留泛型。Java泛型只存在存在于源码里，编译时会检查一下泛型类型是否正确，而到了运行时就不检查了。

为什么要泛型擦除：为了向下兼容，没有泛型的话，遍历的时候就得强制类型了。

像`ArrayList<String>`和`ArrayList<Integer>`在编译时是不同的类型，但是在编译完成后都被编译器简化成了`ArrayList`。

泛型擦除的后果：

反射困境：由于运行时没有泛型，导致很多基于反射的实现需要提供额外的泛型类型信息

```java
public class Foo {
  private LinkedList<Bar> bars;
  // 省略 get 和 set 方法
}
```

为什么？因为Gson在运行时通过反射拿不到bars里的元素应该是什么类型！所以对于这种情况，你就需要更绕也更反直觉的代码来解决问题。

**类型数量爆炸**和**基本数据类型歧视**







#### **问：反射的作用、原理、优劣、适用场景**

作用：Java的反射是指程序在运行期可以拿到一个对象的所有信息，解决在运行期，对某个实例一无所知的情况下，如何调用其方法。

原理：JVM为每个加载的`class`及`interface`创建了对应的`Class`实例来保存`class`及`interface`的所有信息；获取一个`class`对应的`Class`实例后，就可以获取该`class`的所有信息；获取`class` 的`Class` 实例的方法主要有：

1. 直接通过一个`class`的静态变量`class`获取
2. 如果我们有一个实例变量，可以通过该实例变量提供的`getClass()`方法获取
3. 如果知道一个`class`的完整类名，可以通过静态方法`Class.forName()`获取

优缺点：反射的优点就是比较灵活，能够在运行时动态获取类的实例。不过反射也存在很明显的缺点：

1. 性能瓶颈：反射相当于一系列解释操作，通知 JVM 要做的事情，性能比直接的 Java 代码要慢很多。
2. 安全问题：反射机制破坏了封装性，因为通过反射可以获取并调用类的私有方法和字段。

适用场景有动态代理机制等。

参考文章：[反射](https://www.liaoxuefeng.com/wiki/1252599548343744/1255945147512512) 、[面试题-反射](https://flying-veal.notion.site/13893931261e4dcfa620988756a43803)



问：sleep(）方法与wait()方法的区别

答：https://blog.csdn.net/qq_45036591/article/details/103214450







## Java集合

#### 问：HashMap的底层数据结构？

答：在JDK1.8之前，HashMap的底层是数组+链表结合在一起使用也就是链表散列。

在JDK1.8之后，变成了数组+链表+红黑树。由于数组+链表的形式会产生链表过长的现象，链表过长使得通过key值依次查找的效率变得很低，所以当链表长度大于8并且 HashMap 数组长度超过 64 的时候，链表就会转换为红黑树，加快查询。



#### 问：HashMap的存取原理？

答：put方法：首先根据key的hashcode做hash运算得到所在的数组下标；如果没有碰撞则直接放在数组里；如果碰撞了，会逐一检查这个链表里有没有相同的key对象，如果有则覆盖掉旧值，没有的话则插入在链表末尾。其次如果碰撞导致链表过长，也会把链表转换成红黑树。如果HashMap中元素个数超过了数组大小*负载因子时则会进行扩容。

get方法：首先是对key的hashcode做哈希运算得到所在下标；如果数组第一个节点命中，则直接返回；如果有冲突，则通过key.equals() 去在链表或者红黑树中查找。



#### 问：HashMap为什么线程不安全？

答：HashMap是线程不安全的，其主要体现：多线程扩容，可能会引起环形链或数据丢失。

所以建议使用比如ConcurrentHashmap、Hashtable等线程安全等集合类。

参考文章：[面试官：HashMap 为什么线程不安全？](https://mp.weixin.qq.com/s/VtIpj-uuxFj5Bf6TmTJMTw) 、[待看](https://juejin.cn/post/6844903518331994119)



#### 问：默认初始化大小是多少？为啥是这么多？为啥大小都是2的幂？

答：默认初始化大小是16，之所以选择16，是==为了服务将Key映射到index的算法==。当容量是 $2^{n}$时，$2^{n}-1$ 与 hash值进行按位与运算，相当于取hash值中的低n位的值作为对应的数组下标。只要输入的HashCode本身分布均匀，Hash算法的结果就是均匀的。这样子会比取模运算快上很多。



#### 问：HashMap的扩容方式？负载因子是多少？为什是这么多？

答：当HashMap中的元素个数超过数组大小 * 负载因子时，就会进行扩容。扩容分两步：

- ==扩容==：创建一个新的Entry空数组，长度是原数组的2倍。
- ==ReHash==：遍历原Entry数组，把所有的Entry重新Hash到新数组。

之所以不能直接复制是因为长度扩大以后，Hash的规则也随之改变。

负载因子是0.75是对空间和时间效率的一个平衡选择。



#### **问：为什么变成红黑树阀值是8呢?**

答：jdk作者选择8，是经过了严格的运算，觉得在长度为8的时候，与其保证链表结构的查找开销，不如转换为红黑树，改为维持其平衡开销。



#### 问：ConcurrentHashMap 和 Hashtable 的区别

`ConcurrentHashMap` 和 `Hashtable` 的区别主要体现在实现线程安全的方式上不同。

- **底层数据结构**： JDK1.7 的 `ConcurrentHashMap` 底层采用 **分段的数组+链表** 实现，JDK1.8 采用的数据结构跟 `HashMap1.8` 的结构一样，是数组+链表/红黑二叉树。而 `Hashtable` 数组+链表组成的，数组是主体，链表则是主要为了解决哈希冲突而存在的。
- **实现线程安全的方式**：`ConcurrentHashMap` 在JDK1.7时采用分段锁实现并发安全，在JDK1.8时并发控制使用 `synchronized` 和CAS来操作。而 `Hashtable` 是利用 `synchronized` 来实现一把全表锁保证线程安全，效率比较低下。











#### **问：Collection 的划分、List、Set、Map的区别**

首先介绍 `Collection` 接口下的容器：

（1）List

- `ArrayList`：`Object[]` 数组
- `Vector`：`Object[]` 数组
- `LinkedList`：双向链表

（2）Set

- `HashSet`（无序，唯一）：基于`HashMap` 实现的，底层采用 `HashMap` 来保存元素
- `LinkedHashSet`：`LinkedHashSet` 是 `HashSet` 的子类，并且其内部是通过 `LinkedHashMap` 来实现的。
- `TreeSet`(有序，唯一)：红黑树

（3）Queue

- `PriorityQueue`：`Object[]` 数组来实现二叉堆
- `ArrayQueue`：`Object[]` 数组 + 双指针

接下是 `Map` 接口下的容器：

- `HashMap`： JDK1.8 之前 `HashMap` 由数组+链表组成的，数组是 `HashMap` 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。JDK1.8 以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间
- `LinkedHashMap`： `LinkedHashMap` 继承自 `HashMap`，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，`LinkedHashMap` 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。
- `Hashtable`： 数组+链表组成的，数组是 `Hashtable` 的主体，链表则是主要为了解决哈希冲突而存在的
- `TreeMap`： 红黑树（自平衡的排序二叉树）



#### **问：arrayList和LinkedList的区别**

首先来讲讲 `ArrayList` 和 `Vector` 两者都是 `List` 的实现类，且底层使用 `Object[]` 存储，区别在于 `ArrayList` 查找高效却线程不安全，而 `Vector` 支持线程的同步故是线程安全，当然效率相对较低。

接着比较一下 `ArrayList` 和 `LinkedList` ，主要从以下方面来看：

1. 是否保证线程安全：两者都是线程不同步的，也即是不保证线程安全；
2. 底层数据结构：`ArrayList` 底层适用 `Object[]` 数组，而 `LinkedList` 则是双向链表；
3. 查找、插入和删除是否受元素位置的影响：`ArrayList` 支持随机元素访问，但是`LinkedList` 的首尾插入和删除高效。
4. 内存空间占用：`ArrayList` 的空间浪费主要体现在在 list 列表的结尾会预留一定的容量空间，而 LinkedList 的空间花费则体现在它的每一个元素需要保存索消耗空间。

可见两者的区别主要是取绝于底层数据结构的特性



#### **问：arraylist扩容，为什么是1.5倍**

先从 `ArrayList` 的构造函数谈起，分有参和无参两种情况。有参构造情况下，会直接创建对应大小的数组空间；无参构造时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为 10。

随着数据量增加，数组当前容量小于所需的容量时，则会调用 `ArrayList` 扩容的核心方法 `grow()` ，它会计算新容量为数组原本容量的1.5倍左右，并且做两重检查：一是新容量是否大于 `minCapacity` （最小需要容量），若还是小于 `minCapacity` ，那么就把 `minCapacity` 当作数组的新容量；二是 `minCapacity` 是否大于 `MAX_ARRAY_SIZE`（可分配的最大容量），若是则新容量则为`Integer.MAX_VALUE`，否则新容量大小则为 `MAX_ARRAY_SIZE`。在确定数组新容量后，则调用 `Arrays.copyof()` 申请一个新的数组，将数据拷贝。

此外，ArrayList 添加大量元素之前，用户也可以主动使用`ensureCapacity` 方法，以减少增量重新分配的次数。



​	

**问：ArrayDeque 与 LinkedList 的区别**

#### **问：HashMap 和 Hashtable 的区别**

- 线程是否安全： `HashMap` 是非线程安全的，`Hashtable` 是线程安全的,因为 `Hashtable` 内部的方法基本都经过`synchronized` 修饰。--> 顺带推出前者效率会比后者更高
- 初始容量大小和扩容机制不同
- 底层数据不同





#### 问：ConcurrentHashMap 

**JDK1.7** 

==JDK1.7==

ConcurrentHashMap由Segment(分段锁)数组结构和HashEntry数组组成，Segment是一种可重入锁，一个Segment中包含一个HashEntry数组，每个HashEntry又是一个链表结构，有点类似HashMap。

因此在ConcurrentHashMap查询一个元素的过程需要进行两次Hash操作，如下所示：

- 第一次Hash定位到Segment，
- 第二次Hash定位到元素所在的链表的头部

而同样它的插入操作则是先计算put 的 key 的位置，获取指定位置的 Segment，如果指定位置的 Segment 为空，则初始化这个 Segment，Segment.put 插入 key,value 值。

ConcurrentHashMap 的扩容是仅仅和每个Segment元素中HashEntry数组的长度有关，并且只扩容当前Segment中HashEntry数组。扩容只会扩容到原来的两倍。老数组里的数据移动到新的数组时，位置要么不变，要么变为 index+ oldSize，参数里的 node 会在扩容之后使用链表**头插法**插入到指定位置。

正是通过Segment分段锁技术，将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问，能够实现真正的并发访问。

==JDK1.8==

ConcurrentHashMap 它取消了 Segment 分段锁，数据结构是数组+链表/红黑二叉树。当冲突链表达到一定长度时，链表会转换成红黑树。采用 CAS 和 synchronized 来保证并发安全。

而在JDK1.8中只需要一次定位，并且采用CAS+synchronized的机制。如果对应下标处没有结点，说明没有发生哈希冲突，此时直接通过CAS进行插入，若成功，直接返回。若失败，则使用synchronized进行加锁写入。

链接：https://zhuanlan.zhihu.com/p/237295675

**为什么 key 和 value 不允许为 null** ：

1. 在并发编程中，==null 值容易引来歧义==， 假如先调用 `get(key)` 返回的结果是 null，那么我们无法确认是因为当时这个 key 对应的 value 本身放的就是 null，还是说这个 key 值根本不存在，这会引起歧义，==如果在非并发编程中，可以进一步通过调用 containsKey 方法来进行判断，但是并发编程中无法保证两个方法之间没有其他线程来修改 key 值，所以就直接禁止了 null 值的存在==。

参考文章：[源码](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/concurrent-hash-map-source-code?id=_1-concurrenthashmap-17)







## Java并发

#### **问：synchronized关键字**

答：synchronized 关键字解决的是多个线程之间访问资源的同步性，synchronized关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。

synchronized关键字最主要的三种使用方式：

1. 修饰实例方法：作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁；
2. 修饰静态方法：相当于给当前类加锁，会作用类的所有对象实例，进入同步代码前要获得当前class的锁；
3. 修饰代码块：指定加锁对象，对给定对象/类加锁。`synchronized(this | object)` 表示进入同步代码库前要获得给定对象的锁。`synchronized(类.class)` 表示进入同步代码前要获得当前class的锁。

synchronized关键字的底层原理：

情况一：synchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。

​	① 在执行`monitorenter`时，会尝试获取对象的锁，如果锁的计数器为 0 则表示锁可以被获取，获取后将锁计数器设为 1 也就是加 1。

​	② 在执行 `monitorexit` 指令后，将锁计数器设为 0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。

情况二：synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法。

不过两者的本质都是对对象监视器 monitor 的获取。

参考文章：[JavaGuide](https://snailclimb.gitee.io/javaguide/#/docs/java/concurrent/java%E5%B9%B6%E5%8F%91%E8%BF%9B%E9%98%B6%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93)



#### **问：volatile 关键字**

答：因为Java内存模型下，线程可以把变量保存本地内存中，而不是直接在主存中进行读写。这就可能在主存中修改了一个变量的值，而另外一个线程还继续使用它在寄存器中的变量值的拷贝，造成数据的不一致。

要解决这个问题，就需要把变量声明为 volatile ，这就指示 JVM，这个变量是共享且不稳定的，每次使用它都到主存中进行读取。所以volatile 关键字 除了防止 JVM 的指令重排 ，还有一个重要的作用就是保证变量的可见性。



#### **问：并发编程的三个重要特性**

1. **原子性** : 一个的操作或者多次操作，要么所有的操作全部都得到执行并且不会收到任何因素的干扰而中断，要么所有的操作都执行，要么都不执行。`synchronized` 可以保证代码片段的原子性。
2. **可见性** ：当一个线程对共享变量进行了修改，那么另外的线程都是立即可以看到修改后的最新值。`volatile` 关键字可以保证共享变量的可见性。
3. **有序性** ：代码在执行的过程中的先后顺序，Java 在编译器以及运行期间的优化，代码的执行顺序未必就是编写代码时候的顺序。`volatile` 关键字可以禁止指令进行重排序优化。



#### 问：ThreadLocal

==ThreadLocal的作用主要是做数据隔离，提供线程内部的局部变量，并且能保证各个线程里的变量相对独立于其他线程内的变量。==

内存泄漏：

每个 Thread 类里面都有一个 ThreadLocalMap，而 ThreadLocalMap 中真正承载数据的是一个 Entry 数组，Entry 的 Key 是 threadlocal 对象的弱引用。

实际开发中，当我们不需要 threadlocal 后，为了 GC 将 threadlocal 变量置为 null，没有任何强引用指向堆中的 threadlocal 对象时，堆中的 threadlocal 对象将会被 GC 回收。

如果此时当前线程还在运行，那么 ==Entry 中的 key 为 null 的 Value 对象并不会被回收（存在强引用），这就发生了内存泄漏==，当然这种内存泄漏分情况，如果当前线程执行完毕会被回收，那么 Value 自然也会被回收，但是如果使用的是线程池呢，线程跑完任务以后放回线程池（线程没有销毁，不会被回收），Value 会一直存在，这就发生了内存泄漏。

**降低内存泄漏的风险**：当前线程使用完 threadlocal 后，我们可以通过调用 ThreadLocal 的 remove 方法进行清除从而降低内存泄漏的风险。

![image-20220307154012483](C:\Users\Lenovo\Desktop\笔记\CS-Learning-Notes\notes\pics\image-20220307154012483.png)

在每个线程中都有一个类型为`ThreadLocal.ThreadLocalMap`的实例变量`threadLocals`，也就是说每个线程有一个自己的`ThreadLocalMap`。

`ThreadLocalMap`有自己的独立实现，其中的`key`是`ThreadLocal`的一个弱引用，`value`为代码中放入的值。

每个线程在往`ThreadLocal`里放值的时候，都会往自己的`ThreadLocalMap`里存，读也是以`ThreadLocal`作为引用，在自己的`map`里找对应的`key`，从而实现了**线程隔离**。

`ThreadLocalMap`有点类似`HashMap`的结构，只是`HashMap`是由**数组+链表**实现的，而`ThreadLocalMap`中并没有**链表**结构。

`Java`的**四种引用类型**：

- **强引用**：我们常常 new 出来的对象就是强引用类型，只要强引用存在，垃圾回收器将永远不会回收被引用的对象，哪怕内存不足的时候
- **软引用**：使用 SoftReference 修饰的对象被称为软引用，软引用指向的对象在内存要溢出的时候被回收
- **弱引用**：使用 WeakReference 修饰的对象被称为弱引用，只要发生垃圾回收，若这个对象只被弱引用指向，那么就会被回收
- **虚引用**：虚引用是最弱的引用，在 Java 中使用 PhantomReference 进行定义。虚引用中唯一的作用就是用队列接收对象即将死亡的通知



参考文章：https://juejin.cn/post/6844904165383110664

https://juejin.cn/post/7042211997743579144#heading-4



#### **问：Executor框架的使用**

使用线程池可以降低资源消耗、提高响应速度以及提高线程的可管理性。

Executor框架结构由三大部分组成：

- 任务（Runnable / Callable）：执行任务需要实现的 Runnable 接口 或 Callable接口。Runnable 接口或 Callable 接口 实现类都可以被 ThreadPoolExecutor 或 ScheduledThreadPoolExecutor 执行;
- 任务的执行（Executor）：使用最多的是 ThreadPoolExecutor;
- 异步计算的结果（Future）：Future 接口以及 Future 接口的实现类 FutureTask 类都可以代表异步计算的结果。

![Executor框架的使用示意图](pics\Executor框架的使用示意图.png)

Executor框架的使用：

1. 主线程首先创建一个实现了Runnable 或者 Callable 接口的任务对象；
2. 把任务对象直接交给 ExecutorService 执行
3. 如果执行 `ExecutorService.submit(..)`，ExecutorService 将返回一个实现了Future 接口的对象；
4. 最后，主线程可以执行 `FutureTask.get()` 方法来等待执行完成。



#### **问：介绍ThreadPoolExecutor函数参数**

ThreadPoolExecutor 3 个最重要的参数：

- corePoolSize : 核心线程数线程数定义了最小可以同时运行的线程数量。
- maximumPoolSize : 当队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。
- workQueue: 当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。

ThreadPoolExecutor其他常见参数:

- keepAliveTime：当线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁；
- unit：keepAliveTime 参数的时间单位。
- threadFactory：executor 创建新线程的时候会用到。
- handler：如果当前同时运行的线程数量达到最大线程数量并且队列也已经被放满了任务时，采用的策略。



#### **问：ThreadPoolExecutor 饱和策略**

- ThreadPoolExecutor.AbortPolicy ：抛出 RejectedExecutionException来拒绝新任务的处理。
- ThreadPoolExecutor.CallerRunsPolicy ：调用执行自己的线程运行任务，如果执行程序已关闭，则会丢弃该任务。
- ThreadPoolExecutor.DiscardPolicy ：不处理新任务，直接丢弃掉。
- ThreadPoolExecutor.DiscardOldestPolicy ： 此策略将丢弃最早的未处理的任务请求。



#### **问：ThreadPoolExecutor执行流程**

1. 提交一个任务，线程池里存活的核心线程数小于线程数corePoolSize时，线程池会创建一个核心线程去处理提交的任务。
2. 如果线程池核心线程数已满，即线程数已经等于corePoolSize，一个新提交的任务，会被放进任务队列workQueue排队等待执行。
3. 当线程池里面存活的线程数已经等于corePoolSize了，并且任务队列workQueue也满，判断线程数是否达到maximumPoolSize，即最大线程数是否已满，如果没到达，创建一个非核心线程执行提交的任务。
4. 如果当前的线程数达到了maximumPoolSize，还有新的任务过来的话，直接采用拒绝策略处理。



#### **问：介绍线程池的工作队列**

（1）ArrayBlockingQueue（有界队列）：一个用数组实现的有界阻塞队列，按FIFO排序量

（2）LinkedBlockingQueue（可设置容量队列）：基于链表结构的阻塞队列，按FIFO排序任务，容量可以选择进行设置，不设置的话，将是一个无边界的阻塞队列，最大长度为Integer.MAX_VALUE；newFixedThreadPool线程池使用了这个队列。

（3）DelayQueue（延迟队列）：一个任务定时周期的延迟执行的队列。根据指定的执行时间从小到大排序，否则根据插入到队列的先后排序。newScheduledThreadPool线程池使用了这个队列。

（4）PriorityBlockingQueue（优先级队列）：一个具有优先级的无界阻塞队列。

（5）SynchronousQueue（同步队列）：一个不存储元素的阻塞队列，每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于LinkedBlockingQuene。newCachedThreadPool线程池使用了这个队列。



#### **问：介绍几种常见的线程池**

（1）newFixedThreadPool：固定数目线程的线程池

特点：核心线程数和最大线程数大小一样，没有所谓的非空闲时间，工作队列为LinkedBlockingQueue。

工作机制：提交任务后，如果线程数少于核心线程，创建核心线程执行任务；如果线程数等于核心线程，把任务添加到LinkedBlockingQueue阻塞队列；如果线程执行完任务，去阻塞队列取任务，继续执行。

使用场景：FixedThreadPool适用于处理CPU密集型的任务，确保CPU在长期被工作线程使用的情况下，尽可能地少分配线程，即适用执行长期地任务

（2）newCachedThreadPool：可缓存线程的线程池

特点：核心线程数为0，最大线程数为Integer.MAX_VALUE，工作队列是SynchronousQueue。

工作机制：提交任务后，因为没有核心线程，所以任务直接加到SynchronousQueue队列，判断是否有空闲线程。如果有，就去取出任务执行；如果没有空闲线程，就新建一个线程执行。执行完任务的线程，还可以存活60秒，如果在这期间，接到任务，可以继续活下去；否则，被销毁。

使用场景：用于并发执行大量短期的小任务。

（3）newSingleThreadExecutor：单线程的线程池

特点：核心线程数和最大线程数都为1，工作队列为LinkedBlockingQueue

工作机制：提交任务，判断线程是否存在，如果没有则创建一个新线程，如果由则加入阻塞队列。唯一线程依次执行。

使用场景：适用于串行执行任务的场景，一个任务一个任务地执行。

（4）newScheduledThreadPool：定时及周期执行的线程池

 主要用来在给定的延迟后运行任务，或者定期执行任务。任务队列使用的是DelayQueue。每次线程从 DelayQueue 中获取 time 大于等于当前时间的任务。

使用场景：周期性执行任务的场景，需要限制线程数量的场景。



#### **问：线程池状态有哪些？**

![线程池状态.png](pics\线程池状态-4872074e2f9640688171e9d577315a04.png)

RUNNING

- 该状态的线程池会接收新任务，并处理阻塞队列中的任务;
- 调用线程池的shutdown()方法，可以切换到SHUTDOWN状态;
- 调用线程池的shutdownNow()方法，可以切换到STOP状态;

SHUTDOWN

- 该状态的线程池不会接收新任务，但会处理阻塞队列中的任务；
- 队列为空，并且线程池中执行的任务也为空,进入TIDYING状态;

STOP

- 该状态的线程不会接收新任务，也不会处理阻塞队列中的任务，而且会中断正在运行的任务；
- 线程池中执行的任务为空,进入TIDYING状态;

TIDYING

- 该状态表明所有的任务已经运行终止，记录的任务数量为0。
- terminated()执行完毕，进入TERMINATED状态

TERMINATED

- 该状态表示线程池彻底终止



#### **问：何为 AQS以及AQS 原理？**

答：AQS （AbstractQueuedSynchronizer，抽象队列同步器）是一个抽象的队列式的同步器，主要用来构建锁和同步器。

AQS中 维护了一个`volatile int state`（代表共享资源）和一个`FIFO`线程等待队列CLH，当并发的线程争用资源被阻塞时会进入此队列。

这里`volatile`能够保证多线程下的可见性，当`state=1`则代表当前对象锁已经被占有，其他线程来加锁时则会失败，加锁失败的线程会被放入一个`FIFO`的等待队列中，比列会被`UNSAFE.park()`操作挂起，等待其他获取锁的线程释放锁才能够被唤醒。而底层利用了 CAS 机制来保证`state`并发修改操作的原子性。

> CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS 是将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点（Node）来实现锁的分配。

<img src="pics\CLH-954c81e049174c2d9b15720c6b31afae.png" alt="CLH.png" style="zoom: 67%;" />

参考文章：[aqs原理以及aqs同步组件总结](https://snailclimb.gitee.io/javaguide/#/docs/java/concurrent/aqs%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8Aaqs%E5%90%8C%E6%AD%A5%E7%BB%84%E4%BB%B6%E6%80%BB%E7%BB%93) 、[AQS](https://mp.weixin.qq.com/s/hB5ncpe7_tVovQj1sNlDRA) 、[我画了35张图就是为了让你深入 AQS](https://mp.weixin.qq.com/s/trsjgUFRrz40Simq2VKxTA)



#### **问：AQS对资源的共享方式**

（1）Exclusive（独占）

只有一个线程能执行，如 ReentrantLock。又可分为公平锁和非公平锁，ReentrantLock 同时支持两种锁，下面以 ReentrantLock 对这两种锁的定义做介绍：

- 公平锁 ：按照线程在队列中的排队顺序，先到者先拿到锁
- 非公平锁 ：当线程要获取锁时，先通过两次 CAS 操作去抢锁，如果没抢到，当前线程再加入到队列中等待唤醒。

 ReentrantLock 中相关的源代码中公平锁和非公平锁只有两处不同：

1. 非公平锁在调用 lock 后，首先就会调用 CAS 进行一次抢锁，如果这个时候恰巧锁没有被占用，那么直接就获取到锁返回了。
2. 非公平锁在 CAS 失败后，和公平锁一样都会进入到 tryAcquire 方法，在 tryAcquire 方法中，如果发现锁这个时候被释放了（state == 0），非公平锁会直接 CAS 抢锁，但是公平锁会判断等待队列是否有线程处于等待状态，如果有则不去抢锁，乖乖排到后面。

公平锁和非公平锁就这两点区别，如果这两次 CAS 都不成功，那么后面非公平锁和公平锁是一样的，都要进入到阻塞队列等待唤醒。

相对来说，非公平锁会有更好的性能，因为它的吞吐量比较大。当然，非公平锁让获取锁的时间变得更加不确定，可能会导致在阻塞队列中的线程长期处于饥饿状态。

（2）Share（共享）

多个线程可同时执行，如 Semaphore/CountDownLatch。Semaphore、CountDownLatCh、 CyclicBarrier、ReadWriteLock 等。

ReentrantReadWriteLock 可以看成是组合式，因为 ReentrantReadWriteLock 也就是读写锁允许多个线程同时对某一资源进行读。

不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源 state 的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS 已经在上层已经帮我们实现好了。

AQS底层提供的模板方法：

```java
isHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。 

tryAcquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。

tryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。 

tryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 

tryReleaseShared(int)//共享方式。尝试释放资源，成功则返回true，失败则返回false。
```



#### **问：ReentrantLock 的底层可重入的原理**

ReentrantLock是个典型的独占模式AQS，同步状态为0时表示空闲。当有线程获取到空闲的同步状态时，它会将同步状态加1，将同步状态改为非空闲，于是其他线程挂起等待。

在修改同步状态的同时，并记录下自己的线程，作为后续重入的依据，即一个线程持有某个对象的锁时，再次去获取这个对象的锁是可以成功的。如果是不可重入的锁的话，就会造成死锁。



ReentrantLock 也属于一种 AQS 同步器。state 初始化为 0，表示未锁定状态。A 线程 `lock()` 时，会调用 `tryAcquire()`独占该锁并将 state+1。此后，其他线程再 `tryAcquire()` 时就会失败，直到 A 线程 unlock()到 state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A 线程自己是可以重复获取此锁的（state 会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证 state 是能回到零态的。

参考文章：https://juejin.cn/post/6844903805683761165#heading-7





#### **问：介绍 Semaphore及其应用场景**

答：Semaphore 信号量，用来控制同一时间，资源可被访问的线程数量，一般可用于流量的控制。

Semaphore 维持了一个可获得许可证的数量。当一个线程执行 `acquire()` 方法会阻塞，直到有一个许可证可以获得然后拿走一个许可证；而 `release()` 时则会归还许可证。这属于共享锁的一种实现。

此外Semaphore 有两种模式，公平模式和非公平模式。

- 公平模式： 调用 `acquire()` 方法的顺序就是获取许可证的顺序，遵循 FIFO；
- 非公平模式： 抢占式的。



#### **问：介绍CountDownLatch及其应用场景**

答：CountDownLatch 倒计时器，允许 count 个线程阻塞在一个地方，直至所有线程的任务都执行完毕。

CountDownLatch 是共享锁的一种实现,它默认构造 AQS 的 state 值为 count。当线程使用 `countDown()` 方法时,其实使用了`tryReleaseShared()`方法以 CAS 的操作来减少 state,直至 state 为 0 。当调用 `await()` 方法的时候，如果 state 不为 0，那就证明任务还没有执行完毕，`await()` 方法就会一直阻塞，也就是说 `await()` 方法之后的语句不会被执行。然后，CountDownLatch 会自旋 CAS 判断 `state == 0`，如果 `state == 0` 的话，就会释放所有等待的线程，`await()` 方法之后的语句得到执行。

CountDownLatch 的典型用法有两种：

- 某一线程在开始运行前等待 n 个线程执行完毕。

  将 CountDownLatch 的计数器初始化为 n （`new CountDownLatch(n)`），每当一个任务线程执行完毕，就将计数器减 1 （`countdownlatch.countDown()`），当计数器的值变为 0 时，在 `CountDownLatch 上 await()` 的线程就会被唤醒。一个典型应用场景就是启动一个服务时，主线程需要等待多个组件加载完毕，之后再继续执行。

- 实现多个线程开始执行任务的最大并行性。

CountDownLatch 的不足：CountDownLatch 是一次性的，计数器的值只能在构造方法中初始化一次，之后没有任何机制再次对其设置值，当 CountDownLatch 使用完毕后，它不能再次被使用。



#### **问：介绍CyclicBarrier 及其应用场景**

答：CyclicBarrier 循环栅栏，和CountDownLatch 非常类似，可以实现一组线程互相等待，直到所有线程都到达一个同步点。

应用场景：CyclicBarrier 可以用于多线程计算数据，最后合并计算结果的应用场景。

CyclicBarrier 和CountDownLatch的区别：

1. CountDownLatch 的实现是基于 AQS 的，而 CycliBarrier 是基于 ReentrantLock和 Condition 的。
2. CountDownLatch 是计数器，只能使用一次，而 CyclicBarrier 的计数器提供 reset 功能，可以多次使用。
3. 对于 CountDownLatch 来说，重点是“一个线程（多个线程）等待”，而其他的 N 个线程在完成“某件事情”之后，可以终止，也可以等待。而对于 CyclicBarrier，重点是多个线程，在任意一个线程没有完成，所有的线程都必须等待。



#### **问：什么是 CAS 及其局限性？**

CAS (Compare and Swap)  比较并交换，是解决多线程并行情况下使用锁造成性能损耗的一种机制，CAS 操作包含三个操作数——内存位置(V)、预期原值(A)和新值(B)。

如果内存位置的值(V)与预期原值(A)相匹配，那么处理器会自动将该位置值更新为新值(B)。否则，处理器不做任何操作。无论哪种情况，它都会在 CAS 指令之前返回该位置的值。

CAS 有效地说明了“我认为位置(V)应该包含值(A)。如果包含该值，则将新值(B)放到这个位置；否则，不要更改该位置，只告诉我这个位置现在的值即可”。

CAS的底层：当多个线程尝试使用 CAS 同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。举个扣减库存的例子，通过乐观锁的实现如下：

```sql
// 查询出商品信息，得quantity = 3,即id=1的商品数量为3
select quantity from items where id = 1;
// 当且仅当该商品的数量依然为3时，修改商品库存为2
update items set quantity = 2 where id = 1 and quantity = 3;
```

<img src="C:/Users/Lenovo/Desktop/笔记/CS-Learning-Notes/notes/pics/7038163-8ca34c3adccb7ca6.webp" style="zoom:67%;" />

CAS存在ABA问题：如果另一个线程修改V值假设原来是A，先修改成B，再修改回成A。当前线程的CAS操作无法分辨当前V值是否发生过变化。

一个比较好的解决方法，就是通过一个单独的可以顺序递增的version字段。优化如下：

<img src="C:/Users/Lenovo/Desktop/笔记/CS-Learning-Notes/notes/pics/7038163-a23fb455b893f97f.webp" style="zoom:67%;" />

乐观锁每次在执行数据修改操作时，都会带上一个版本号，一旦版本号和数据的版本号一致就可以执行修改操作并对版本号执行 +1 操作，否则就执行失败。因为每次操作的版本号都会随之增加，所以不会出现 ABA 问题。除了 version 以外，还可以使用时间戳，因为时间戳天然具有顺序递增性。





## Java虚拟机

#### **问：JVM 运行时数据区域**

<img src="C:\Users\Lenovo\Desktop\笔记\CS-Learning-Notes\notes\pics\16be51956d5c3662~tplv-t2oaga2asx-watermark.awebp" alt="img" style="zoom: 33%;" />

答：



#### **问：如何判断对象是否死亡**

答：GC 主要发生在堆，判断一个对象是否死亡的方式有两种：

（1）引用计数法

给对象添加一个引用计数器，每当有一个地方引用它，计数器加1；当引用失效，计数器就减1；当计数器为0时，说明对象不再被引用，可以被可回收。

缺点：如果对象存在循环依赖，那就无法定位该对象是否应该被回收（A依赖B，B依赖A）

（2）可达性分析算法

这个算法就是通过一系列的称为「GC Roots」的对象作为起点，从这些节点开始向下搜索，当对象到「GC Roots」都没有任何引用相连时，说明对象是不可用的，可以被回收。

可以作为 GC Root 的对象有以下几类：

- 虚拟机栈（栈帧中的本地变量表）中引用的对象（int a = new Test();）
- 方法区中类静态属性引用的对象 
- 方法区中常量引用的对象
- 本地方法栈中 JNI（即一般说的 Native 方法）引用的对象

参考文章：[看完这篇垃圾回收，和面试官扯皮没问题了](https://mp.weixin.qq.com/s/_AKQs-xXDHlk84HbwKUzOw)



#### **问：GC 的几种主要的收集方法及其优缺点**

答：（1）标记-清除算法

该算法分为标记和清除阶段：首先标记出所有不需要回收的对象，在标记完成后统一回收掉所有没有被标记的对象。它是最基础的收集算法，后续的算法都是对其不足进行改进得到。

这种垃圾收集算法会带来两个明显的问题：一是效率问题，二是空间问题（标记清除后会产生大量不连续的碎片）。

（2）标记-复制算法

为了解决效率问题，标记-复制收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。

标记-复制收集算法的问题也很明显，可用空间直接少了一半，另外每次回收也要把存活对象移动到另一半，效率低下。

（3）标记-整理算法

前面两步和标记清除法一样，不同的是它在标记-清除算法的基础上添加了一个整理的过程，即把所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。

这算法缺点也很明显：每进一次垃圾清除都要频繁地移动存活的对象，效率十分低下

（4）分代收集算法

==分代算法是根据对象存活周期的不同将内存分为几块==。一般将 java 堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。

比如在新生代中，每次收集都会有大量对象死去，所以可以选择”标记-复制“算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。

#### 问：分代收集工作原理

（问：讲一下内存分配策略？）

新生代：老年代 = 1 : 2；

minor GC：从年轻代（Eden+Survivor）空间回收内存被称为Minor GC；

Major是清理永久代。Full Gc是清理整个堆空间-包括年轻代和永久代；

年轻代：使用标记-复制算法进行清理；

老年代：标记-整理算法：标记处仍然存活的对象，将所有存活的对象向一端移动，以保内存的连续。

1. 对象在新生代的分配与回收

   - Eden: S0: S1 = 8:1:1；
   - 大部分对象在很短的时间内都会被回收，对象一般分配在 Eden 区
   - 当 Eden 区将满时，触发 Minor GC
   - 在 Eden 区的垃圾回收我们采用的是**标记 - 复制算法**

2. 对象何时晋升老年代

   - 当对象的年龄达到了我们设定的阈值，则会从S0（或S1）晋升到老年代。
   - 大对象减少复制开销也放在老年代
   - 相同年龄的对象大于S0空间的一半，更老的去老年代

3. 空间分配担保

   空间分配担保就是担保**老年代的内存足够多，新生代中的对象能够存放**

   - 在发生Minor GC 之前，虚拟机必须先检查

     ==老年代最大可用得连续空间是否大于新生代所有对象总空间或者历次晋升的平均大小==

     JDK 6 Update 24之后不再使用-XX：HandlePromotionFailure参数（是否允许担保失败）

     - 如果条件成立，那么这一次Minor GC就是安全的。

     - 如果不成立，则虚拟机会先查看 -XX:HandlePromotionFailure参数的设置值是否允许担保失败；

       - 如果允许，那会继续检查

         老年代最大可用的连续空间是否大于历次晋升到老年代的平均大小

         - 如果大于，将尝试进行YGC，尽管这次YGC有风险
         - 如果小于，那么就需要进行Full GC

       - 如果不允许，则进行Full GC

   谁进行空间担保：老年代需要能存放年轻代中的对象，所以是**老年代进行空间分配担保**

   ==目的：避免FullGC过于频繁==

4. Stop The World

   如果老年代满了，会触发 Full GC, Full GC 会同时回收新生代和老年代（即对整个堆进行GC），它会导致 Stop The World（简称 STW）,造成挺大的性能开销。

   什么是 STW ？所谓的 STW, 即在 GC（minor GC 或 Full GC）期间，只有垃圾回收器线程在工作，其他工作线程则被挂起。

   合理设置新生代与老年代的空间大小比例就是尽可能地避免对象过早地进入老年代，尽可能晚地触发 Full GC。

   由于 Full GC（或Minor GC） 会影响性能，所以我们要在一个合适的时间点发起 GC，这个时间点被称为 Safe Point，这个时间点的选定既不能太少以让 GC 时间太长导致程序过长时间卡顿，也不能过于频繁以至于过分增大运行时的负荷。一般当线程在这个时间点上状态是可以确定的，如确定 GC Root 的信息等，可以使 JVM 开始安全地 GC。Safe Point 主要指的是以下特定位置：

   - 循环的末尾
   - 方法返回前
   - 调用方法的 call 之后
   - 抛出异常的位置 另外需要注意的是由于新生代的特点（大部分对象经过 Minor GC后会消亡）， Minor GC 用的是复制算法，而在老生代由于对象比较多，占用的空间较大，使用复制算法会有较大开销（复制算法在对象存活率较高时要进行多次复制操作，同时浪费一半空间）所以根据老生代特点，在老年代进行的 GC 一般采用的是标记整理法来进行回收。



#### **问：垃圾收集器种类**

<img src="pics\垃圾回收期分类.png" alt="图片" style="zoom:33%;" />

- 在新生代工作的垃圾回收器：Serial, ParNew, ParallelScavenge
- 在老年代工作的垃圾回收器：CMS，Serial Old, Parallel Old
- 同时在新老生代工作的垃圾回收器：G1



- Serial 收集器，串行收集器是最古老，最稳定以及效率高的收集器，可能会产生较长的停顿，只使用一个线程去回收。
- ParNew 收集器，ParNew 收集器其实就是 Serial 收集器的多线程版本。
- Parallel 收集器，Parallel Scavenge 收集器类似 ParNew 收集器，Parallel 收集器更关注系统的吞吐量。
- Parallel Old 收集器，Parallel Old 是 Parallel Scavenge 收集器的老年代版本，使用多线程和“标记－整理”算法
- CMS 收集器，CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。
- G1 收集器，G1 (Garbage-First)是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征

（1）Serial 收集器

Serial 收集器只使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ "Stop The World" ），直到它收集结束。

新生代采用标记-复制算法，老年代采用标记-整理算法。

（2）ParNew 收集器

ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和 Serial 收集器完全一样。

新生代采用标记-复制算法，老年代采用标记-整理算法。

（3）Parallel Scavenge 收集器

Parallel Scavenge 收集器也是使用标记-复制算法的多线程收集器，与CMS等垃圾回收器的关注点更多关注点是用户线程的停顿时间（提高用户体验）不同，Parallel Scavenge 收集器关注点是吞吐量（高效率的利用 CPU），所以更适合做后台运算等不需要太多用户交互的任务。

新生代采用标记-复制算法，老年代采用标记-整理算法。

（4）Serial Old 收集器

Serial 收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种用途是在 JDK1.5 以及以前的版本中与 Parallel Scavenge 收集器搭配使用，另一种用途是作为 CMS 收集器的后备方案。

（5）Parallel Old 收集器

Parallel Scavenge 收集器的老年代版本。使用多线程和“标记-整理”算法。在注重吞吐量以及 CPU 资源的场合，都可以优先考虑 Parallel Scavenge 收集器和 Parallel Old 收集器。

（6）CMS 收集器

用Seria 和 parallel 系列的垃圾收集器，在垃圾回收的时候，用户线程都会完全停止，直到垃圾收集完成。

CMS对比上面的垃圾收集器，最大的特点就是并发：==在GC线程工作的时候，用户线程「不会完全停止」，用户线程在「部分场景下」与GC线程一起并发执行。==

CMS（Concurrent Mark Sweep）收集器是以实现最短 STW 时间为目标的收集器。

老年代主要用标记整理法，而 CMS 虽然工作于老年代，但采用的是==标记清除法==。

主要有以下四个步骤：

- **初始标记：** 暂停所有的其他线程，并记录下直接与GCRoots 相连的对象，速度很快 ；
- **并发标记：** 同时开启 GC 和用户线程，主要是从GC Roots向下「追溯」，标记所有可达的对象。因为用户线程可能会不断的更新引用域，所以 GC 线程无法保证可达性分析的实时性。所以这个算法里会==跟踪记录这些发生引用更新的地方==。
- **重新标记：** 重新标记阶段就是为了==修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录==，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短
- **并发清除：** 开启用户线程，同时 GC 线程开始对未标记的区域做清扫。

CMS是一款优秀的垃圾收集器，主要优点：**并发收集、低停顿**。但是它有下面三个明显的缺点：

- 对 CPU 资源敏感；
- 无法处理浮动垃圾；
- 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。



（7）G1（Garbage First） 收集器

G1又可以理解为在CMS垃圾收集器上进行”升级”。

G1 可以面向堆内存任何部分来进行垃圾回收，并且建立了一个可预测的停顿时间模型来满足用户的停顿时间期望。

在G1中堆被逻辑划分成多个大小相等的区域叫做Region，方便控制它的收集时间。回收的衡量标准是哪块内存中垃圾数量最多，回收收益最大。

初始标记：仅仅标记一下 GC Roots 能直接关联的对象，并修改 TAMS 指针的值。该阶段需停顿线程，但耗时很短，而且是借进行 Minor GC 时同步完成的，实际上并没有额外的停顿

并发标记：从 GC Roots 开始对堆中对象进行可达性分析，找出要回收对象。该阶段耗时较长，但可与用户程序并发执行。当扫描完成后，还要重新处理 SATB 记录下在并发时有引用变动的对象

最终标记：用户线程短暂暂停，处理并发阶段结束后遗留下来的少量更新对象的记录

筛选回收：==更新 Region 数据，对各个 Region 的回收价值和成本进行排序，根据用户所期望的停顿时间制定回收计划，然后把要回收的那一部分 Region 的存活对象复制到空的 Region 中，再清理掉整个旧 Region 的全部空间。这里涉及到存活对象的移动，必须暂停用户线程==

缺点：对机器要求高，耗CPU资源
链接：https://juejin.cn/post/6910450561246560269

G1 (Garbage-First) 是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征。它具备以下特点：

- 并行与并发：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。

- 分代收集：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。

- 空间整合：与 CMS 的“标记-清理”算法不同，G1 从整体来看是基于“标记-整理”算法实现的收集器；从局部上来看是基于“标记-复制”算法实现的。

- 可预测的停顿：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内。

G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来) 。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。

参考文章：[分代收集算法](https://javaguide.cn/java/jvm/jvm-garbage-collection/#_3-4-%E5%88%86%E4%BB%A3%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95) 



#### **问：介绍类加载过程**

答：Class 文件需要加载到虚拟机中之后才能运行和使用，虚拟机加载 Class 类型的文件主要三步：加载->连接->初始化。连接过程又可分为三步：验证->准备->解析。

![img](C:\Users\Lenovo\Desktop\笔记\CS-Learning-Notes\notes\pics\类加载过程-完善.png)

1. 加载：通过一个完整的类或接口名称来获得其二进制流的形式并将其按照Java虚拟机规范将数据存储到运行时数据区。

   - 通过全类名获取定义此类的==二进制字节流==
   - 将字节流所代表的==静态存储结构转换为方法区的运行时数据结构==
   - 在内存中生成一个代表该类的 `Class` 对象，作为方法区这些数据的访问入口

2. 连接：获取类或接口类型的二进制形式并将其结合到Java虚拟机的运行时状态以便执行的过程

   1. 验证：类加载进来了肯定是需要对格式做一个校验，要不然什么东西都直接放到内存里面，Java的安全性就完全无法得到保障。文件格式验证（版本号），元数据验证（字段合法），字节码验证（逻辑）
   2. 准备：准备工作是正式开始分配内存地址的一个阶段，**主要为类或接口创建静态字段(类变量和常量)，并将这些字段初始化为默认值。**
   3. 解析：虚拟机将常量池内的符号引用替换为直接引用的过程，也就是得到类或者字段、方法在内存中的指针或者偏移量。

3. 初始化：执行初始化方法 `<init> ()`方法的过程，是类加载的最后一步，这一步 JVM 才开始真正执行类中定义的 Java 程序代码(字节码)。

4. 卸载：卸载类即该类的 Class 对象被 GC。

   卸载类需要满足 3 个要求:

   1. 该类的所有的实例对象都已被 GC，也就是说堆不存在该类的实例对象。
   2. 该类没有在其他任何地方被引用
   3. 该类的类加载器的实例已被 GC

   所以，在 JVM 生命周期内，由 jvm 自带的类加载器加载的类是不会被卸载的。但是由我们自定义的类加载器加载的类是可能被卸载的。

参考文章：[类加载全过程](https://juejin.cn/post/6872955370369875976#heading-0)、 [类加载过程详解](https://javaguide.cn/java/jvm/class-loading-process/#)



#### **问：双亲委派机制**

为什么是双亲委派机制：说到双亲委派，那就必然先要说一下Java 中的类加载器，java中的类加载器有：

- Bootstrap ClassLoader 启动类加载器，默认加载的是jdk\lib目录下jar中诸多类；
- Extention ClassLoader 扩展类加载器，默认加载jdk\lib\ext\目录下jar中诸多类；
- Application ClassLoader 应用类加载器，负责加载开发人员所编写的诸多类。
- User ClassLoader 用户自定义类加载器

所谓的双亲委派机制，指的就是：**当一个类加载器收到了类加载的请求的时候，他不会直接去加载指定的类，而是把这个请求委托给自己的父加载器去加载。只有父加载器无法加载这个类的时候，子加载器才会尝试去加载这个类。**

为什么需要双亲委派机制：因为类加载器之间有严格的层次关系，那么也就使得Java类也随之具备了层次关系。这种机制好处体现在：

首先，**通过委派的方式，可以避免类的重复加载**，当父加载器已经加载过某一个类时，子加载器就不会再重新加载这个类。

另外，**通过双亲委派的方式，还保证了安全性**。因为Bootstrap ClassLoader在加载的时候，只会加载JAVA_HOME中的jar包里面的类，如java.lang.Integer，那么这个类是不会被随意替换的，除非有人跑到你的机器上， 破坏你的JDK。

双亲委派是怎么实现的：**实现双亲委派的代码都集中在java.lang.ClassLoader的loadClass()方法之中**：

1. 先检查类是否已经被加载过
2. 若没有加载则调用父加载器的loadClass()方法进行加载 
3. 若父加载器为空则默认使用启动类加载器作为父加载器。 
4. 如果父类加载失败，抛出ClassNotFoundException异常后，再调用自己的findClass()方法进行加载。

- loadClass()
  - 就是主要进行类加载的方法，默认的双亲委派机制就实现在这个方法中。
- findClass()
  - 根据名称或位置加载.class字节码
- definclass()
  - 把字节码转化为Class

如何主动破坏双亲委派机制：

1. 当我们想要自定义一个类加载器的时候，并且像破坏双亲委派原则时，继承ClassLoader类，我们会重写loadClass方法。（因为双亲委派的逻辑就写在了loadClass()方法中)）
2. 如果我们想定义一个类加载器，但是不想破坏双亲委派模型的时候，可以继承ClassLoader，并且重写findClass方法实现加载逻辑。

使用场景：因为双亲委派机制不能支持SPI(Service Provider Interface), 所以才会引入线程上下文类加载器，有了它，就可以打通双亲委派模型的层次结构来反向使用类加载器来完成类加载。

参考文章：[你确定你真的理解"双亲委派"了吗？！](https://juejin.cn/post/6916314841472991239)





#### **问：new 一个对象在堆中的历程**

对象的创建过程分五步，如下图：

<img src="C:\Users\Lenovo\Desktop\笔记\CS-Learning-Notes\notes\pics\v2-fc5cfaa4197d14310aaa5baab36c05d5_r.jpg" alt="preview" style="zoom:50%;" />

new 一个对象在堆中的过程主要分为五个步骤：

1）类加载检查：具体来说，当 Java 虚拟机遇到一条字节码 new 指令时，它会首先检查根据 **类文件常量池**（Constant Pool Table）能否找到这个类对应的符号引用，然后去==方法区中的运行时常量池==中查找该符号引用所指向的类是否已被 JVM 加载、解析和初始化过

- 如果没有，那就先执行相应的类加载过程
- 如果有，那么进入下一步，为新生对象分配内存

2）分配内存：就是在堆中给划分一块内存空间分配给这个新生对象用。具体的分配方式根据堆内存是否规整有两种方式：

- 堆内存规整的话采用的分配方式就是指针碰撞：所有被使用过的内存都被放在一边，空闲的内存被放在另一边，中间放着一个指针作为分界点的指示器，分配内存就是把这个指针向空闲空间方向挪动一段与对象大小相等的距离
- 堆内存不规整的话采用的分配方式就是空闲列表：所谓内存不规整就是已被使用的内存和空闲的内存相互交错在一起，那就没有办法简单地进行指针碰撞了，JVM 就必须维护一个列表，记录哪些内存块是可用的，在分配的时候从列表中找到一块足够大的连续空间划分给这个对象，并更新列表上的记录，这就是空闲列表的方式

3）初始化零值：对象在内存中的布局可以分为 3 块区域：对象头、实例数据和对齐填充，对齐填充仅仅起占位作用，没啥特殊意义，初始化零值这个操作就是初始化实例数据这个部分，比如 boolean 字段初始化为 false 之类的

4）设置对象头：这个步骤就是设置对象头中的一些信息

5）执行 init 方法：最后就是执行构造函数，构造函数即 Class 文件中的 `<init>()` 方法，一般来说，new 指令之后会接着执行 `<init>()` 方法，按照构造函数的意图对这个对象进行初始化，这样一个真正可用的对象才算完全地被构造出来了



> **jvm中的常量池分为三种**
>
>   1.类文件常量池(Class Constant Pool)
>
>   2.运行时常量池(Runtime Constant Pool)
>
>   3.字符串常量池(String Constant Pool)
>
> **类文件常量池**:每一个Java类被编译后，就会形成一份class文件,包含类的版本、字段、方法、接口等描述信息外，还有一项信息就是常量池(constant pool table)，用于存放编译器生成的各种字面量(Literal)和符号引用(Symbolic References)。
>
> **运行时常量池可以在运行期间将 class 常量池表中的符号引用解析为直接引用**。简单来说，class 常量池表就相当于一堆索引，运行时常量池根据这些索引来查找对应方法或字段所属的类型信息和名称及描述符信息,放class文件元信息描述，编译后的代码数据，引用类型数据，类文件常量池
>
> **字符串常量池**:字符串常量,在每个HotSpot VM的实例只有一份，被所有的类共享。
>
> Hotspot 虚拟机的对象头包括两部分信息：
>
> 第一部分用于存储对象自身的运行时数据（如哈希码（HashCode）、GC 分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等
>
> 另一部分是类型指针，即对象指向它的类型元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例



参考文章：[new 一个对象在堆中的历程](https://zhuanlan.zhihu.com/p/465960384)



#### 问：Java对象的内存分配过程是如何保证线程安全的

在为对象创建内存的时候，还需要考虑一个问题：并发安全问题。

对象创建在虚拟机中是非常频繁的行为，以上面介绍的指针碰撞法为例，即使只修改一个指针所指向的位置，在并发情况下也并不是线程安全的，可能出现某个线程正在给对象 A 分配内存，指针还没来得及修改，另一个线程创建了对象 B 又同时使用了原来的指针来分配内存的情况。

解决这个问题有两种可选方案：

- 方案 1：**CAS + 失败重试**：CAS 大伙应该都熟悉，比较并交换，乐观锁方案，如果失败就重试，直到成功为止
- 方案 2：**本地线程分配缓冲**（Thread Local Allocation Buffer，`TLAB`）：每个线程在堆中预先分配一小块内存，每个线程拥有的这一小块内存就称为 TLAB。哪个线程要分配内存了，就在哪个线程的 TLAB 中进行分配，这样各个线程之间互不干扰。如果某个线程的 TLAB 用完了，那么虚拟机就需要为它分配新的 TLAB，这时才需要进行同步锁定。可以通过 `-XX：+/-UseTLAB` 参数来设定是否使用 TLAB。



#### 问：Unsafe类

Unsafe类使Java语言拥有了类似C语言指针一样操作内存空间的能力。

Unsafe类为一单例实现，Unsafe提供的API大致可分为内存操作、CAS、Class相关、对象操作、线程调度、系统 信息获取、内存屏障、数组操作等几类。

我们在Java中创建的对象都处于堆内内存（heap）中，堆内内存是由JVM所管控的Java进程内存，并且它们遵循JVM的内存管理机制，JVM会采用垃圾回收机制统一管理堆内存。与之相对的是堆外内存，存在于JVM管控之外的内存区域，Java中对堆外内存的操作，依赖于Unsafe提供的操作堆外内存的native方法。

**使用堆外内存的原因**：对垃圾回收停顿的改善，提升程序I/O操作的性能。
==DirectByteBuffer是Java用于实现堆外内存的一个重要类，通常用在通信过程中做缓冲池，如在Netty、MINA等NIO框架中应用广泛。==DirectByteBuffer对于堆外内存的创建、使用、销毁等逻辑均由Unsafe提供的堆外内存API来实现。



#### 问：JVM内存溢出排查？

JDK自带的工具jvisualvm









# 系统设计

## 设计模式

#### **问：介绍一下单例模式**

答：单例是指在当前进程中，通过单例模式创建的类有且只有一个实例。

单例模式经典的有饿汉式和懒汉式。

```java
// 饿汉式单例实现 饿汉式之所以是线程安全的，是因为JVM在类加载的过程，保证了不会初始化多个`static`对象。
public class Singleton {
  // 创建一个实例对象
    private static Singleton instance = new Singleton();
    /**
     * 私有构造方法，防止被实例化
     */
    private Singleton(){}
    /**
     * 静态get方法
     */
    public static Singleton getInstance(){
        return instance;
    }
}
```

```java
// synchronized 和 volatile 修饰的懒汉式
public class Singleton {
    private volatile static Singleton instance = null;
    private Singleton(){}
    public static Singleton getInstance(){
        //先检查实例是否存在，如果不存在才进入下面的同步块
        if(instance == null){
            //同步块，线程安全的创建实例
            synchronized (Singleton.class) {
                //再次检查实例是否存在，如果不存在才真正的创建实例
                if(instance == null){
                    instance = new Singleton();
                }
            }
        }
        return instance;
    }
}
```

```java
// 枚举写法
public enum Singleton {
    /**
     * 定义一个枚举的元素，它就代表了Singleton的一个实例。
     */
    Instance;
}
```

为什么不用静态方法而用单例模式？

两者其实都能实现我们加载的最终目的，但是他们一个是基于对象，一个是面向对象的，就像我们不面向对象也能解决问题一样，面向对象的代码提供一个更好的编程思想。

如果一个方法和他所在类的实例对象无关，那么它就应该是静态的，反之他就应该是非静态的。如果我们确实应该使用非静态的方法，但是在创建类时又确实只需要维护一份实例时，就需要用单例模式了。



#### **问：介绍一下工厂模式**

答：工厂模式主要可以分为三大类：简单工厂模式、工厂方法模式、抽象工程模式。

简单工厂模式通常是定义一个工厂类，它可以根据参数的不同返回不同类的实例，被创建的实例通常都具有共同的父类。在简单工厂模式中用于被创建实例的方法通常为静态(static)方法,因此简单工厂模式又被成为静态工厂方法。最重要的缺点是系统扩展困难，一旦增加新产品不得不修改工厂逻辑。

工厂方法模式：定义了一个用于创建对象的接口，但是让子类决定将哪一个类实例化。即让类的实例化延迟到子类。

参考链接：https://juejin.cn/post/6844903474639929357





#### **问：介绍一下代理模式**

答：所谓代理模式就是使用代理对象来代替对真实对象的访问，这样就可以在不修改原目标对象的前提下，提供额外的功能操作，扩展目标对象的功能。代理模式有三大角色：

- Real Subject：真实类，也就是被代理类、委托类。用来真正完成业务服务功能
- Proxy：代理类。将自身的请求用 Real Subject 对应的功能来实现，代理类对象并不真正的去实现其业务功能
- Subject：定义 RealSubject 和 Proxy 角色都应该实现的接口

代理模式分为静态代理和动态代理。

先来说静态代理：

静态代理就是，对于你想要增强的委托类，我们需要新建一个代理类，这两个类实现一个同样的接口，然后==将委托类注入进代理类中==，在代理类的方法中调用委托类中的对应方法。这样，我们就可以通过代理类屏蔽对目标对象的访问，并且可以在目标方法执行前后做一些自己想做的事情。

从 JVM 层面来说， 静态代理就是在编译时就将接口、委托类、代理类这些都变成了一个个实际的 .class 文件。

静态代理的弊端很明显，一个委托类对应一个代理类，多个委托类就需要新建多个代理类，我们能不能将代理类做成一个通用的呢？

为此，动态代理应用而生。

动态代理的实现方式有很多种，常见的有：JDK 动态代理和 CGLIB 动态代理

先来说 JDK 动态代理：

同样的，JDK 动态代理需要委托类实现一个接口，不过代理类就不需要也实现同样的接口了，但是，JDK 动态代理机制中添加了一个新的角色，那就是处理类。具体来说，我们需要新建一个处理类，然后将委托类注入处理类，另外，这个==处理类需要实现 InvocationHandler 接口，并重写其 invoke 方法==，在 invoke 方法中可以利用反射机制调用委托类的方法，并可以在其前后添加一些额外的处理逻辑。最后，我们定义一个创建代理对象的工厂类Proxy（代理类），通过 Proxy.newProxyInstance() 创建委托类对象的代理对象。

这样做的缺点也是明显，那就是参数 Interfaces 是委托类的接口，是必传的，JDK 动态代理是通过与委托类实现同样的接口，然后在实现的接口方法里进行增强来实现的，这就意味着如果要用 JDK 代理，委托类必须实现接口。

如果目标类没有实现接口，就得选择使用CGLIB来动态代理目标类。CGLIB（Code Generation Library），是一个字节码生成的类库，可以在==运行时动态的生成某个类的子类==（通过修改字节码来实现代理）。

CGlib 动态代理也提供了类似的 Enhance 类，增强逻辑写在 MethodInterceptor.intercept() 中，也就是说所有委托类的非 final 方法都会被方法拦截器拦截，从而在拦截器里实现逻辑增强。

当然 CGLib 动态代理也有自身限制：只能代理委托类中任意的非 final 的方法，另外它是通过继承自委托类来生成代理的，所以如果委托类是 final 的，就无法被代理了。

那么什么情况下需要使用动态代理呢？

1. 设计模式中有一个设计原则是开闭原则，即对修改关闭，对扩展开放，我们在工作中有时会接手很多前人的代码，里面代码逻辑让人摸不着头脑，就很难去下手修改代码，那么这时我们就可以通过代理对类进行增强。
2. 我们在使用 RPC 框架的时候，框架本身并不能提前知道各个业务方要调用哪些接口的哪些方法。那么这个时候，就可用通过动态代理的方式来建立一个中间人给客户端使用，也方便框架进行搭建逻辑，某种程度上也是客户端代码和框架松耦合的一种表现。
3. Spring 的 AOP 机制同样也是采用了动态代理



## SSM

#### **问：简单介绍一下Spring框架、Spring 等框架的好处**

答：Spring 是一个开源框架，一个基于模块化、轻量级的 Java 开发框架，它是为了解决企业应用开发的复杂性而创建的。

使用 Spring 框架的好处有：

- 方便解耦，简化开发：通过Spring 提供的 IoC 容器，我们可以将对象之间的依赖关系交给Spring进行控制，避免程序过度耦合，可以更专注于上层的应用。
- AOP编程的支持：通过Spring提供的AOP功能，方便进行面向切面的编程，许多不容易用传统OOP实现的功能可以通过AOP轻松应付，如测试监控等。
- 提供良好的事务管理、异常处理机制，并且方便调试。



#### **问：介绍IoC 和IoC 的实现**

答：IoC （Inverse of Control，控制反转）是一种设计思想，用来简化应用的开发，把应用从复杂的依赖关系中解放出来。

具体来说就是将原本在程序中手动创建和管理对象的控制权，交由Spring 容器来管理。IoC 容器是 Spring 用来实现 IoC 的载体， IoC 容器实际上就是个Map（key，value）,Map 中存放的是各种对象。

当我们需要创建一个对象的时候，只需要配置好配置文件或注解即可，IoC容器会完成对象注入，而不用考虑对象的依赖关系和创建过程。

Spring IoC 的初始化过程

![图片](pics\Spring IoC初始化过程.png)

DI（Dependency Injection，依赖注入）是IoC的一种手段，它说明了IoC控制的什么被反转了，也就是获得依赖对象的方式反转了。由Spring容器负责将需要的对象注入到调用者的成员变量，即为调用者注入了它依赖的实例，这就是依赖注入。 

参考文章：[Spring IoC](https://www.zhihu.com/question/23277575/answer/169698662) 、[IoC与DI的理解](https://blog.csdn.net/jisuanjiguoba/article/details/81532965)



#### **问：介绍AOP 和AOP 的实现**

答：AOP（Aspect-oriented Programming）面向切面编程，为了解决横切逻辑代码存在的代码重复问题、以及横切逻辑代码和业务代码混杂在一起，代码臃肿，不便于维护等问题，而提出的一种编程思想。

它提出了横向抽取机制，将横切逻辑代码和业务逻辑代码分离。使得开发者在不改变源代码的前提下，为系统中不同业务组件添加某些通用功能。常见的应用场景有如日志功能、事务管理等。

Spring AOP实现方式：如果你的代理类有接口的话，那走的JDK的动态代理，如果没有的话，则使用cglib字节码技术去创建代理类对象，主要就是这两种方式来做的。

实现AOP功能的框架主要有Spring AOP和AspectJ。Spring AOP 属于运行时增强，而 AspectJ 是编译时增强。 Spring AOP 基于代理(Proxying)，而 AspectJ 基于字节码操作(Bytecode Manipulation)。

> - 切面(Aspect)：似于 Java 中的类声明，常用于应用中配置事务或者日志管理。一般使用 `@Aspect` 注解或者 `<aop:aspect>` 来定义一个切面。
> - 连接点(Join Point)：程序执行中的特定点，比如方法执行、处理一个异常等
> - 切点(Pointcut)：通过一种规则匹配的正则表达式，当有连接点可以匹配到切点时，就会触发改切点相关联的指定通知。
> - 通知(Advice)：在切面中某个连接点采取的动作，通知方式也有5种





#### **问：Spring 管理事务的方式**

答：一是编程式事务，在代码中硬编码。二是声明式事务，在配置文件中配置，其中可分成基于XML和基于注解两种。



#### **问：SpringMVC 工作原理**

<img src="C:\Users\Lenovo\Desktop\mvc.png" alt="图片" style="zoom: 67%;" />

<img src="C:\Users\Lenovo\Desktop\笔记\CS-Learning-Notes\notes\pics\SpringMVC工作原理.png" alt="图片" style="zoom: 67%;" />

1. 客户端（浏览器）发送请求，直接请求到 `DispatcherServlet`。
2. `DispatcherServlet` 根据请求信息调用 `HandlerMapping`，解析请求对应的 `Handler`。
3. 解析到对应的 `Handler`（也就是我们平常说的 `Controller` 控制器）后，开始由 `HandlerAdapter` 适配器处理。
4. `HandlerAdapter` 会根据 `Handler`来调用真正的处理器开处理请求，并处理相应的业务逻辑。
5. 处理器处理完业务后，会返回一个 `ModelAndView` 对象，`Model` 是返回的数据对象，`View` 是个逻辑上的 `View`。
6. `ViewResolver` 会根据逻辑 `View` 查找实际的 `View`。
7. `DispatcherServlet` 把返回的 `Model` 传给 `View`（视图渲染）。
8. 把 `View` 返回给请求者（浏览器）



#### **问：Spring Bean 的生命周期**

答：（1）Spring在启动的时候需要「扫描」在XML/注解/JavaConfig 中需要被Spring管理的Bean信息，随后，会将这些信息封装成BeanDefinition，最后会把这些信息放到一个beanDefinitionMap中，这个Map的key应该是beanName，value则是BeanDefinition对象。

（2）接着会遍历这个beanDefinitionMap，执行BeanFactoryPostProcessor这个Bean工厂后置处理器的逻辑，对Bean的元信息进行修改。比如注入占位符等等。

（3）通过反射对对象进行实例化，这时对象具体的属性是还没注入的。

（4）接着开始初始化操作，首先判断该Bean是否实现了Aware相关的接口，如果存在则填充相关的资源。

（4）然后是BeanPostProcessor后置处理器有两个方法，一个是before，一个是after。这个BeanPostProcessor后置处理器是AOP实现的关键

（5）在这期间会执行init相关的方法

（6）执行完后，就可以通过getBean() 获取使用了。

（7）销毁的时候看有没有配置destroy方法，执行就结束了。

参考文章：[Spring Aware接口](https://blog.csdn.net/iechenyb/article/details/83788338?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0.pc_relevant_default&spm=1001.2101.3001.4242.1&utm_relevant_index=3) 、[生命周期](http://javainterview.gitee.io/luffy/2021/08/19/05-Spring/03.%20SpringBean%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/)



#### 问：Spring 解决循环依赖

答：解决方式是用到了三级缓存，也就是三个Map，三级缓存定义如下：

singletonObjects（一级，正式Bean对象）

earlySingletonObjects（二级，还没进行属性注入，由三级缓存放进来，半成品）

singletonFactories（三级，Value是一个对象工厂）

过程：

1. A对象实例化之后，属性注入之前，其实会把A对象放入三级缓存中，key是BeanName，Value是ObjectFactory；
2. 等到A对象属性注入时，发现依赖B，又去实例化B时，B属性注入需要去获取A对象，这里就是从三级缓存里拿出ObjectFactory，从ObjectFactory得到对应的Bean（就是对象A）；
3. 把三级缓存的A记录给干掉，然后放到二级缓存中。显然，二级缓存存储的key是BeanName，value就是Bean；
4. 等到完全初始化之后，就会把二级缓存给remove掉，塞到一级缓存中；
5. 我们自己去getBean的时候，实际上拿到的是一级缓存的。



#### **问：Spring Boot 启动流程**

答：根据SpringBoot最常见的启动类写法，主要是有两个部分：

- @SpringBootApplication注解
- 一个main() 方法，里面调用SpringAppliction.run方法

首先看@SpringBootApplication注解，它是由三个注解组合而成，分别是：

- @ComponentScan：告诉Spring扫描哪个包下面的类，加载符合条件的组件
- @EnableAutoConfiguration：这是一个复合注解，看起来很多注解，实际上关键在@Import注解，它会加载AutoConfigurationImportSelector 自动配置加载类，然后就会触发这个类的selectImports()方法。根据返回的String数组（存储配置类的Class的名称）加载配置类。所以@EnableAutoConfiguration注解的作用其实就是开启自动配置，自动配置主要则依靠这种加载方式来实现。
- @SpringBootConfiguration：继承自自@Configuration，二者功能也一致，标注当前类是配置类， 并会将当前类内声明的一个或多个以@Bean注解标记的方法的实例纳入到Spring容器中，并且实例名就是方法名。

接下来讲main方法里执行的这句代码，这是SpringApplication类的run()，它会new一个SpringApplication实例。创建了SpringApplication实例之后，就完成SpringApplication类的初始化工作，如监听器、初始化器等等。得到SpringApplication实例后，接下来就调用实例方法run()。

表面启动类看起来就一个@SpringBootApplication注解，一个run()方法。其实是经过高度封装后的结果。

参考文章：[springboot启动流程](https://zhuanlan.zhihu.com/p/295451397)



#### **问：Spring Boot 自动装配**

Spring Boot自动配置的核心注解是@SpringBootApplication，该注解是spring boot的启动类注解，它是一个复合注解。其中的`@EnableAutoConfiguration`  表示开启自动配置，它也是一个复合注解，其中`@AutoConfigurationPackage` 注解会将启动类所在包下的所有子包的组件扫描注入到spring容器中，另一个注解 `@Import(AutoConfigurationImportSelector.class)` 则是会导入了一个类`EnableAutoConfigurationImportSelector` 加载自动配置类，该类方法可以查找位于META-INF/spring.factories文件中的所有自动配置类，并加载这些类。

前面加载的所有自动配置类并不是都生效的，每一个`xxxAutoConfiguration`自动配置类都是在某些特定的条件之下才会生效。这些条件限制是通过`@ConditionOnxxx`注解实现的。

参考文章：[Spring Boot自动装配](https://blog.csdn.net/weixin_42556307/article/details/108405009)



#### **问：SpringBoot、Spring MVC和Spring有什么区别？**

Spring：Spring最重要的特征是依赖注入。所有Spring Modules不是依赖注入就是IOC控制反转。
当我们恰当的使用DI或者是IOC的时候，可以开发松耦合应用。

Spring MVC：Spring MVC提供了一种分离式的方法来开发Web应用。通过运用像DispatcherServelet，MoudlAndView 和 ViewResolver 等一些简单的概念，开发 Web 应用将会变的非常简单。

SpringBoot：Spring和Spring MVC的问题在于需要配置大量的参数。SpringBoot通过一个自动配置和启动的项来解决这个问题。



#### **问：SpringBoot 中如何实现定时任务?**

答：定时任务也是一个常见的需求，SpringBoot 中对于定时任务的支持主要还是来自 Spring 框架。

在 SpringBoot 中使用定时任务主要有两种不同的方式，一个就是使用 Spring 中的 @Scheduled 注解，另一个则是使用第三方框架 Quartz。



#### 问：spirng事务

Spring 支持两种事务管理：编程式事务管理（TransactionTemplate）和声明式事务管理（`@Transactional`注解）



#### Spring 中的 bean 的作用域有哪些?

- singleton : 唯一 bean 实例，Spring 中的 bean 默认都是单例的。
- prototype : 每次请求都会创建一个新的 bean 实例。
- request : 每一次HTTP请求都会产生一个新的bean，该bean仅在当前HTTP request内有效。
- session : 每一次HTTP请求都会产生一个新的 bean，该bean仅在当前 HTTP session 内有效。
- global-session：全局session作用域，仅仅在基于portlet的web应用中才有意义，Spring5已经没有了。Portlet是能够生成语义代码(例如：HTML)片段的小型Java Web插件。它们基于portlet容器，可以像servlet一样处理HTTP请求。但是，与 servlet 不同，每个 portlet 都有不同的会话



#### 将一个类声明为Spring的 bean 的注解有哪些?

我们一般使用 `@Autowired` 注解自动装配 bean，要想把类标识成可用于 `@Autowired` 注解自动装配的 bean 的类,采用以下注解可实现：

- `@Component` ：通用的注解，可标注任意类为 `Spring` 组件。如果一个Bean不知道属于哪个层，可以使用`@Component` 注解标注。
- `@Repository` : 对应持久层即 Dao 层，主要用于数据库相关操作。
- `@Service` : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao层。
- `@Controller` : 对应 Spring MVC 控制层，主要用户接受用户请求并调用 Service 层返回数据给前端页面。



#### Spring 事务中哪几种事务传播行为?

**支持当前事务的情况：**

- **TransactionDefinition.PROPAGATION_REQUIRED：** 如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。
- **TransactionDefinition.PROPAGATION_SUPPORTS：** 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。
- **TransactionDefinition.PROPAGATION_MANDATORY：** 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性）

**不支持当前事务的情况：**

- **TransactionDefinition.PROPAGATION_REQUIRES_NEW：** 创建一个新的事务，如果当前存在事务，则把当前事务挂起。
- **TransactionDefinition.PROPAGATION_NOT_SUPPORTED：** 以非事务方式运行，如果当前存在事务，则把当前事务挂起。
- **TransactionDefinition.PROPAGATION_NEVER：** 以非事务方式运行，如果当前存在事务，则抛出异常。

**其他情况：**

- **TransactionDefinition.PROPAGATION_NESTED：** 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。



#### @Transactional事务里的事务隔离级别和事务传播机制概念。





#### 各种Restful请求格式以及各种http请求返回码。





## Redis

#### **问：为什么要用Redis?**

#### **问：Redis 的数据结构、原理、使用场景**

#### **问：zset 为何使用跳表？**

#### **问：如何保证Redis和DB的一致性**

#### **问：Redis 为什么快，为什么Redis单线程也快呢？**

#### **问：Redis 在分布式中会有什么问题怎么解决？**

#### **问：Redis 的两种持久化方式以及优劣**

#### **问：Redis的内存回收机制**

#### **问：Redis挂了，流量把数据库也打挂了，怎么办？**

#### **问：Redis的高可用架构**

参考文章：[Redis面试题集锦](http://www.yswang.tech/archives/redis-mian-shi-ti-ji-jin#HMkdXwRD)



#### 问：Redis事务

事务的原理是将一个事务范围内的若干命令发送给Redis，然后再让Redis依次执行这些命令。

事务的生命周期：

1. 使用MULTI开启一个事务
2. 在开启事务的时候，每次操作的命令将会被插入到一个队列中，同时这个命令并不会被真的执行
3. EXEC命令进行提交事务





## Zookeeper

#### **问：Zookeeper的选举机制**

#### **问：Zookeeper的Watcher机制**

#### **问：Zookeeper的应用场景**

参考文章：[Zookeeper基础](http://www.yswang.tech/archives/zookeeper-basic#waicziXG)



## Netty

#### **阻塞和非阻塞，同步与异步的区别**

阻塞与非阻塞：所谓阻塞就是发起读取数据请求的时，当数据还没准备就绪的时候，这时请求是即刻返回，还是在这里等待数据的就绪，如果需要等待的话就是阻塞，反之如果即刻返回就是非阻塞。

同步与异步：在IO模型里面如果请求方从发起请求到数据最后完成的这一段过程中都需要自己参与，那么这种我们称为同步请求；反之，如果应用发送完指令后就不再参与过程了，只需要等待最终完成结果的通知，那么这就属于异步。

同步阻塞与同步非阻塞：两者不同的只是发起读取请求的时候一个请求阻塞，一个请求不阻塞，但是相同的是，他们都需要应用自己监控整个数据完成的过程。





#### **Java BIO编程**

1. `I/O` 模型简单的理解：就是用什么样的通道进行数据的发送和接收，很大程度上决定了程序通信的性能。
2. `Java` 共支持 `3` 种网络编程模型 `I/O` 模式：`BIO`、`NIO`、`AIO`。
3. `Java BIO`：同步并阻塞（传统阻塞型），==服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销==。
4. `Java NIO`：同步非阻塞，==服务器实现模式为一个线程处理多个请求（连接），即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有 I/O 请求就进行处理==。
5. `Java AIO(NIO.2)`：异步非阻塞，`AIO` 引入异步通道的概念，采用了 `Proactor` 模式，简化了程序编写，有效的请求才启动线程，它的特点是==先由操作系统完成后才通知服务端程序启动线程去处理，一般适用于连接数较多且连接时间较长的应用==。

Java BIO 问题分析：

1. 每个请求都需要创建独立的线程，与对应的客户端进行数据 `Read`，业务处理，数据 `Write`。
2. 当并发数较大时，需要创建大量线程来处理连接，系统资源占用较大。
3. 连接建立后，如果当前线程暂时没有数据可读，则线程就阻塞在 `Read` 操作上，造成线程资源浪费。

参考文章：[Netty学习手册](https://dongzl.github.io/netty-handbook/#/README)



#### **Java NIO编程**

1. `NIO` 有三大核心部分: `Channel`（通道）、`Buffer`（缓冲区）、`Selector`（选择器） 。
2. `NIO` 是面向缓冲区，或者面向块编程的。数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动，这就增加了处理过程中的灵活性，使用它可以提供非阻塞式的高伸缩性网络。
3. `Java NIO` 的非阻塞模式，使一个线程从某通道发送请求或者读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取，而不是保持线程阻塞，所以直至数据变的可以读取之前，该线程可以继续做其他的事情。非阻塞写也是如此，一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。

<img src="pics\chapter03_01.png" alt="img" style="zoom: 33%;" />

1. `Selector` 对应一个线程，一个线程对应多个 `Channel`（连接）。
2. 每个 `Channel` 都会对应一个 `Buffer`。
3. `Selector` 会根据不同的事件，在各个通道上切换。
4. `Buffer` 就是一个内存块，底层是有一个数组。
5. 数据的读取写入是通过 `Buffer`，这个和 `BIO`，`BIO` 中要么是输入流，或者是输出流，不能双向，但是 `NIO` 的 `Buffer` 是可以读也可以写，需要 `flip` 方法切换 `Channel` 是双向的，可以返回底层操作系统的情况，比如 `Linux`，底层的操作系统通道就是双向的。

- 缓冲区（`Buffer`）

==缓冲区本质上是一个可以读写数据的内存块，可以理解成是一个容器对象（含数组），该对象提供了一组方法，可以更轻松地使用内存块，缓冲区对象内置了一些机制，能够跟踪和记录缓冲区的状态变化情况。Channel 提供从文件、网络读取数据的渠道，但是读取或写入的数据都必须经由 Buffer。==

- 通道（`Channel`）

类似于流，不同之处在于通道可以同时进行读写，可以实现异步读写。

- 选择器（`Selector`）

1. ==能够检测已经注册的通道上是否有事件发生（注意：多个 Channel 以事件的方式可以注册到同一个 Selector），如果有事件发生，便获取事件然后针对每个事件进行相应的处理。这样就可以只用一个单线程去管理多个通道，也就是管理多个连接和请求。==
2. 只有在连接/通道真正有读写事件发生时，才会进行读写，就大大地减少了系统开销，并且不必为每个连接都创建一个线程，不用去维护多个线程。
3. 避免了多线程之间的上下文切换导致的开销。

NIO非阻塞网络编程原理：

```java
public static void main(String[] args) throws Exception {
        InetSocketAddress address = new InetSocketAddress(7001);
        ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();
        ServerSocket serverSocket = serverSocketChannel.socket();
        serverSocket.bind(address);

        //创建buffer
        ByteBuffer byteBuffer = ByteBuffer.allocate(4096);

        while (true) {
            SocketChannel socketChannel = serverSocketChannel.accept();
            int readcount = 0;
            while (-1 != readcount) {
                try {
                    readcount = socketChannel.read(byteBuffer);
                } catch (Exception ex) {
                    // ex.printStackTrace();
                    break;
                }
                //
                byteBuffer.rewind(); //倒带 position = 0 mark 作废
            }
        }
    }
```

1. 当客户端连接时，服务器端监听到则会通过 `ServerSocketChannel` 生成得到 `SocketChannel`。
3. 将 `socketChannel` 注册到 `Selector` 上，`register(Selector sel, int ops)`，一个 `Selector` 上可以注册多个 `SocketChannel`。
4. 注册后返回一个 `SelectionKey`，会和该 `Selector` 关联（集合）。
4. `Selector` 进行监听 `select` 方法，返回有事件发生的通道的个数。
5. 进一步得到各个 `SelectionKey`（有事件发生）。
6. 在通过 `SelectionKey` 反向获取 `SocketChannel`，方法 `channel()`。
7. 可以通过得到的 `channel`，完成业务处理。



#### **Reactor模式**

==Reactor模式是IO复用结合线程池技术，使用 IO 复用监听事件，收到事件后，分派到相应的处理线程。==

- 单Reactor单线程：
  1. `Reactor` 对象通过 `Select` 监控客户端请求事件，收到事件后通过 `Dispatch` 进行分发
  2. 此时一个Reactor既然负责处理连接请求，又要负责处理读写请求。Reactor正在处理读写请求的时候，其他请求只能等着，只有等处理完了，才可以处理下一个请求。

优缺点：

1. 优点：模型简单，没有多线程、进程通信、竞争的问题，全部都在一个线程中完成
2. 缺点：性能问题，只有一个线程，无法完全发挥多核 `CPU` 的性能。`Handler`在处理某个连接上的业务时，整个进程无法处理其他连接事件，很容易导致性能瓶颈

- 单Reactor多线程
  1. `Reactor` 对象通过 `Select` 监控客户端请求事件，收到事件后，通过 `Dispatch` 进行分发
  2. 此时，Reactor还是既要负责处理连接事件，又要负责处理客户端的写事件，不同的是，多了一个线程池的概念。当客户端发起连接请求后，Reactor会把任务交给acceptor处理，如果客户端发起了写请求，Reactor会把任务交给线程池进行处理，这样一个服务端就可以同时为N个客户端服务了。

优缺点：

1. 优点：可以充分的利用多核 `cpu` 的处理能力
2. 缺点：多线程数据共享和访问比较复杂，`Reactor` 处理所有的事件的监听和响应，在单线程运行，在高并发场景容易出现性能瓶颈。

- 主从Reactor多线程

针对单 `Reactor` 多线程模型中，`Reactor` 在单线程中运行，高并发场景下容易成为性能瓶颈，可以让 `Reactor` 在多线程中运行。

1. ==Reactor 主线程 MainReactor 对象通过 select 监听连接事件，收到事件后，mainReactor只负责连接请求，而subReactor 负责创建handler处理客户端的业务处理==。
2. 当有新事件发生时，`subreactor` 就会调用对应的 `handler` 处理
3. `handler` 通过 `read` 读取数据，分发给后面的 `worker` 线程处理
4. `worker` 线程池分配独立的 `worker` 线程进行业务处理，并返回结果
5. `handler` 收到响应的结果后，再通过 `send` 将结果返回给 `client`
6. `Reactor` 主线程可以对应多个 `Reactor` 子线程，即 `MainRecator` 可以关联多个 `SubReactor`

优缺点：

1. 优点：父线程与子线程的数据交互简单职责明确，父线程只需要接收新连接，子线程完成后续的业务处理。
2. 缺点：编程复杂度较高



#### **Netty模型**

==Netty主要基于主从Reactors多线程模型做了一定的改进。==

1. `Netty` 抽象出两组线程池， `BossGroup` 专门负责接收客户端的连接，`WorkerGroup` 专门负责网络的读写。
2. 两者类型都是 `NioEventLoopGroup`， `NioEventLoopGroup`相当于一个事件循环组，每个事件循环 `NioEventLoop`表示一个不断循环的执行处理任务的线程， 并且每个 `NioEventLoop` 都有一个 `Selector`，用于监听绑定在其上的 `socket` 。
3. 当`Boss NioEventLoop` 接收到 `Accept` 事件，生成 `NioScocketChannel`，并将其注册到某个 `worker` `NIOEventLoop` 上的 `Selector`，并让它进行IO业务处理。
4. 每个 `Worker NIOEventLoop` 处理业务时，会使用 `pipeline`（管道），`pipeline` 中包含了 `channel`，即通过 `pipeline` 可以获取到对应通道，管道中维护了很多的处理器.



> socket起源于Unix，而Unix/Linux基本哲学之一就是“一切皆文件”，都可以用“打开open –> 读写write/read –> 关闭close”模式来操作。我的理解就是Socket就是该模式的一个实现，socket即是一种特殊的文件，一些socket函数就是对其进行的操作。

<img src="C:\Users\Lenovo\Desktop\笔记\CS-Learning-Notes\notes\pics\chapter05_10.png" alt="img" style="zoom: 67%;" />

1. `BossGroup` 线程维护 `Selector`，只关注 `Accecpt`
2. 当接收到 `Accept` 事件，获取到对应的 `SocketChannel`，封装成 `NIOScoketChannel` 并注册到 `Worker` 线程（事件循环），并进行维护
3. 当 `Worker` 线程监听到 `Selector` 中通道发生自己感兴趣的事件后，就进行处理（就由 `handler`），注意 `handler` 已经加入到通道



1. `Netty` 抽象出两组线程池 `BossGroup` 专门负责接收客户端的连接，`WorkerGroup` 专门负责网络的读写
2. `BossGroup` 和 `WorkerGroup` 类型都是 `NioEventLoopGroup`
3. `NioEventLoopGroup` 相当于一个事件循环组，这个组中含有多个事件循环，每一个事件循环是 `NioEventLoop`
4. `NioEventLoop` 表示一个不断循环的执行处理任务的线程，每个 `NioEventLoop` 都有一个 `Selector`，用于监听绑定在其上的 `socket` 的网络通讯
5. `NioEventLoopGroup` 可以有多个线程，即可以含有多个 `NioEventLoop`
6. 每个 `BossNioEventLoop` 循环执行的步骤有 3 步：
   - 轮询 `accept` 事件
   - 处理 `accept` 事件，与 `client` 建立连接，生成 `NioScocketChannel`，并将其注册到某个 `worker` `NIOEventLoop` 上的 `Selector`
   - 处理任务队列的任务，即 `runAllTasks`
7. 每个 `Worker` `NIOEventLoop`循环执行的步骤
   - 轮询 `read`，`write` 事件
   - 处理 `I/O` 事件，即 `read`，`write` 事件，在对应 `NioScocketChannel` 处理
   - 处理任务队列的任务，即 `runAllTasks`
8. 每个 `Worker NIOEventLoop` 处理业务时，会使用 `pipeline`（管道），`pipeline` 中包含了 `channel`，即通过 `pipeline` 可以获取到对应通道，管道中维护了很多的处理器

补充：

1. `Netty` 抽象出两组线程池，`BossGroup` 专门负责接收客户端连接，`WorkerGroup` 专门负责网络读写操作。
2. `NioEventLoop` 表示一个不断循环执行处理任务的线程，每个 `NioEventLoop` 都有一个 `Selector`，用于监听绑定在其上的 `socket`网络通道。
3. `NioEventLoop` 内部采用串行化设计，从消息的 **读取->解码->处理->编码->发送**，始终由 `IO` 线程 `NioEventLoop` 负责

`NioEventLoopGroup` 下包含多个 `NioEventLoop`

- 每个 `NioEventLoop` 中包含有一个 `Selector`，一个 `taskQueue`
- 每个 `NioEventLoop` 的 `Selector` 上可以注册监听多个 `NioChannel`
- 每个 `NioChannel` 只会绑定在唯一的 `NioEventLoop` 上
- 每个 `NioChannel` 都绑定有一个自己的 `ChannelPipeline`



#### **异步模型**

1. ==异步的概念和同步相对。当一个异步过程调用发出后，调用者不能立刻得到结果。实际处理这个调用的组件在完成后，通过状态、通知和回调来通知调用者。==
2. `Netty` 中的 `I/O` 操作是异步的，包括 `Bind、Write、Connect` 等操作会简单的返回一个 `ChannelFuture`。
3. 调用者并不能立刻获得结果，而是通过 `Future-Listener` 机制，用户可以方便的主动获取或者通过通知机制获得 `IO` 操作结果。
4. `Netty` 的异步模型是建立在 `future` 和 `callback` 的之上的。

Future说明：

1. 表示异步的执行结果,可以通过它提供的方法来检测执行是否完成，比如检索计算等等。
2. `ChannelFuture` 是一个接口：`public interface ChannelFuture extends Future<Void>` 我们可以添加监听器，当监听的事件发生时，就会通知到监听器。



#### **Netty核心模块组件**

```java
public static void main(String[] args) throws Exception {
        
        //创建两个线程组
        EventLoopGroup bossGroup = new NioEventLoopGroup(1);
        EventLoopGroup workerGroup = new NioEventLoopGroup(); //8个NioEventLoop
        try {

            ServerBootstrap serverBootstrap = new ServerBootstrap();

            serverBootstrap.group(bossGroup, workerGroup);
            serverBootstrap.channel(NioServerSocketChannel.class);
            serverBootstrap.handler(new LoggingHandler(LogLevel.INFO));
            serverBootstrap.childHandler(new ChannelInitializer<SocketChannel>() {

                @Override
                protected void initChannel(SocketChannel ch) throws Exception {
                    ChannelPipeline pipeline = ch.pipeline();
                    //加入一个netty 提供 IdleStateHandler
                    /*
                    说明
                    1. IdleStateHandler 是netty 提供的处理空闲状态的处理器
                    2. long readerIdleTime : 表示多长时间没有读, 就会发送一个心跳检测包检测是否连接
                    3. long writerIdleTime : 表示多长时间没有写, 就会发送一个心跳检测包检测是否连接
                    4. long allIdleTime : 表示多长时间没有读写, 就会发送一个心跳检测包检测是否连接

                    5. 文档说明
                    triggers an {@link IdleStateEvent} when a {@link Channel} has not performed
 * read, write, or both operation for a while.
 *                  6. 当 IdleStateEvent 触发后 , 就会传递给管道 的下一个handler去处理
 *                  通过调用(触发)下一个handler 的 userEventTiggered , 在该方法中去处理 IdleStateEvent(读空闲，写空闲，读写空闲)
                     */
                    pipeline.addLast(new IdleStateHandler(7000, 7000, 10, TimeUnit.SECONDS));
                    //加入一个对空闲检测进一步处理的handler(自定义)
                    pipeline.addLast(new MyServerHandler());
                }
            });

            //启动服务器
            ChannelFuture channelFuture = serverBootstrap.bind(7000).sync();
            channelFuture.channel().closeFuture().sync();

        } finally {
            bossGroup.shutdownGracefully();
            workerGroup.shutdownGracefully();
        }
    }
```

- Bootstrap、ServerBootstrap

`Netty` 中 `Bootstrap` 类是客户端程序的启动引导类，`ServerBootstrap` 是服务端启动引导类。主要作用是配置整个 `Netty` 程序，串联各个组件

- Future、ChannelFuture

`Netty` 中所有的 `IO` 操作都是异步的，不能立刻得知消息是否被正确处理。但是可以注册一个监听，当操作执行成功或失败时就会自动触发注册的监听事件，具体的实现就是通过 `Future` 和 `ChannelFutures`

- Channel

  ![img](pics\chapter06_03.png)

1. `Netty` 网络通信的组件，能够用于执行网络 `I/O` 操作，类似socket。
2. 通过 `Channel` 可获得当前网络连接的通道的状态
3. 通过 `Channel` 可获得网络连接的配置参数（例如接收缓冲区大小）

- ChannelHandler及其实现类

ChannelHandler是一个接口，处理I/O事件或拦截I/O操作，并将其转发到其ChannelPipeline（业务处理链）中的下一个处理程序。

- ChannelPipeline 

保存 `ChannelHandler` 的 双向链表，让入站事件和出站操作变得有序。

`ChannelPipeline` 实现了一种高级形式的拦截过滤器模式，使用户可以完全控制事件的处理方式，以及 `Channel` 中各个的 `ChannelHandler` 如何相互交互。

- ChannelHandlerContext

1. 保存 `Channel` 相关的所有上下文信息，同时关联一个 `ChannelHandler` 对象
2. 即 `ChannelHandlerContext` 中包含一个具体的事件处理器 `ChannelHandler`，同时 `ChannelHandlerContext` 中也绑定了对应的 `pipeline` 和 `Channel` 的信息，方便对 `ChannelHandler` 进行调用。

- EventLoopGroup和其实现类NioEventLoopGroup

1. `EventLoopGroup` 是一组 `EventLoop` 的抽象，`Netty` 为了更好的利用多核 `CPU` 资源，一般会有多个 `EventLoop` 同时工作，每个 `EventLoop` 维护着一个 `Selector` 实例。
2. `EventLoopGroup` 提供 `next` 接口，可以从组里面按照一定规则获取其中一个 `EventLoop` 来处理任务。在 `Netty` 服务器端编程中，我们一般都需要提供两个 `EventLoopGroup`，例如：`BossEventLoopGroup` 和 `WorkerEventLoopGroup`。
3. 通常一个服务端口即一个 `ServerSocketChannel` 对应一个 `Selector` 和一个 `EventLoop` 线程。`BossEventLoop` 负责接收客户端的连接并将 `SocketChannel` 交给 `WorkerEventLoopGroup` 来进行 `IO` 处理。

参考文章：[Netty 核心模块组件](https://dongzl.github.io/netty-handbook/#/_content/chapter06?id=第-6-章-netty-核心模块组件)



#### **Netty编码解码器**

1. 当 `Netty` 发送或者接受一个消息的时候，就将会发生一次数据转换。入站消息会被解码：从字节转换为另一种格式（比如 `java` 对象）；如果是出站消息，它会被编码成字节。
2. `Netty` 提供一系列实用的编解码器，他们都实现了 `ChannelInboundHadnler` 或者 `ChannelOutboundHandler` 接口。在这些类中，`channelRead` 方法已经被重写了。以入站为例，对于每个从入站 `Channel` 读取的消息，这个方法会被调用。随后，它将调用由解码器所提供的 `decode()` 方法进行解码，并将已经解码的字节转发给 `ChannelPipeline` 中的下一个 `ChannelInboundHandler`。



#### **Netty 对TCP 粘包和拆包及解决方案**

分析原因：

Netty底层使用的是TCP协议，收发两端（客户端和服务器端）都要有一一成对的 `socket`，因此，发送端为了将多个发给接收端的包，更有效的发给对方，使用了优化方法（`Nagle` 算法），将多次间隔较小且数据量小的数据，合并成一个大的数据块，然后进行封包。这样做虽然提高了效率，但是接收端就难于分辨出完整的数据包了，因为==面向流的通信是无消息保护边界的==。

解决：

在没有 Netty 的情况下，用户如果自己需要拆包，基本原理就是不断从 TCP 缓冲区中读取数据，每次读取完都需要判断是否是一个完整的数据包 如果当前读取的数据不足以拼接成一个完整的业务数据包，那就保留该数据，继续从 TCP 缓冲区中读取，直到得到一个完整的数据包。 如果当前读到的数据加上已经读取的数据足够拼接成一个数据包，那就将已经读取的数据拼接上本次读取的数据，构成一个完整的业务数据包传递到业务逻辑，多余的数据仍然保留，以便和下次读到的数据尝试拼接。 

而在Netty中，已经造好了许多类型的拆包器，我们直接用就好：

![img](pics\166cf8197f460d3c~tplv-t2oaga2asx-watermark.awebp)

选好拆包器后，在代码中client段和server端将拆包器加入到chanelPipeline之中就好了

参考文章：[彻底理解Netty，这一篇文章就够了](https://juejin.cn/post/6844903703183360008)





#### **Netty 的零拷贝**

![img](pics\format.png)

传统意义的拷贝

是在发送数据的时候，传统的实现方式是：

通过`read()`把数据从硬盘读取到内核缓冲区，再复制到用户缓冲区；然后再通过`write()`写入到socket缓冲区，最后写入网卡设备。

整个过程发生了4次用户态和内核态的上下文切换和4次拷贝，具体流程如下：

用户进程通过`read()`方法向操作系统发起调用，此时上下文从用户态转向内核态

DMA控制器把数据从硬盘中拷贝到读缓冲区

CPU把读缓冲区数据拷贝到应用缓冲区，上下文从内核态转为用户态，`read()`返回

用户进程通过`write()`方法发起调用，上下文从用户态转为内核态

CPU将应用缓冲区中数据拷贝到socket缓冲区

DMA控制器把数据从socket缓冲区拷贝到网卡，上下文从内核态切换回用户态，`write()`返回

![img](pics\6d5541e6219348b296af0cf5da95d9b3~tplv-k3u1fbpfcp-watermark.awebp)

DMA（Direct Memory Access）直接内存访问技术，本质上来说他就是一块主板上独立的芯片，通过它来进行内存和IO设备的数据传输，从而减少CPU的等待时间。

那么对于零拷贝而言，并非真的是完全没有数据拷贝的过程，只不过是==减少用户态和内核态的切换次数以及CPU拷贝的次数==。

 （1）其中一种零拷贝技术是mmap+write简单来说就是使用`mmap`替换了read+write中的read操作，减少了一次CPU的拷贝。（Java -> FileChannel.map）

==mmap主要实现方式是将读缓冲区的地址和用户缓冲区的地址进行映射，内核缓冲区和应用缓冲区共享，从而减少了从读缓冲区到用户缓冲区的一次CPU拷贝。==

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1612743692ad4044b55dd372ed474c9b~tplv-k3u1fbpfcp-watermark.awebp)

整个过程发生了**4次用户态和内核态的上下文切换**和**3次拷贝**，具体流程如下：

1. 用户进程通过`mmap()`方法向操作系统发起调用，上下文从用户态转向内核态
2. DMA控制器把数据从硬盘中拷贝到读缓冲区
3. **上下文从内核态转为用户态，mmap调用返回**
4. 用户进程通过`write()`方法发起调用，上下文从用户态转为内核态
5. **CPU将读缓冲区中数据拷贝到socket缓冲区**
6. DMA控制器把数据从socket缓冲区拷贝到网卡，上下文从内核态切换回用户态，`write()`返回

`mmap`的方式节省了一次CPU拷贝，同时由于用户进程中的内存是虚拟的，只是映射到内核的读缓冲区，所以可以节省一半的内存空间，比较适合大文件的传输。

（2）相比`mmap`来说，`sendfile`同样减少了一次CPU拷贝，而且还减少了2次上下文切换。（Java -> FileChannel.transferTo）

![img](pics\25ea37c046234f5e82b61d6b77daf727~tplv-k3u1fbpfcp-watermark.awebp)

`sendfile`是Linux2.1内核版本后引入的一个系统调用函数，==通过使用sendfile数据可以直接在内核空间进行传输，因此避免了用户空间和内核空间的拷贝，同时由于使用sendfile替代了read+write从而节省了一次系统调用，也就是2次上下文切换==。

整个过程发生了**2次用户态和内核态的上下文切换**和**3次拷贝**，具体流程如下：

1. 用户进程通过`sendfile()`方法向操作系统发起调用，上下文从用户态转向内核态
2. DMA控制器把数据从硬盘中拷贝到读缓冲区
3. CPU将读缓冲区中数据拷贝到socket缓冲区
4. DMA控制器把数据从socket缓冲区拷贝到网卡，上下文从内核态切换回用户态，`sendfile`调用返回

`sendfile`方法IO数据对用户空间完全不可见，所以只能适用于完全不需要用户空间处理的情况，比如静态文件服务器。



Netty 中的零拷贝的实现是基于 Java 的，换言之，底层也是基于操作系统实现的。相对于 Java 中的零拷贝而言，Netty 的零拷贝更多的是偏向于优化数据操作的概念。

Netty 中的零拷贝体现在以下几个方面：

- 使用 Netty 提供的 `CompositeByteBuf` 类, 可以将多个`ByteBuf` 合并为一个逻辑上的 `ByteBuf`, 避免了各个 `ByteBuf` 之间的拷贝。
- `ByteBuf` 支持 slice 操作, 因此可以将 ByteBuf 分解为多个共享同一个存储区域的 `ByteBuf`, 避免了内存的拷贝。
- 通过 `FileRegion` 包装的`FileChannel.tranferTo` 实现文件传输, 可以直接将文件缓冲区的数据发送到目标 `Channel`, 避免了传统通过循环 write 方式导致的内存拷贝问题.

参考文章：[阿里二面：什么是mmap？](https://juejin.cn/post/6962404435620266015) 、[Netty的零拷贝](https://juejin.cn/post/6995789317947785223)





#### **Netty 内部执行流程**

![img](C:\Users\Lenovo\Desktop\笔记\CS-Learning-Notes\notes\pics\166cf93e830044e7~tplv-t2oaga2asx-watermark.awebp)

服务端（客户端同）：

1、创建ServerBootStrap实例

2、设置并绑定Reactor线程池：EventLoopGroup，EventLoop就是处理所有注册到本线程的Selector上面的Channel

3、设置并绑定服务端的channel

4、5、创建处理网络事件的ChannelPipeline和handler，网络时间以流的形式在其中流转，handler完成多数的功能定制：比如编解码 SSl安全认证

6、绑定并启动监听端口

7、当轮训到准备就绪的channel后，由Reactor线程：NioEventLoop执行pipline中的方法，最终调度并执行channelHandler 



#### **问：为什么要用 Netty 呢？**

答：因为 Netty 具有下面这些优点，并且相比于直接使用 JDK 自带的 NIO 相关的 API 来说更加易用。

- 统一的 API，支持多种传输类型，阻塞和非阻塞的。
- 简单而强大的线程模型。
- 自带编解码器解决 TCP 粘包/拆包问题。
- 自带各种协议栈。
- 真正的无连接数据包套接字支持。
- 比直接使用 Java 核心 API 有更高的吞吐量、更低的延迟、更低的资源消耗和更少的内存复制。
- 安全性不错，有完整的 SSL/TLS 以及 StartTLS 支持。
- 社区活跃
- 成熟稳定，经历了大型项目的使用和考验，而且很多开源项目都使用到了 Netty， 比如我们经常接触的 Dubbo、RocketMQ 等等。



#### **问：能不能通俗地说一下使用 Netty 可以做什么事情？**

答：理论上来说，NIO 可以做的事情 ，使用 Netty 都可以做并且更好。Netty 主要用来做网络通信 :

1. 作为 RPC 框架的网络通信工具 ：我们在分布式系统中，不同服务节点之间经常需要相互调用，这个时候就需要 RPC 框架了。不同服务节点之间的通信是如何做的呢？可以使用 Netty 来做。比如我调用另外一个节点的方法的话，至少是要让对方知道我调用的是哪个类中的哪个方法以及相关参数吧！
2. 实现一个自己的 HTTP 服务器 ：通过 Netty 我们可以自己实现一个简单的 HTTP 服务器，这个大家应该不陌生。说到 HTTP 服务器的话，作为 Java 后端开发，我们一般使用 Tomcat 比较多。一个最基本的 HTTP 服务器可要以处理常见的 HTTP Method 的请求，比如 POST 请求、GET 请求等等。
3. 实现一个即时通讯系统 ：使用 Netty 我们可以实现一个可以聊天类似微信的即时通讯系统，这方面的开源项目还蛮多的，可以自行去 Github 找一找。
4. 实现消息推送系统：市面上有很多消息推送系统都是基于 Netty 来做的。

参考文章：[Netty常见面试题总结](https://mp.weixin.qq.com/s/eJ-dAtOYsxylGL7pBv7VVA)



#### **问：你再说说自己对 Bootstrap 和 ServerBootstrap 的了解**

答：`Bootstrap` 是客户端的启动引导类/辅助类，而`ServerBootstrap` 服务端的启动引导类/辅助类。

在使用上：

1. `Bootstrap` 通常使用 `connet()` 方法连接到远程的主机和端口，作为一个 Netty TCP 协议通信中的客户端。另外，`Bootstrap` 也可以通过 `bind()` 方法绑定本地的一个端口，作为 UDP 协议通信中的一端。
2. `ServerBootstrap`通常使用 `bind()` 方法绑定本地的端口上，然后等待客户端的连接。
3. `Bootstrap` 只需要配置一个线程组— `EventLoopGroup` ,而 `ServerBootstrap`需要配置两个线程组— `EventLoopGroup` ，一个用于接收连接，一个用于具体的处理。



#### **问：NioEventLoopGroup 默认的构造函数会起多少线程？**

答：`NioEventLoopGroup` 默认的构造函数实际会起的线程数为 **`CPU核心数*2`**



#### **问：Netty 线程模型了解么？**

答：大部分网络框架都是基于 Reactor 模式设计开发的。

在 Netty 主要靠 `NioEventLoopGroup` 线程池来实现具体的线程模型的 。

我们实现服务端的时候，一般会初始化两个线程组：

1. **`bossGroup`** :接收连接。
2. **`workerGroup`** ：负责具体的处理，交由对应的 Handler 处理。

下面我们来详细看一下 Netty 中的线程模型吧！



#### **问：Netty 服务端和客户端的启动过程了解么？**

简单解析一下服务端的创建过程具体是怎样的：

1.首先你创建了两个 `NioEventLoopGroup` 对象实例：`bossGroup` 和 `workerGroup`。

- `bossGroup` : 用于处理客户端的 TCP 连接请求。
- `workerGroup` ：负责每一条连接的具体读写数据的处理逻辑，真正负责 I/O 读写操作，交由对应的 Handler 处理。

2.接下来 我们创建了一个服务端启动引导/辅助类：`ServerBootstrap`，这个类将引导我们进行服务端的启动工作。

3.通过 `.group()` 方法给引导类 `ServerBootstrap` 配置两大线程组，确定了线程模型。

4.通过`channel()`方法给引导类 `ServerBootstrap`指定了 IO 模型为`NIO`

- `NioServerSocketChannel` ：指定服务端的 IO 模型为 NIO，与 BIO 编程模型中的`ServerSocket`对应
- `NioSocketChannel` : 指定客户端的 IO 模型为 NIO， 与 BIO 编程模型中的`Socket`对应

5.通过 `.childHandler()`给引导类创建一个`ChannelInitializer` ，然后指定了服务端消息的业务处理逻辑 `HelloServerHandler` 对象

6.调用 `ServerBootstrap` 类的 `bind()`方法绑定端口

继续分析一下客户端的创建流程：

1.创建一个 `NioEventLoopGroup` 对象实例

2.创建客户端启动的引导类是 `Bootstrap`

3.通过 `.group()` 方法给引导类 `Bootstrap` 配置一个线程组

4.通过`channel()`方法给引导类 `Bootstrap`指定了 IO 模型为`NIO`

5.通过 `.childHandler()`给引导类创建一个`ChannelInitializer` ，然后指定了客户端消息的业务处理逻辑 `HelloClientHandler` 对象

6.调用 `Bootstrap` 类的 `connect()`方法进行连接，这个方法需要指定两个参数：

- `inetHost` : ip 地址
- `inetPort` : 端口号

`connect` 方法返回的是一个 `Future` 类型的对象，也就是说这个方是异步的，我们通过 `addListener` 方法可以监听到连接是否成功。



#### **问：什么是 TCP 粘包/拆包?有什么解决办法呢？**

TCP 粘包/拆包：

TCP的粘包是指TCP协议可能将数个小的包封装成一个大的包来进行传输.

而分包就是指TCP协议在进行传输时将一个大的包拆分称为数个小的包来进行数据传输.

解决办法有：

**1.使用 Netty 自带的解码器**

- **`LineBasedFrameDecoder`** : 发送端发送数据包的时候，每个数据包之间以换行符作为分隔，`LineBasedFrameDecoder` 的工作原理是它依次遍历 `ByteBuf` 中的可读字节，判断是否有换行符，然后进行相应的截取。
- **`DelimiterBasedFrameDecoder`** : 可以自定义分隔符解码器，**`LineBasedFrameDecoder`** 实际上是一种特殊的 `DelimiterBasedFrameDecoder` 解码器。
- **`FixedLengthFrameDecoder`**: 固定长度解码器，它能够按照指定的长度对消息进行相应的拆包。
- **`LengthFieldBasedFrameDecoder`**

**2.自定义序列化编解码器**

在 Java 中自带的有实现 `Serializable` 接口来实现序列化，但由于它性能、安全性等原因一般情况下是不会被使用到的。

通常情况下，我们使用 Protostuff、Hessian2、json 序列方式比较多，另外还有一些序列化性能非常好的序列化方式也是很好的选择：

- 专门针对 Java 语言的：Kryo，FST 等等
- 跨语言的：Protostuff（基于 protobuf 发展而来），ProtoBuf，Thrift，Avro，MsgPack 等等



#### **问：为什么需要心跳机制？Netty 中心跳机制了解么？**

答：我们知道 TCP 在进行读写之前，server 与 client 之间必须提前建立一个连接。建立连接的过程，需要我们常说的三次握手，释放/关闭连接的话需要四次挥手。这个过程是比较消耗网络资源并且有时间延迟的。

所谓，短连接说的就是 server 端 与 client 端建立连接之后，读写完成之后就关闭掉连接，如果下一次再要互相发送消息，就要重新连接。短连接的有点很明显，就是管理和实现都比较简单，缺点也很明显，每一次的读写都要建立连接必然会带来大量网络资源的消耗，并且连接的建立也需要耗费时间。

长连接说的就是 client 向 server 双方建立连接之后，即使 client 与 server 完成一次读写，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。长连接的可以省去较多的 TCP 建立和关闭的操作，降低对网络资源的依赖，节约时间。对于频繁请求资源的客户来说，非常适用长连接。

在 TCP 保持长连接的过程中，可能会出现断网等网络异常出现，异常发生的时候， client 与 server 之间如果没有交互的话，它们是无法发现对方已经掉线的。为了解决这个问题, 我们就需要引入 心跳机制 。

心跳机制的工作原理是: 在 client 与 server 之间在一定时间内没有数据交互时, 即处于 idle 状态时, 客户端或服务器就会发送一个特殊的数据包给对方, 当接收方收到这个数据报文后, 也立即发送一个特殊的数据报文, 回应发送方, 此即一个 PING-PONG 交互。所以, 当某一端收到心跳消息后, 就知道了对方仍然在线, 这就确保 TCP 连接的有效性.

TCP 实际上自带的就有长连接选项，本身是也有心跳包机制，也就是 TCP 的选项：`SO_KEEPALIVE`。但是，TCP 协议层面的长连接灵活性不够。所以，一般情况下我们都是在应用层协议上实现自定义心跳机制的，也就是在 Netty 层面通过编码实现。通过 Netty 实现心跳机制的话，核心类是 `IdleStateHandler` 。



Netty中用虚引用检测内存泄漏

https://www.jianshu.com/p/ff45bcd82095





## Dubbo

#### Q：SPI机制简介

SPI是Service Provider Interface 的简称，即服务提供者接口的意思。SPI说白了就是一种扩展机制，我们在相应配置文件中定义好某个接口的实现类，然后再根据这个接口去这个配置文件中加载这个实现类并实例化。有了SPI机制，那么就为一些框架的灵活扩展提供了可能，而不必将框架的一些实现类写死在代码里面。

SPI机制的主要目的

1. **为了解耦，将接口和具体实现分离开来；**
2. **提高框架的扩展性。以前写程序的时候，接口和实现都写在一起，调用方在使用的时候依赖接口来进行调用，无权选择使用具体的实现类。**

Dubbo的SPI机制：

Dubbo更是把SPI机制应用的淋漓尽致，Dubbo基本上自身的每个功能点都提供了扩展点，比如提供了集群扩展，路由扩展和负载均衡扩展等差不多接近30个扩展点。如果Dubbo的某个内置实现不符合我们的需求，那么我们只要利用其SPI机制将我们的实现替换掉Dubbo的实现即可。
上面的三个栗子先让我们直观感受下某些框架利用SPI机制是如何做到灵活扩展的。



基础知识

参考文章：[Dubbo 面试题/知识点总结](https://mp.weixin.qq.com/s/2qSA6aJn6KRXrATVE44k0w)





## RPC

#### **问：RPC 是什么？**

RPC 称远程过程调用（Remote Procedure Call），用于解决分布式系统中服务之间的调用问题。 通俗地讲，就是开发者能够像调用本地方法一样调用远程的服务。 所以，RPC的作用主要体现在这两个方面：

- 屏蔽远程调用跟本地调用的区别，让我们感觉就是调用项目内的方法；
- 隐藏底层网络通信的复杂性，让我们更专注于业务逻辑。

**RPC 框架基本架构**

RPC 框架包含三个最重要的组件，分别是客户端、服务端和注册中心。在一次 RPC 调用流程中，这三个组件是这样交互的：

- 服务端在启动后，会将它提供的服务列表发布到注册中心，客户端向注册中心订阅服务地址；
- 客户端会通过本地代理模块 Proxy 调用服务，Proxy 模块收到负责将方法、参数等数据转化成网络字节流；
- 客户端从服务列表中选取其中一个的服务地址，并将数据通过网络发送给服务端；
- 服务端接收到数据后进行解码，得到请求信息；
- 服务端根据解码后的请求信息调用对应的服务，然后将调用结果返回给客户端。

![rpc](C:\Users\Lenovo\Desktop\笔记\CS-Learning-Notes\notes\pics\rpc.png)

从上面这张图中，可以看见 RPC 框架一般有这些组件：服务治理(注册发现)、负载均衡、容错、序列化/反序列化、编解码、网络传输、线程池、动态代理等角色，当然有的RPC框架还会有连接池、日志、安全等角色。

1. **客户端（服务消费端）** ：调用远程方法的一端。
2. **客户端 Stub（桩）** ： 这其实就是一代理类。代理类主要做的事情很简单，就是把你调用方法、类、方法参数等信息传递到服务端。
3. **网络传输** ： 网络传输就是你要把你调用的方法的信息比如说参数啊这些东西传输到服务端，然后服务端执行完之后再把返回结果通过网络传输给你传输回来。网络传输的实现方式有很多种比如最近基本的 Socket或者性能以及封装更加优秀的 Netty（推荐）。
4. **服务端 Stub（桩）** ：这个桩就不是代理类了。我觉得理解为桩实际不太好，大家注意一下就好。这里的服务端 Stub 实际指的就是接收到客户端执行方法的请求后，去指定对应的方法然后返回结果给客户端的类。
5. **服务端（服务提供端）** ：提供远程方法的一端。

**具体调用过程**

1）服务消费方（client）调用以本地调用方式调用服务；

2）client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体；

3）client stub找到服务地址，并将消息发送到服务端；

4）server stub收到消息后进行解码；

5）server stub根据解码结果调用本地的服务；

6）本地服务执行并将结果返回给server stub；

7）server stub将返回结果打包成消息并发送至消费方；

8）client stub接收到消息，并进行解码；

9）服务消费方得到最终结果。

RPC的目标就是要2~8这些步骤都封装起来，让用户对这些细节透明。

参考文章：[如何手撸一个较为完整的RPC框架](https://juejin.cn/post/6992867064952127524) 



#### **问：RPC框架需要解决的问题**

RPC要达到的目标：远程调用时，要能够像本地调用一样方便，让调用者感知不到远程调用的逻辑。

**1、 怎么做到透明化远程服务调用？**

使用代理，一般RPC框架采用动态代理方式，通过实现一个RpcProxyClient 代理类，代理类的invoke 方法中封装与远端服务通信的细节，消费方可以从RPCProxyClient 获得服务提供方的接口，当执行方法时就会调用invoke 方法。

**2、怎么对消息进行编码和解码？**

远程调用中，因为两个进程的地址空间不一样，所以无法通过调用函数指针调用函数，也没法通过内存传递参数。

使用网络通信的第一步，就是要确定客户端和服务端相互通信的消息结构。客户端的请求消息结构一般需要包括以下内容：

- 接口名称、方法名称、参数类型和参数值；
- requestId，标识唯一请求id等等。

同理，服务端返回的消息结构一般包括：返回值，状态码和requestId等。

一旦确定了消息的数据结构后，下一步就是要考虑序列化与反序列化。通常要重合考虑通用性、性能和可扩展性。

**3、如何实现网络通信？**

消息数据结构被序列化为二进制流后，下一步就要进行网络通信。一般直接是使用基于netty的通信框架。

**4、如何实现服务发布和订阅？**

分布式架构中，一个服务势必会有多个实例，需要解决如何获取实例的问题。采用zookeeper实现服务自动注册与发现功能，zookeeper可以充当一个服务注册表（Service Registry），让多个服务提供者形成一个集群，让服务消费者通过服务注册表获取具体的服务访问地址（ip+端口）去访问具体的服务提供者。

此外，zookeeper提供了“心跳检测”功能，它会定时向各个服务提供者发送一个请求（实际上建立的是一个 Socket 长连接），如果长期没有响应，服务中心就认为该服务提供者已经“挂了”，并将其剔除。

服务消费者也会去监听相应路径，一旦路径上的数据有任务变化（增加或减少），zookeeper都会通知服务消费方服务提供者地址列表已经发生改变，从而进行更新。

**5、Redis缓存**

如果每次都去注册中心查询列表，效率很低，那么就要加缓存。有了缓存，就要考虑缓存的更新问题

**6、异步调用AIO**

客户端总不能每次调用完都等着服务端返回数据，所以就要支持异步调用。AIO作为异步非阻塞，AIO发起IO操作之后，通知服务器去完成函数操作，这个时间客户端可以去做其他的事情。等到服务器完成操作之后，就会调用客户端的接口，返回结果数据。

**7、使用线程池**

针对高并发的客户端请求，服务端可以维护一个线程池，针对每次的客户端的请求，可以直接从线程池中拿出一个线程，直接去执行客户端的任务，省去了每次创建线程的开销，同时提高了服务端的响应速度。

参考文章：[深入理解 RPC](https://juejin.cn/post/6844903443237175310) 、[设计RPC框架应考虑的问题](https://blog.csdn.net/weixin_41907511/article/details/102834674) 、[一个优秀的RPC框架需要考虑的问题](https://blog.csdn.net/weixin_37704921/article/details/89212111?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2.pc_relevant_aa&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2.pc_relevant_aa&utm_relevant_index=5)



#### **问：自己动手从0开始实现一个分布式 RPC 框架**

表达思路：[如何设计一个 RPC 框架](https://juejin.cn/post/6870276943448080392)

参考文章：[自己动手从0开始实现一个分布式 RPC 框架](https://zhuanlan.zhihu.com/p/388848964)



#### **问：RPC心跳机制相关讨论**

参考文章：[面试官：要不我们聊一下“心跳”的设计？](https://juejin.cn/post/7041405419713429511)



#### **问：负载均衡是怎么做的，如何考虑？**

参考文章：[五种负载均衡策略](https://www.whywhy.vip/archives/40)





## Kafka

**队列模型：早期的消息模型**



![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/2/20/17060959ff5dea6d~tplv-t2oaga2asx-watermark.awebp)



**使用队列（Queue）作为消息通信载体，满足生产者与消费者模式，一条消息只能被一个消费者使用，未被消费的消息在队列中保留直到被消费或超时。** 比如：我们生产者发送 100 条消息的话，两个消费者来消费一般情况下两个消费者会按照消息发送的顺序各自消费一半（也就是你一个我一个的消费。）

**队列模型存在的问题**

假如我们存在这样一种情况：我们需要将生产者产生的消息分发给多个消费者，并且每个消费者都能接收到完成的消息内容。

这种情况，队列模型就不好解决了。很多比较杠精的人就说：我们可以为每个消费者创建一个单独的队列，让生产者发送多份。这是一种非常愚蠢的做法，浪费资源不说，还违背了使用消息队列的目的。

**发布-订阅模型:Kafka 消息模型**

发布-订阅模型主要是为了解决队列模型存在的问题。



![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/2/20/1706095a37e769ed~tplv-t2oaga2asx-watermark.awebp)



发布订阅模型（Pub-Sub） 使用**主题（Topic）** 作为消息通信载体，类似于**广播模式**；发布者发布一条消息，该消息通过主题传递给所有的订阅者，**在一条消息广播之后才订阅的用户则是收不到该条消息的**。

**在发布 - 订阅模型中，如果只有一个订阅者，那它和队列模型就基本是一样的了。所以说，发布 - 订阅模型在功能层面上是可以兼容队列模型的。**

**Kafka 采用的就是发布 - 订阅模型。**

参考问题：https://juejin.cn/post/6844904068771479560





# 分布式基础

#### 问：分布式与集群的区别

答：分布式（distributed）是指在多台不同的服务器中部署不同的服务模块，通过远程调用协同工作，对外提供服务。

集群（cluster）是指在多台不同的服务器中部署相同应用或服务模块，构成一个集群，通过负载均衡设备对外提供服务。



#### **问：谈谈自己对分布式的理解**

分布式或者说 SOA 分布式重要的就是面向服务，说简单的分布式就是我们把整个系统拆分成不同的服务然后将这些服务放在不同的服务器上减轻单体服务的压力提高并发量和性能。

现在常用的开源分布式框架一个是阿里开源的dubbo，还有一个就是Spring cloud。

最初的服务化解决方案是  相同服务提供一个统一的域名，然后客户端发送http请求，由Nginx负责请求分发和跳转，耦合了服务调用逻辑，相当于一个重量级的ESB；有以下几个缺点：

1. 作为消费者不知道由哪个服务实例提供服务
2. 无法观测到服务消费者和服务提供者之间的通信频率和运行状况
3. 消费者的失败重发，加大了服务开发难度，Nginx没有统一的管理策略

综合种种原因，分布式框架诞生，dubbo就是一款很优秀的开源框架；他集成了服务注册中心和服务监控中心

服务提供者通过服务注册中心注册自己需要提供的服务，消费者则从注册中心订阅自己需要的服务；监控中心则是

统计消费者和提供者之间的通信频率、调度时间、运行状况


从框架的自身结构来看，dubbo也有其缺点

1. 服务提供者和服务消费，很依赖第三方组件（zookeeper,redis）,一旦该组件出了问题，框架本身也会启不起来
2. 消费者严重依赖服务提供者，双方代码耦合度较高，一旦服务提供者在导jar包的过程中出错，程序就会出现问题


分布式框架系统定理 ： C —— 数据一致性   A ——  服务可用性  P —— 服务对网络分区故障的容错性，分布式框架很难都满足，一般符合其中两者;包括dubbo在内的其它使用zookeeper的分布式框架是满足CP，因为当客户端发送请求时，集群正在进行master选举或者半数以上的机器宕掉，服务可用性就很难做到；而对于持续集成，快速演化微服务来说，可用性就显得尤为重要，spring cloud由此诞生。


spring cloud和dubbo的区别，两者的区别也是各自的优劣

1：dubbo的服务注册与发现是用的zookeeper，spring cloud服务注册与发现用的是Eureka 后者各个节点之间都是平等的不存在主从关系，只要一个节点还在，就能保证服务正常调用，即使全部节点都死掉，服务与服务之间也能通过缓存调用信息，这就保证了微服务之间的调用足够的健壮

2：对于调用方式dubbo采用rpc的方式，代码耦合度高，spring cloud中的提供方和消费方通过http  rest方式，不存在代码的强依赖显得更为灵活

参考文章：[谈谈自己对分布式的理解](https://blog.csdn.net/u012547613/article/details/79283196)



#### **问：分布式锁的几种实现**

参考文章：[分布式锁的几种实现方式~](https://juejin.cn/post/6844903585008861191)



#### **问：CAP理论**

答：CAP理论告诉我们，一个分布式系统不可能同时满足以下三种：

- 一致性（C:Consistency）：所有节点访问同一份最新的数据副本

- 可用性（A:Available）：非故障的节点在合理的时间内返回合理的响应
- 分区容错性（P:Partition Tolerance）：分布式系统出现网络分区的时候，仍然能够对外提供服务。

> 分布式系统中，多个节点之前的网络本来是连通的，但是因为某些故障（比如部分节点网络出了问题）某些节点之间不连通了，整个网络就分成了几块区域，这就叫网络分区。

==当发生网络分区的时候，如果我们要继续服务，那么强一致性和可用性只能 2 选 1==。也就是说当网络分区之后 P 是前提，决定了 P 之后才有 C 和 A 的选择。也就是说分区容错性（Partition tolerance）我们是必须要实现的。

如果系统没有发生“分区”的话，节点间的网络连接通信正常的话，也就不存在 P 了。这个时候，我们就可以同时保证 C 和 A 了。

简而言之就是：CAP 理论中分区容错性 P 是一定要满足的，在此基础上，只能满足可用性 A 或者一致性 C。

举个例子：若系统出现“分区”，系统中的某个节点在进行写操作。为了保证 C， 必须要禁止其他节点的读写操作，这就和 A 发生冲突了。如果为了保证 A，其他节点的读写操作正常的话，那就和 C 发生冲突了。

Zookeeper保证CP，任何时刻对 ZooKeeper 的读请求都能得到一致性的结果，但是， ZooKeeper 不保证每次请求的可用性比如在 Leader 选举过程中或者半数以上的机器不可用的时候服务就是不可用的。

Eureka保证AP，Eureka 在设计的时候就是优先保证 A （可用性）。在 Eureka 中不存在什么 Leader 节点，每个节点都是一样的、平等的。因此 Eureka 不会像 ZooKeeper 那样出现选举过程中或者半数以上的机器不可用的时候服务就是不可用的情况。 Eureka 保证即使大部分节点挂掉也不会影响正常提供服务，只要有一个节点是可用的就行了。只不过这个节点上的数据可能并不是最新的。

Nacos则保证AP或者CP。

参考文章：[CAP和BASE](https://juejin.cn/post/6898288789371027470)、[一致性算法](http://www.yswang.tech/archives/zookeeper-basic#FXEjdWhA)



#### **问：BASE理论**

答：BASE 是 Basically Available（基本可用） 、Soft-state（软状态） 和 Eventually Consistent（最终一致性）。

基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。指响应时间或功能上的损失。

软状态是指允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。

最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。

BASE 理论的核心思想是==即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性==。AP 方案只是在系统发生分区的时候放弃一致性，而不是永远放弃一致性。在分区故障恢复后，系统应该达到最终一致性。这一点其实就是 BASE 理论延伸的地方。

总结：**ACID 是数据库事务完整性的理论，CAP 是分布式系统设计理论，BASE 是 CAP 理论中 AP 方案的延伸。**



#### **问：2PC（两阶段提交）**

答：两阶段提交（2 phase-commit）是一种保证分布式系统数据一致性的协议，现在很多数据库都是采用的两阶段提交协议来完成 分布式事务 的处理。实现整个调用链中，我们所有服务的数据处理要么都成功要么都失败，即所有服务的 原子性问题 。

在两阶段提交中，主要涉及到两个角色，分别是协调者和参与者。（类似王者荣耀五排，只能5个人凑齐才能点开始）

**提交请求（投票）阶段**

![img](C:\Users\Lenovo\Desktop\笔记\CS-Learning-Notes\notes\pics\16cefb83734bde82~tplv-t2oaga2asx-watermark.awebp)

- 协调者向所有参与者发送prepare请求与事务内容，询问是否可以准备事务提交，并等待参与者的响应。
- 参与者执行事务中包含的操作，并记录undo日志（用于回滚）和redo日志（用于重放），但不真正提交。
- 参与者向协调者返回事务操作的执行结果，执行成功返回yes，否则返回no。

**提交（执行）阶段**

分为成功与失败两种情况。

![img](C:\Users\Lenovo\Desktop\笔记\CS-Learning-Notes\notes\pics\16cefb838f46db4e~tplv-t2oaga2asx-watermark.awebp)

若所有参与者都返回yes，说明事务可以提交：

- 协调者向所有参与者发送commit请求。
- 参与者收到commit请求后，将事务真正地提交上去，并释放占用的事务资源，并向协调者返回ack。
- 协调者收到所有参与者的ack消息，事务成功完成。

若有参与者返回no或者超时未返回，说明事务中断，需要回滚：

- 协调者向所有参与者发送rollback请求。
- 参与者收到rollback请求后，根据undo日志回滚到事务执行前的状态，释放占用的事务资源，并向协调者返回ack。
- 协调者收到所有参与者的ack消息，事务回滚完成。

虽然2PC解决了各个事务的原子性问题，但也随之产生了许多新的问题：

- **单点故障问题**，如果协调者挂了那么整个系统都处于不可用的状态了。
- **阻塞问题**，即当协调者发送 prepare 请求，参与者收到之后如果能处理那么它将会进行事务的处理但并不提交，这个时候会一直占用着资源不释放，如果此时协调者挂了，那么这些资源都不会再释放了，这会极大影响性能。
- **数据不一致问题**，比如当第二阶段，协调者只发送了一部分的 commit 请求就挂了，那么也就意味着，收到消息的参与者会进行事务的提交，而后面没收到的则不会进行事务提交，那么这时候就会产生数据不一致性问题

参考文章：[2PC](https://juejin.cn/post/6844903929847742478)



#### 问：3PC（三阶段提交）

三阶段提交在协调者和参与者中引入超时机制，并且把两阶段提交的第一阶段拆分为两步：询问，然后锁住资源，最后真正提交。

![image.png](C:\Users\Lenovo\Desktop\笔记\CS-Learning-Notes\notes\pics\3PC.png)

1. **CanCommit阶段**：协调者向所有参与者发送 `CanCommit` 请求，参与者收到请求后会根据自身情况查看是否能执行事务，如果可以则返回 YES 响应并进入预备状态，否则返回 NO 。
2. **PreCommit阶段**：协调者根据参与者返回的响应来决定是否可以进行下面的 `PreCommit` 操作。如果上面参与者返回的都是 YES，那么协调者将向所有参与者发送 `PreCommit` 预提交请求，**参与者收到预提交请求后，会进行事务的执行操作，并将 `Undo` 和 `Redo` 信息写入事务日志中** ，最后如果参与者顺利执行了事务则给协调者返回成功的响应。如果在第一阶段协调者收到了 **任何一个 NO** 的信息，或者 **在一定时间内** 并没有收到全部的参与者的响应，那么就会中断事务，它会向所有参与者发送中断请求（abort），参与者收到中断请求之后会立即中断事务，或者在一定时间内没有收到协调者的请求，它也会中断事务。
3. **DoCommit阶段**：这个阶段其实和 `2PC` 的第二阶段差不多，如果协调者收到了所有参与者在 `PreCommit` 阶段的 YES 响应，那么协调者将会给所有参与者发送 `DoCommit` 请求，**参与者收到 `DoCommit` 请求后则会进行事务的提交工作**，完成后则会给协调者返回响应，协调者收到所有参与者返回的事务提交成功的响应之后则完成事务。若协调者在 `PreCommit` 阶段 **收到了任何一个 NO 或者在一定时间内没有收到所有参与者的响应** ，那么就会进行中断请求的发送，参与者收到中断请求后则会 **通过上面记录的回滚日志** 来进行事务的回滚操作，并向协调者反馈回滚状况，协调者收到参与者返回的消息后，中断事务。

3PC 通过一系列的超时机制很好的缓解了阻塞问题，但是最重要的一致性并没有得到根本的解决，比如在 `PreCommit` 阶段，当一个参与者收到了请求之后其他参与者和协调者挂了或者出现了网络分区，这个时候收到消息的参与者都会进行事务提交，这就会出现数据不一致性问题。



*问：Paxos算法

Paxos 算法是基于**消息传递且具有高度容错特性的一致性算法**，是目前公认的解决分布式一致性问题最有效的算法之一，**其解决的问题就是在分布式系统中如何就某个值（决议）达成一致** 。

在 Paxos 中主要有三个角色，分别为 `Proposer提案者`、`Acceptor表决者`、`Learner学习者`。`Paxos` 算法和 `2PC` 一样，也有两个阶段，分别为 `Prepare` 和 `accept` 阶段。

**prepare 阶段**

- `Proposer提案者`：负责提出 `proposal`，每个提案者在提出提案时都会首先获取到一个 **具有全局唯一性的、递增的提案编号N**，即在整个集群中是唯一的编号 N，然后将该编号赋予其要提出的提案，在**第一阶段是只将提案编号发送给所有的表决者**。
- `Acceptor表决者`：每个表决者在 `accept` 某提案后，会将该提案编号N记录在本地，这样每个表决者中保存的已经被 accept 的提案中会存在一个**编号最大的提案**，其编号假设为 `maxN`。每个表决者仅会 `accept` 编号大于自己本地 `maxN` 的提案，在批准提案时表决者会将以前接受过的最大编号的提案作为响应反馈给 `Proposer` 。

> 下面是 `prepare` 阶段的流程图，你可以对照着参考一下。

![paxos第一阶段](http://www.yswang.tech/upload/2021/12/22e8d512d954676bdf0cc92d200af8ef-977745d37b124619af688e0f7f311461.png)

**accept 阶段**

当一个提案被 `Proposer` 提出后，如果 `Proposer` 收到了超过半数的 `Acceptor` 的批准（`Proposer` 本身同意），那么此时 `Proposer` 会给所有的 `Acceptor` 发送真正的提案（你可以理解为第一阶段为试探），这个时候 `Proposer` 就会发送提案的内容和提案编号。

表决者收到提案请求后会再次比较本身已经批准过的最大提案编号和该提案编号，如果该提案编号 **大于等于** 已经批准过的最大提案编号，那么就 `accept` 该提案（此时执行提案内容但不提交），随后将情况返回给 `Proposer` 。如果不满足则不回应或者返回 NO 。

![paxos第二阶段1](http://www.yswang.tech/upload/2021/12/b82536f956f70a584c6a20c10113f225-66b897c36bfd42969a390336fa5142b3.png)

当 `Proposer` 收到超过半数的 `accept` ，那么它这个时候会向所有的 `acceptor` 发送提案的提交请求。需要注意的是，因为上述仅仅是超过半数的 `acceptor` 批准执行了该提案内容，其他没有批准的并没有执行该提案内容，所以这个时候需要**向未批准的 `acceptor` 发送提案内容和提案编号并让它无条件执行和提交**，而对于前面已经批准过该提案的 `acceptor` 来说 **仅仅需要发送该提案的编号** ，让 `acceptor` 执行提交就行了。

![paxos第二阶段2](http://www.yswang.tech/upload/2021/12/743889b97485fdfe2094e5ef0af6b141-a85c6a7b19644f0baa84e31d5c064e4f.png)

而如果 `Proposer` 如果没有收到超过半数的 `accept` 那么它将会将 **递增** 该 `Proposal` 的编号，然后 **重新进入 `Prepare` 阶段** 。

**paxos 算法的死循环问题**

其实就有点类似于两个人吵架，小明说我是对的，小红说我才是对的，两个人据理力争的谁也不让谁。

比如说，此时提案者 P1 提出一个方案 M1，完成了 `Prepare` 阶段的工作，这个时候 `acceptor` 则批准了 M1，但是此时提案者 P2 同时也提出了一个方案 M2，它也完成了 `Prepare` 阶段的工作。然后 P1 的方案已经不能在第二阶段被批准了（因为 `acceptor` 已经批准了比 M1 更大的 M2），所以 P1 自增方案变为 M3 重新进入 `Prepare` 阶段，然后 `acceptor` ，又批准了新的 M3 方案，它又不能批准 M2 了，这个时候 M2 又自增进入 `Prepare` 阶段。。。

就这样无休无止的永远提案下去，这就是 `paxos` 算法的死循环问题。

那么如何解决呢？很简单，人多了容易吵架，我现在 **就允许一个能提案** 就行了。



问：ZAB协议

Zab协议 的全称是 Zookeeper Atomic Broadcast （Zookeeper原子广播）。==Zookeeper 是通过 Zab 协议来保证分布式事务的最终一致性==。

Zab借鉴了Paxos算法，在 ZAB 中三个主要的角色，Leader 领导者、Follower跟随者、Observer观察者 。

- Leader ：集群中 唯一的写请求处理者 ，能够发起投票（投票也是为了进行写请求）。
- Follower：能够接收客户端的请求，如果是读请求则可以自己处理，如果是写请求则要转发给 Leader 。在选举过程中会参与投票，有选举权和被选举权 。
- Observer ：就是没有选举权和被选举权的 Follower 。

Zab 协议包括两种基本的模式：**崩溃恢复** 和 **消息广播**

协议过程：

（1）当整个集群启动过程中，或者当 Leader 服务器出现网络中弄断、崩溃退出或重启等异常时，Zab协议就会 进入崩溃恢复模式，选举产生新的Leader。

（2）当选举产生了新的 Leader，同时集群中有过半的机器与该 Leader 服务器完成了状态同步（即数据同步）之后，Zab协议就会退出崩溃恢复模式，进入消息广播模式。

（3）这时，如果有一台遵守Zab协议的服务器加入集群，因为此时集群中已经存在一个Leader服务器在广播消息，那么该新加入的服务器自动进入恢复模式：找到Leader服务器，并且完成数据同步。同步完成后，作为新的Follower一起参与到消息广播流程中。

协议状态切换：

当Leader出现崩溃退出或者机器重启，亦或是集群中不存在超过半数的服务器与Leader保存正常通信，Zab就会再一次进入崩溃恢复，发起新一轮Leader选举并实现数据同步。同步完成后又会进入消息广播模式，接收事务请求。

保证消息有序：

在整个消息广播中，Leader会将每一个事务请求转换成对应的 proposal 来进行广播，并且在广播 事务Proposal 之前，Leader服务器会首先为这个事务Proposal分配一个全局单递增的唯一ID，称之为事务ID（即zxid），由于Zab协议需要保证每一个消息的严格的顺序关系，因此必须将每一个proposal按照其zxid的先后顺序进行排序和处理。

**消息广播**

1）在zookeeper集群中，数据副本的传递策略就是采用消息广播模式。zookeeper中数据副本的同步方式与二段提交相似，但是却又不同。二段提交要求协调者必须等到所有的参与者全部反馈ACK确认消息后，再发送commit消息。要求所有的参与者要么全部成功，要么全部失败。二段提交会产生严重的阻塞问题。

2）Zab协议中 Leader 等待 Follower 的ACK反馈消息是指“只要半数以上的Follower成功反馈即可，不需要收到全部Follower反馈”

具体过程：

1. 客户端发起一个写操作请求；
2. Leader服务器将客户端的请求转化为事务Proposal 提案，同时为每个Proposal 分配一个全局的ID，即zxid；
3. Leader服务器为每个Follower服务器分配一个单独的队列，然后将需要广播的 Proposal依次放到队列中去，并且根据FIFO策略进行消息发送；
4. Follower接收到Proposal后，会首先将其以事务日志的方式写入本地磁盘中，写入成功后向Leader反馈一个Ack响应消息；
5. Leader接收到超过半数以上Follower的Ack响应消息后，即认为消息发送成功，可以发送commit消息；
6. Leader向所有Follower广播commit消息，同时自身也会完成事务提交。Follower 接收到commit消息后，会将上一条事务提交；

**崩溃恢复**

一旦 Leader 服务器出现崩溃或者由于网络原因导致 Leader 服务器失去了与过半 Follower 的联系，那么就会进入崩溃恢复模式。

在 Zab 协议中，为了保证程序的正确运行，就一种Leader选举算法需要能快速选举出新的 Leader 。并且Leader 选举算法不仅仅需要让 Leader 自己知道自己已经被选举为 Leader ，同时还需要让集群中的所有其他机器也能够快速感知到选举产生的新 Leader 服务器。

崩溃恢复主要包括两部分：**Leader选举** 和 **数据恢复**

假设两种异常情况：
1、一个事务在 Leader 上提交了，并且过半的 Folower 都响应 Ack 了，但是 Leader 在 Commit 消息发出之前挂了。
2、假设一个事务在 Leader 提出之后，Leader 挂了。

要确保如果发生上述两种情况，数据还能保持一致性，那么 Zab 协议选举算法必须满足以下要求：

Zab 协议崩溃恢复要求满足以下两个要求：
1）确保已经被 Leader 提交的 Proposal 必须最终被所有的 Follower 服务器提交。
2）确保丢弃已经被 Leader 提出的但是没有被提交的 Proposal。

根据上述要求
Zab协议需要保证选举出来的Leader需要满足以下条件：
1）新选举出来的 Leader 不能包含未提交的 Proposal 。
即新选举的 Leader 必须都是已经提交了 Proposal 的 Follower 服务器节点。
2）新选举的 Leader 节点中含有最大的 zxid 。
这样做的好处是可以避免 Leader 服务器检查 Proposal 的提交和丢弃工作。

**Zookeeper的选举机制**

（1）服务器1启动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上（3票），选举无法完成，服务器1状态保持为LOOKING；

（2）服务器2启动，再发起一次选举。服务器1和2分别投自己一票并交换选票信息：**此时服务器1发现服务器2的myid比自己目前投票推举的（服务器1） 大，更改选票为推举服务器2**。此时服务器1票数0票，服务器2票数2票，没有半数以上结果，选举无法完成，服务器1，2状态保持LOOKING；

（3）服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING；

（4）服务器4启动，发起一次选举。此时服务器1，2，3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为 1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为FOLLOWING；

（5）服务器5启动，同4一样当小弟

**Zab 如何数据同步**

1）完成 Leader 选举后（新的 Leader 具有最高的zxid），在正式开始工作之前（接收事务请求，然后提出新的 Proposal），Leader 服务器会首先确认事务日志中的所有的 Proposal 是否已经被集群中过半的服务器 Commit。

2）Leader 服务器需要确保所有的 Follower 服务器能够接收到每一条事务的 Proposal ，并且能将所有已经提交的事务 Proposal 应用到内存数据中。等到 Follower 将所有尚未同步的事务 Proposal 都从 Leader 服务器上同步过啦并且应用到内存数据中以后，Leader 才会把该 Follower 加入到真正可用的 Follower 列表中。

参考文章：[Zab协议详解](https://blog.csdn.net/liuchang19950703/article/details/111406622)







# 其他

## 场景相关题

#### 问：10亿用户2亿商品如何维护销量排行榜和点击量的数据

答：首先那么大的数据量，单机的存储能力、连接数都是支撑不了的，得考虑分库分表，将数据分散到多个数据库中，使得每个数据库中数据量小响应快。一般来说单表数量







## HR面

**自我介绍**

答：面试官好，我叫汪勇圣，目前湖南大学计算机学院研二在读，做过的项目主要有两个，第一个是一个基于深度学习的产前超声诊断项目，主要是参与了前期的数据传输接口开发和胎儿骨骼切面检测模型的训练，目前这个项目已经上线，在300多家医院使用。第二个项目是实现了一个基于Netty+Zookeeper的RPC框架，实现了服务注册订阅、负载均衡以及自定义的通信协议等功能，主要目的通过造轮子的方式来学习RPC的工作原理。



#### **问：说一下Mini-RPC-Framework项目主要功能**

答：这个项目是一款基于Zookeeper 实现的 RPC 框架，它实现了基于Socket和Netty两个版本，

- 实现了基于 Java 原生 Socket 传输与 Netty 传输两种网络传输方式
- 实现了四种序列化算法，Json 方式、Kryo 算法、Hessian 算法与 Google Protobuf 方式（默认采用 Kryo方式序列化）
- 实现了两种负载均衡算法：随机算法与轮转算法
- 使用 Nacos 作为注册中心，管理服务提供者信息
- 消费端如采用 Netty 方式，会复用 Channel 避免多次连接
- 如消费端和提供者都采用 Netty 方式，会采用 Netty 的心跳机制，保证连接
- 接口抽象良好，模块耦合度低，网络传输、序列化器、负载均衡算法可配置
- 实现自定义的通信协议
- 服务提供侧自动注册服务

这个项目使用 Spring 提供依赖注入与参数配置，使用 Netty 实现 NIO 方式的数据传输，使用 ZooKeeper 实现服务注册与发现。





#### **问：Mini-RPC-Framework项目中的比较有挑战的点，项目的难点**





#### **问：介绍一下产前超声AI智慧云平台这个项目**





zookeeper的设计模式

zookeeper的共识算法

主要技术栈

写了观测者模式

volatile

JMM 

内存屏障

JMM缓存一致性协议

JAVA如何实现原子性操作

内存逃逸

JAVA unsafe

泛型

如何主动进行垃圾回收

为什么要这样设计，有什么好处

JMETER

研究YOLOv5网络结构，

字节码插桩

Seletor.select





# ——核心部分——

## 项目相关

### 一、注册中心

策略：服务注册原理、注册中心结构、zookeeper的原理、几个注册中心的区别、分布式算法、分布式事务。

项目细节：服务注册、服务发现、服务注销、监听机制

#### **分布式算法**

因为分布式的数据分区特性，如果没有Leader来协调数据处理，就很难保证数据是一致的了。 如果没有Leader，则系统的每次操作都要进行一次投票，开销非常大。

一、2PC

**角色**：协调者和参与者

（1）提交请求过程：

协调者发送prepare和事务内容给参与者，参与者执行并记录日志，但是不提交，然后向协调者发送Yes或No的执行结果。

（2）提交阶段

若是所有参与者返回Yes，说明事务可以提交。接着向参与者发送commit请求，参与者收到commit请求则会提交事务并返回ACK。协调者收到所有参与者的ACK，事务成功完成。

如果有参与者返回No，或者超时未返回，说明事务中断，需要发送rollback回滚。

**存在问题**：单点故障问题、阻塞问题、数据不一致问题。



二、3PC

三阶段提交在协调者和参与者中引入超时机制，并且把两阶段提交的第一阶段拆分为两步：询问，然后锁住资源，最后真正提交。

（1）CanCommit阶段：发送CanCommit请求，询问各个参与者资源是否可以正常执行。全部Yes继续，否则中断事务。

（2）PreCommit阶段：发送PreCommit请求，参与者收到后会执行并且记录日志，但是不提交，然后返回执行结果。

（3）DoCommit阶段：和2PC的提交阶段一样。



三、ZAB

Zab协议（原子广播协议）借鉴Paxos算法，是Zookeeper用来保证分布式事务一致性的协议。

**角色**：

- Leader ：集群中 唯一的写请求处理者 ，能够发起投票（投票也是为了进行写请求）。
- Follower：能够接收客户端的请求，如果是读请求则可以自己处理，如果是写请求则要转发给 Leader 。在选举过程中会参与投票，有选举权和被选举权 。
- Observer ：就是没有选举权和被选举权的 Follower 。

Zab协议包括两种基本的模式：**崩溃恢复** 和 **消息广播**

**消息广播**

集群的数据副本通过广播传递，Leader只需要等待半数以上的Follower成功返回ACK即可认为发送成功，不需要收到全部的Follower反馈。

**奔溃恢复**

奔溃恢复包括Leader选举和数据恢复两部分

**数据恢复**：

为了让新Leader能正常工作：需要满足两个要求：一个是已经提交的信息不能丢失，二是被丢弃的信息不要再出现。因此，新Leader不能包含没提交的Proposal，并且含有最大的ZXID。

**Leader选举**：

- 3个参考指标：Epoch：每个Leader的任期编号；ZXID：事务ID；SID：服务器ID。

- 选举规则：Epoch>ZXID>SID

- 节点状态：Leading、Following、Looking

在启动期间的选举：

1. 首先，每个节点给自己投一票，然后把投票信息广播出去；
2. 节点收到投票信息，会和自己的投票对比，先比ZXID再比SID，此时大家都是Looking状态；
3. 投票完，统计票数，如果超过集群半数选择了某个节点，那么该节点成为Leader，选举结束；
4. 最后，节点更新状态。Leader为Leading，follower改为Following。

在运行期间的选举：

1. Leader宕机了，非Observer节点就会把自己状态改为Looking，进入选举流程；
2. 节点生成投票信息，和第一轮一样广播出去；
3. 同样会优先ZXID，然后选择SID，最后统计票数，修改节点状态，选举结束。

**数据同步**：

1. Leader 服务器会首先确认事务日志中的所有的 Proposal 是否已经被集群中过半的服务器 Commit。
2. 让Follower采用全量同步或者先回滚再差异化同步。



四、Raft

Raft算法是通过一切以领导者为准的方式，实现一系列值得共识和各节点日志的一致。

**角色**：

- Leader（领导者）：处理写请求、管理日志复制和不断地发送心跳信息；
- Candidate（候选者）：向其他节点发起投票RPC信息，如果得到大多数投票就晋升为领导者
- Follower（追随者）：接收心跳信息，超时就推荐自己成为候选者

**选举领导过程**：

（1）初始状态：初始状态下，集群所有的节点都是跟随者的状态。并且Term （任期编号）为0。

（2）成为候选者：Raft算法实现了随机超时时间的特性，每个节点等待领导者节点心跳信息的超时时间间隔是随机的。当一个节点的超时时间到了后，就会成为候选者，给自己任期加1，并给自己投一票。然后，给其他节点投票消息。

（3）选举规则：一个节点收到一个投票信息，如果发现任期编号比自己的小则直接拒绝消息，如果发现比自己大，则会更新自己的任期编号，并恢复成追随者，投出自己的一票。此外，每个节点对于同一任期编号，只能投一票。

（4）当一个节点得到大多数选票，也就是至少 N/2 + 1 张票，那么就成为领导者。

（5）在任期内，领导者需要不断给其他节点发送心跳信息来通知状态

> 总结：
>
> （1）以上共识算法都有一个共同点，那就是都有一个领导者，主要原因在于因为分布式的数据分区特性，没有一个领导者来协调数据的话，就得靠投票，开销非常大。

参考链接：[Raft](https://juejin.cn/post/6921960955001700359) 



#### Q：一个注册中心,至少需要具备哪些条件

服务注册接口：服务提供者通过调用服务注册接口来完成服务注册。

服务反注册接口：服务提供者通过调用服务反注册接口来完成服务注销。

心跳汇报接口：服务提供者通过调用心跳汇报接口完成节点存活状态上报。

服务订阅接口：服务消费者通过调用服务订阅接口完成服务订阅，获取可用的服务提供者节点列表。

服务变更查询接口：服务消费者通过调用服务变更查询接口，获取最新的可用服务节点列表。

服务查询接口：查询注册中心当前注册了哪些服务信息。

服务修改接口：修改注册中心中某一服务的信息。



#### Q：常用的服务注册中心, 注册中心的差异

![preview](C:\Users\Lenovo\Desktop\笔记\CS-Learning-Notes\notes\pics\v2-d1d45601c6a33782f7b5b9bf2f458dec_r.jpg)

常用的注册中心有Zookeeper、Eureka、Consul、Nacos

注册中心选型：

从一致性协议上来看，注册中心基本上可以分为两种：

- 基于Paxos的一致性系统。特点是有主、单点写、数据一致性能力强。例如基于Raft协议的Etcd、基于Zab协议Zookeeper。
- 对等部署的、无主的、多写一致性系统，例如Nacos和Eureka。

参考文章：[微服务架构：注册中心ZooKeeper、Eureka、Consul 、Nacos 对比！](https://zhuanlan.zhihu.com/p/298005666)

https://juejin.cn/post/6965881883083145229

https://juejin.cn/post/6924889015824941064

https://juejin.cn/post/6844903889024581639



#### Q：注册中心和DNS的区别

注册中心用于实现区域内的服务发现和负载均衡，注册中心类似于一个内部的DNS，不同区域的注册中心是相互隔离的。

DNS也可以用作服务发现和负载均衡，它的优势在于**跨语言**，不过传统DNS负载均衡的缺点是流量可能路由到不健康的或者是已经下线的实例上去，因为DNS需要缓存。
链接：https://juejin.cn/post/6965881883083145229



#### Q ：zookeeper服务容灾？zookeeper服务节点挂掉之后，怎么删除它？

容灾：在集群若干台故障后，整个集群仍然可以对外提供可用的服务。

 一般配置奇数台去构成集群，以避免资源的浪费。

 三机房部署是最常见的、容灾性最好的部署方案。

删除：使用临时节点，会话失效，节点自动清除。



#### Q：ZooKeeper的作用？

注册中心。扩展：

1. 命名服务Name Service，依赖Zookeeper可以生成全局唯一的节点ID，来对分布式系统中的资源进行管理。
2. 分布式协调，这是Zookeeper的核心使用了。利用Wather的监听机制，一个系统的某个节点状态发生改变，另外系统可以得到通知。
3. 集群管理，分布式集群中状态的监控和管理，使用Zookeeper来存储。
4. Master选举，利用Zookeeper节点的全局唯一性，同时只有一个客户端能够创建成功的特点，可以作为Master选举使用，创建成功的则作为Master。
5. 分布式锁，利用Zookeeper创建临时顺序节点的特性。



#### Q：Zookeeper 如何保证一致性的？

Zookeeper通过ZAB原子广播协议来实现数据的最终顺序一致性，他是一个类似2PC两阶段提交的过程。

由于Zookeeper只有Leader节点可以写入数据，如果是其他节点收到写入数据的请求，则会将之转发给Leader节点。

主要流程如下：

1. Leader收到请求之后，将它转换为一个proposal提议，并且为每个提议分配一个全局唯一递增的事务ID：zxid，然后把提议放入到一个FIFO的队列中，按照FIFO的策略发送给所有的Follower
2. Follower收到提议之后，以事务日志的形式写入到本地磁盘中，写入成功后返回ACK给Leader
3. Leader在收到超过半数的Follower的ACK之后，即可认为数据写入成功，就会发送commit命令给Follower告诉他们可以提交proposal了

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/ibBMVuDfkZUmqb0t9xOJOXebntahoHMGMNOMJz5ibicXbq9DbBvotd05SEtuJJMggvxe7NktQyXTFSumDjDyJzHYA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

ZAB包含两种基本模式，崩溃恢复和消息广播。

整个集群服务在启动、网络中断或者重启等异常情况的时候，首先会进入到崩溃恢复状态，此时会通过选举产生Leader节点，当集群过半的节点都和Leader状态同步之后，ZAB就会退出恢复模式。之后，就会进入消息广播的模式。



#### Q：UUID通用的几种方案

- **UUID**：如果只是考虑**唯一性**，那么UUID基本可以满足需求；

- **数据库自增主键**

- **基于Redis生成全局ID策略**

- **snowflake(雪花算法)**







**1、介绍一下服务注册中心怎么做的？**

（1）服务注册和发现

- 服务注册/反注册：保存服务提供者和服务调用者的信息
- 服务订阅/取消订阅：服务调用者订阅服务提供者的信息，最好有实时推送的功能
- 服务路由（可选）：具有筛选整合服务提供者的能力

（2）服务配置（不包括其他无关配置）

- 配置订阅：服务提供者和服务调用者订阅微服务相关的配置
- 配置下发（可选）：主动将配置推送给服务提供者和服务调用者

（3）服务健康检测

- 检测服务提供者的健康情况



**2、一个注册中心,至少需要具备哪些条件?（项目中RPC服务注册中心需要注意什么?）（如果让你设计一个服务注册中心，怎么设计？）**

服务注册接口：服务提供者通过调用服务注册接口来完成服务注册。

服务反注册接口：服务提供者通过调用服务反注册接口来完成服务注销。

心跳汇报接口：服务提供者通过调用心跳汇报接口完成节点存活状态上报。

服务订阅接口：服务消费者通过调用服务订阅接口完成服务订阅，获取可用的服务提供者节点列表。

服务变更查询接口：服务消费者通过调用服务变更查询接口，获取最新的可用服务节点列表。

服务查询接口：查询注册中心当前注册了哪些服务信息。

服务修改接口：修改注册中心中某一服务的信息。



3、注册中心单机还是分布式的，其中一个挂了怎么办？一致性，可靠性怎么保证的？超时控制，加锁和管道支持并发，单机（考虑了多机情况）







4、常用的服务注册中心, 注册中心的差异



5、为什么用Zookeeper做注册中心？(优点，与其他选型对比下)

（使用Zookeeper有什么好处？）

（说一下Zookeeper，为什么使用Zookeeper，不选其他注册中心？）

（了解Nacos和Zookeeper的区别吗？）

（为什么不选择Redis作为注册中心？（Zookeeper临时节点自动宕机自动清除））

（为什么要用Zookeeper（服务注册、发现））

（Zookeeper和Eureka分别是满足CAP中的哪些）





**6、集群一般有几个节点，为什么？**

答：5个，宕机后选举要大于一半成为leader。

> PS：Zookeeper中 Leader 选举算法采用了Zab协议。Zab核心思想是当多数 Server 写成功，则任务数据写成功。



7、socket过程中发生的系统调用



8、zookeeper服务发现



**9、zookeeper服务容灾？zookeeper服务节点挂掉之后，怎么删除它？**

容灾：在集群若干台故障后，整个集群仍然可以对外提供可用的服务。

 一般配置奇数台去构成集群，以避免资源的浪费。

 三机房部署是最常见的、容灾性最好的部署方案。

删除：使用临时节点，会话失效，节点自动清除。



10、Zookeeper有几种角色？

领导者（leader），追随者（follower），观察者（observer）



11、CAP理论解释下？P是什么？

- 一致性（**C**onsistency）多个副本之间的数据一致性
- 可用性（**A**vailability）在合理规定的时间内，是否能返回一个明确的结果。
- 分区容错性（**P**artition tolerance）在分区故障下，仍然可以对外提供正常的服务。

一个分布式系统在以上三个特性中：最多满足其中的两个特性。



12、Zookeeper集群节点宕机了怎么发现剔除的？

发现：watcher机制

剔除：临时节点



13、服务熔断和服务降级有什么区别？

**服务熔断：**如果某个目标服务调用慢或者有大量超时，此时，熔断该服务的调用，对于后续调用请求，不在继续调用目标服务，直接返回，快速释放资源。如果目标服务情况好转则恢复调用。

**服务降级：**当服务器压力剧增的情况下，根据当前业务情况及流量对一些服务和页面有策略的降级，以此释放服务器资源以保证核心任务的正常运行。



14、zookeeper原理？羊群效应，怎么解决，解决之后又有什么问题，又怎么解决，纯粹搞成了循环依赖了。zab协议，具体说来。



15、ZAB算法讲一下（讲了ZAB是paxos的改版，Mysql是paxos、redis sentinel是raft、zookeeper是ZAB、ZAB的具体实现）



16、zk的分布式算法zab，如果选举的时候zxid都相同呢？

比较SID



17、dubbo 怎么注册到zookeeper以及 dubbo协议，zookeeper。



18、zookeeper的节点类型?（持久，临时，顺序）

**PERSISTENT-持久节点**：除非手动删除，否则节点一直存在于Zookeeper上

**EPHEMERAL-临时节点**：临时节点的生命周期与客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。

**PERSISTENT_SEQUENTIAL-持久顺序节点**：基本特性同持久节点，只是增加了顺序属性，节点名后边会追加一个由父节点维护的自增整型数字。

**EPHEMERAL_SEQUENTIAL-临时顺序节点**：基本特性同临时节点，增加了顺序属性，节点名后边会追加一个由父节点维护的自增整型数字。







19、分布式数据一致性协议都知道哪些

（2PC 3PC Paxos）

Raft





20、Raft了不了解



21、分布式事务的几种解决方案（2PC，3PC，TCC，基于消息，然后顺带讲了一下优缺点） 分布式事务的几种方式吧（2pc、3pc、tcc、基于消息）以及区别

答：







22、Zookeeper 如何保证一致性的？

Zookeeper通过ZAB原子广播协议来实现数据的最终顺序一致性，他是一个类似2PC两阶段提交的过程。

由于Zookeeper只有Leader节点可以写入数据，如果是其他节点收到写入数据的请求，则会将之转发给Leader节点。

主要流程如下：

1. Leader收到请求之后，将它转换为一个proposal提议，并且为每个提议分配一个全局唯一递增的事务ID：zxid，然后把提议放入到一个FIFO的队列中，按照FIFO的策略发送给所有的Follower
2. Follower收到提议之后，以事务日志的形式写入到本地磁盘中，写入成功后返回ACK给Leader
3. Leader在收到超过半数的Follower的ACK之后，即可认为数据写入成功，就会发送commit命令给Follower告诉他们可以提交proposal了

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/ibBMVuDfkZUmqb0t9xOJOXebntahoHMGMNOMJz5ibicXbq9DbBvotd05SEtuJJMggvxe7NktQyXTFSumDjDyJzHYA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

ZAB包含两种基本模式，崩溃恢复和消息广播。

整个集群服务在启动、网络中断或者重启等异常情况的时候，首先会进入到崩溃恢复状态，此时会通过选举产生Leader节点，当集群过半的节点都和Leader状态同步之后，ZAB就会退出恢复模式。之后，就会进入消息广播的模式。





23、你知道Zookeeper的分布式锁实现方式吗？

（临时节点，如果服务器挂了，锁会自己消失）





24、ZooKeeper的作用？

注册中心。扩展：

1. 命名服务Name Service，依赖Zookeeper可以生成全局唯一的节点ID，来对分布式系统中的资源进行管理。
2. 分布式协调，这是Zookeeper的核心使用了。利用Wather的监听机制，一个系统的某个节点状态发生改变，另外系统可以得到通知。
3. 集群管理，分布式集群中状态的监控和管理，使用Zookeeper来存储。
4. Master选举，利用Zookeeper节点的全局唯一性，同时只有一个客户端能够创建成功的特点，可以作为Master选举使用，创建成功的则作为Master。
5. 分布式锁，利用Zookeeper创建临时顺序节点的特性。



25、zookeeper有什么特性，讲一下（临时节点、持久节点、ZAB）



26、服务下线还有没有别的实现方法

使用临时节点



27、zookeeper宕机与dubbo直连的情况？



28、任何一个请求(流量)过来都会打到注册中心么?

不会，第一次会，有本地缓存



29、有一大批流量总是被打到一个实例上面,这个实例的兄弟实例分到的流量很少,怎么办?

通过合理负载均衡



30、有一个实例挂了怎么办?

zookeeper心跳检测更新列表并利用watcher机制发给服务消费者



31、注册中心怎么进行心跳检测



32、注册中心对于服务端掉线时怎么处理

移出ip链表，发送给服务消费者，等待服务器上线，重新连接



33、服务端用的哪个类监听的（ServerSocket）



34、自己实现的定时器是啥？



35、RPC心跳怎么实现的？

是服务端给服务注册中心心跳还是服务端给客户端心跳？

服务调用方怎么知道服务不可用了？

(zookeeper的心跳检测+更新ip列表+watcher发送给服务调用方)：注册中心发送

(利用netty的IdleStateHandler实现心跳服务)：客户端给服务端发送PING消息



36、怎么实现的类似本地调用？

本地知道类名+服务名，直接调用



37、如果是你如何设计一个nacos ，rpc如何调用。



38、如果注册中心服务器宕机怎么保证高可用？

高可用：通过设置减少系统不能提供服务的时间。

在zookeeper主要考虑***容灾和扩容\***两方面提高高可用。



39、服务的地址怎么知道？

(注册中心)



40、服务注册信息的拆分要怎么做?



41、服务注册中心的功能除了放在额外的服务器上实现还能放在哪里？怎么实现?



42、RPC服务注册、服务发现、服务注销怎么做的？

服务注册怎么进行服务注销监听?

RPC项目zookeeper怎么实现注册、发现的？（临时节点存储ip+端口+负载均衡策略）



43、了解过zookeeper的问题吗?

（崩溃恢复无法提供服务、写的性能瓶颈是一个问题、选举过程速度缓慢、无法进行有效的权限控制）



### 二、序列化与反序列化以及协议

#### 序列化知识

序列化就是把Java对象转换成字节序列，反序列化则是把字节序列恢复为原先的Java对象。

关键字 Serialzable 是一个标识表示实现了这个接口的类都可以被序列化。

serialVersionUID （序列化版本号）是序列化前后的唯一标识符，在反序列化过程中JVM 会对比被序列化类的序列化版本号和字节流中的序列化版本号，只有一致才能正常序列化。所以希望类的不同版本对序列化兼容，因此需要确保类的不同版本具有相同的serialVersionUID；

特殊情况：凡是被 static 和 transient 修饰的字段都不会被序列化，前者是因为序列化保存的对象的状态，后者是不想隐私信息被序列化。

序列化算法比较：

（1）JDK原生序列化：只要类实现了Serialization 接口就行，序列化具体的实现是由ObjectOutputStream和ObjectInputStream来实现的，缺点是序列化二进制流大，序列化性能差，也无法跨语言。

（3）JSON序列化：很常见的文本型序列化框架，以键值对的方式存储数据。优点是具备可读性 ，不需要对要序列化的类做特殊处理。缺点是空间开销大，并且由于没有类型，需要反射来解决。

（3）Hessian：缺点没法LinkedHashMap、LinkedHashSet等类型不支持。在反序列化时间较久

（4）ProtoBuf 是一种轻便高效的结构化数据存储格式。它使用T-L-V（标识-长度-字段值）的数据格式来存储数据。

- T：代表字段的正数序列 (tag)，在序列化的时候用整数值来代替字段名称，于是传输流量就可以大幅缩减；
- L：代表 Value 的字节长度，一般也只占一个字节；
- V：则代表字段值经过编码后的值。

Protobuf 定义了一套自己的编码方式，几乎可以映射 Java/Python 等语言的所有基础数据类型。不同的编码方式对应不同的数据类型，还能采用不同的存储格式。这样就可以大大地减少编码后的字节数。

在网络资源紧张、性能要求非常高的场景，ProtoBuf协议是不错的选择。缺点就是可读性差，对调试不友好。



1、序列化和反序列化有什么作用

（1）**实现了数据的持久化**：永久性保存对象，保存对象的字节序列到本地文件或者数据库中；
（2）**序列化实现远程通**：通过序列化以字节流的形式使对象在网络中进行传递和接收；
（3）通过序列化在进程间传递对象；



2、Serializable和Externalizable懂吗？（不知道Externalizable）

1、Serializable序列化时不会调用默认的构造器，而Externalizable序列化时会调用默认构造器的！

2、Serializable：一个对象想要被序列化，它的类就要实现 此接口，这个对象的所有属性都可以被序列化和反序列化来保存、传递。 

 Externalizable：自定义序列化可以控制序列化的过程和决定哪些属性不被序列化。

3、使用Externalizable时，必须按照写入时的确切顺序读取所有字段状态。否则会产生异常。



3、serializable关键字的作用（实现原理）？几种序列化协议？ProtoBuff的优点？



4、序列化传输？



5、有没有阅读过序列化（Java Serialization、Fastjson）之后的数据



6、RPC 不同序列化协议了解吗？优缺点是？各种序列号协议的特点？序列化方式有哪几个，区别是什么，自己写过吗？



7、为什么选用ProtoBuff？



8、为什么选KRYO序列化？

java 的压缩算法



9、序列化怎么做的（序列化怎么实现）？Kryo原理了解吗？



10、你说到你自定义了一个简单协议，自定义的协议头里包括哪些内容，多少字节，各自的作用是什么

魔数，消息长度，请求id，消息类型



11、由RPC项目问到了序列化反序列化，问到了对象有一个属性是对象引用，怎么序列化



12、如何实现编解码及序列化？



13、那你这个序列化还是针对Java语言的，如何实现跨语言的序列化或者RPC框架？

Java

RPC框架要想跨语言，本质是在解决**序列化/反序列化**的跨语言问题



### 三、Netty

阻塞与非阻塞指的的是当不能进行读写（网卡满时的写/网卡空的时候的读）的时候，I/O 操作立即返回还是阻塞；

同步异步指的是，当数据已经ready的时候，读写操作是同步读写还是异步读写，也就是方法的执行方是主动方还是其他线程，主线程的话需要方法执行完成，其他线程的话无需等待立即返回方法调用，主线程可以直接执行接下来的代码。

[回调](https://blog.csdn.net/zjpp2580369/article/details/83027547)：

- 类A的a()方法调用了类B的b()方法
- 类B的b方法执行完毕主动调用类A的callback()方法

> 同步阻塞模式：这种模式下，我们的工作模式是先来到厨房，开始烧水，并坐在水壶面前一直等着水烧开。
>
> 同步非阻塞模式：这种模式下，我们的工作模式是先来到厨房，开始烧水，但是我们不一直坐在水壶前面等，而是回到客厅看电视，然后每隔几分钟到厨房看一下水有没有烧开。
>
> 异步非阻塞I/O模型：这种模式下，我们的工作模式是先来到厨房，开始烧水，我们不一一直坐在水壶前面等，也不隔一段时间去看一下，而是在客厅看电视，水壶上面有个开关，水烧开之后他会通知我。
>
> 阻塞VS非阻塞：人是否坐在水壶前面一直等。
>
> 同步VS异步：水壶是不是在水烧开之后主动通知人。



Reactor模式：注册所有感兴趣的事件处理器，单线程轮询选择就绪事件，执行事件处理器。



#### Q：简述AIO、BIO、NIO的具体使用、区别及原理

（1）BIO

BIO属于同步阻塞IO模型，**传统的  java.io 包**，它基于流模型实现。应用程序发起 read 调用后，会一直阻塞，直到在内核把数据拷贝到用户空间，属于是一线程一连接的模型。这种模型常常配合线程池一起使用，因为当一个连接在处理I/O的时候，系统是阻塞的，如果是单线程的话必然就挂死在那里，使用多线程可以更充分利用CPU资源。

模型缺点：严重依赖于线程，但线程是很宝贵的资源，无论是创建销毁还是切换的成本都很高。

适用场景：只适用于在客户端连接数量不高的情况下。

（2）NIO

NIO也就是同步非阻塞 IO 模型。**对应 java.nio 包**，提供了 Channel , Selector，Buffer 等抽象。它支持面向缓冲的，基于通道的 I/O 操作方法。数据总是从通道读取到缓冲区中，或者从缓冲区写入到通道中。Selector(选择区)用于监听多个通道的事件（比如：连接打开，数据到达）。因此，单个线程可以监听多个数据通道。

使用场景：适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器

（3）AIO

AIO也就是异步 IO 是基于事件和回调机制实现的，也就是应用进程调用操作之后会直接返回，继续往下执行，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。

使用场景：用于连接数目多且连接比较长（重操作）的架构，比如相册服务器



#### Q：介绍Reactor和Proactor

在I/O 复用机制需要事件分发器（event dispatcher）。 事件分发器的作用，即将那些读写事件源分发给各读写事件的处理者。事件分发器有两种模式分别是Reactor和Proactor。

Reactor模式是基于同步I/O的，在Reactor模式中，事件分发器监听某个事件，收到事件后，根据事件类型分配给事先注册的线程，由由后者来做实际的操作。

Reactor 模式主要由 Reactor 和处理资源池这两个核心部分组成，它俩负责的事情如下：

- Reactor 负责监听和分发事件，事件类型包含连接事件、读写事件；
- 处理资源池负责处理事件，如 read -> 业务逻辑 -> send；

Proactor模式是和异步I/O相关的，事件处理者（或者代由事件分发器发起）直接发起一个异步读写操作，而实际工作是由操作系统来完成。发起时需要提供参数、回调函数等信息。事件分发器得知了这个请求，等这个请求完成了就会转发给事件处理者或者回调。

参考文章：[NIO](https://zhuanlan.zhihu.com/p/23488863) 、[Reactor和Proactor](https://juejin.cn/post/6955469938404360205#heading-1)



#### Q：Netty 执行流程

![图片](C:\Users\Lenovo\Desktop\笔记\CS-Learning-Notes\notes\pics\netty_zhix.png)



#### Q：NIO的核心组件

- 缓冲区（`Buffer`）

==缓冲区本质上是一个可以读写数据的内存块，可以理解成是一个容器对象（含数组），该对象提供了一组方法，可以更轻松地使用内存块，缓冲区对象内置了一些机制，能够跟踪和记录缓冲区的状态变化情况。Channel 提供从文件、网络读取数据的渠道，但是读取或写入的数据都必须经由 Buffer。==

- 通道（`Channel`）

类似于流，不同之处在于通道可以同时进行读写，可以实现异步读写。

- 选择器（`Selector`）

1. ==能够检测已经注册的通道上是否有事件发生（注意：多个 Channel 以事件的方式可以注册到同一个 Selector），如果有事件发生，便获取事件然后针对每个事件进行相应的处理。这样就可以只用一个单线程去管理多个通道，也就是管理多个连接和请求。==
2. 只有在连接/通道真正有读写事件发生时，才会进行读写，就大大地减少了系统开销，并且不必为每个连接都创建一个线程，不用去维护多个线程。
3. 避免了多线程之间的上下文切换导致的开销。



#### Q：说说你对Netty的认识？

1. Netty 是一个 **基于 NIO** 的 client-server(客户端服务器)框架，使用它可以快速简单地开发网络应用程序。
2. 它极大地简化并优化了 TCP 和 UDP 套接字服务器等网络编程,并且性能以及安全性等很多方面甚至都要更好。
3. **支持多种协议** 如 FTP，SMTP，HTTP 以及各种二进制和基于文本的传统协议。



#### Q：为什么选用Netty来做通信框架？还知道其他网络通信框架？

基于JDK原生NIO来进行网络开发编程复杂，对于新手来说很容易出现bug。并且对于NIO编程来说，一个比较合适的线程模型能充分发挥的它优势，而JDK没有实现，就连个自定义协议拆包都要自己实现。

因为相比于直接使用 JDK 自带的 NIO 相关的 API 来说更加易用。

- 统一的 API，支持多种传输类型，阻塞和非阻塞的。
- 简单而强大的线程模型。
- 自带编解码器解决 TCP 粘包/拆包问题。
- 自带各种协议栈。
- 社区活跃，应用广泛

t-io是什么？一个与netty同类的网络编程框架

Grizzly



#### Q：Netty的核心组件

**Channel**

Channel接口是 Netty 对网络操作的抽象类，对 Java 原生NIO Channel 的进一步封装，它除了包括基本的 I/O 操作，如 `bind()`、`connect()`、`read()`、`write()` 等，还可以配置参数及获取当前连接状态等功能。具体实现有`NioServerSocketChannel`（服务端）和`NioSocketChannel`（客户端），这两个 `Channel` 可以和 BIO 编程模型中的`ServerSocket`以及`Socket`两个概念对应上。

**EventLoop 与 EventLoopGroup**

EventLoop 的Netty的核心组件，用来负责监听网络事件并调用事件处理器进行相关 I/O 操作的处理。EventLoopGroup 是一个 EventLoop 池，包含很多的 EventLoop。

在netty内部，将会为每个Channel分配一个EventLoop，用于这个Channel所有的IO事件。

一个 Channel 一旦与一个 EventLoop 相绑定，那么在整个生命周期内是不能改变的。一个 EventLoop 可以与多个 Channel 绑定。即 Channel 与 EventLoop 的关系是 n:1，而 EventLoop 是单线程的，与线程的关系是 1:1。



**ServerBootstrap 与 Bootstrap**

Bootstarp 和 ServerBootstrap 是启动引导类，可以对应用程序进行配置，并使它们运行起来。Netty处理引导的方式是使你的应用程序和网络层相隔离。

Bootstrap 是客户端的引导类，Bootstrap 在调用 bind()（连接UDP）和 connect()（连接TCP）方法时，会新创建一个 Channel，实现所有的网络通信。

ServerBootstrap 是服务端的引导类，ServerBootstarp 在调用 bind() 方法时会创建一个 ServerChannel 来接受来自客户端的连接，并且该 ServerChannel 管理了多个子 Channel 用于同客户端之间的通信。

Bootstrap 只需要配置一个线程组—EventLoopGroup ,而 ServerBootstrap需要配置两个线程组— EventLoopGroup ，一个用于接收连接，一个用于具体的处理。

**ChannelHandler 与 ChannelPipeline**

ChannelHandler 是对 Channel 消息的具体处理器，用于处理或拦截 IO 事件，这些处理器可以是系统本身定义好的编解码器，也可以是用户自定义的。这些处理器会被统一添加到一个 ChannelPipeline 的对象中，然后按照添加的顺序对 Channel 中的数据进行依次处理。

**ChannelFuture**

Netty 中所有的 I/O 操作都是异步的，即操作不会立即得到返回结果，所以 Netty 中定义了一个 ChannelFuture 对象作为这个异步操作本身。如果想获取到该异步操作的返回值，可以通过该异步操作对象的addListener() 方法为该异步操作添加监听器，当操作执行成功或失败时，会触发回调。

Netty 的异步编程模型都是建立在 Future 与回调概念之上的。



> NioEventLoop是一个单线程的 Executor，
>
> NioEventLoopGroup 是一个线程池线程 Executor，线程数默认当前CPU逻辑核心数的两倍
>
> NioEventLoop 的创建的实在 NioEventLoopGroup 初始化的时候。
>





#### Q：Netty怎么实现高性能的？

Netty高性能主要依赖了哪些特性？Netty为什么快（基于NIO+零拷贝）Netty为啥效率高（零拷贝，线程模型）



#### Q：Netty模型

Netty线程模型是基于 Reactor 模式设计开发的。

在 Netty 主要靠 `NioEventLoopGroup` 线程池来实现具体的线程模型的 。

我们实现服务端的时候，一般会初始化两个线程组：

1. **`bossGroup`** :接收连接。
2. **`workerGroup`** ：负责具体的处理，交由对应的 Handler 处理。

**单线程模型** ：一个线程需要执行处理所有的 `accept`、`read`、`decode`、`process`、`encode`、`send` 事件。对于高负载、高并发，并且对性能要求比较高的场景不适用。

**多线程模型**：一个 Acceptor 线程只负责监听客户端的连接，一个 NIO 线程池负责具体处理：`accept`、`read`、`decode`、`process`、`encode`、`send` 事件。满足绝大部分应用场景，并发连接量不大的时候没啥问题，但是遇到并发连接大的时候就可能会出现问题，成为性能瓶颈。

**主从多线程模型**：从一个 主线程 NIO 线程池中选择一个线程作为 Acceptor 线程，绑定监听端口，接收客户端连接的连接，其他线程负责后续的接入认证等工作。连接建立完成后，Sub NIO 线程池负责具体处理 I/O 读写。如果多线程模型无法满足你的需求的时候，可以考虑使用主从多线程模型 。

> Reactor 模式基于事件驱动，采用多路复用将事件分发给相应的 Handler 处理，非常适合处理海量 IO 的场景。
>
> 使用 `NioEventLoopGroup` 类的无参构造函数设置线程数量的默认值就是 **CPU 核心数 \*2** 。



#### Q：Netty怎么解决epoll空轮询

Selector BUG出现的原因：若Selector的轮询结果为空，也没有wakeup或新消息处理，则发生空轮询，CPU使用率100%。

Netty的解决办法：

- 对Selector的select操作周期进行统计，每完成一次空的select操作进行一次计数，
- 若在某个周期内连续发生N次空轮询，则触发了epoll死循环bug。
- 将问题Selector上注册的Channel转移到新建的Selector上；并将原来的Selector关闭。



#### Q：Netty怎么实现idleState

之所以需要心跳机制：在 TCP 保持长连接的过程中，可能会出现断网等网络异常出现，异常发生的时候， client 与 server 之间如果没有交互的话，它们是无法发现对方已经掉线的。为了解决这个问题, 我们就需要引入 心跳机制 。

心跳就是在 TCP 长连接中,客户端和服务器之间定期发送的一种特殊的数据包，通知对方自己还在线, 以确保 TCP 连接的有效性。TCP 实际上自带的就有长连接选项，本身是也有心跳包机制，也就是 TCP 的选项：`SO_KEEPALIVE`。但是，TCP 协议层面的长连接灵活性不够。所以，一般情况下我们都是在应用层协议上实现自定义心跳机制的，也就是在 Netty 层面通过编码实现。在 Netty 中, 实现心跳机制的关键是IdleStateHandler。

Netty 的超时类型 `IdleState` 主要分为以下3类：

- ALL_IDLE : 一段时间内没有数据接收或者发送。
- READER_IDLE ： 一段时间内没有数据接收。
- WRITER_IDLE ： 一段时间内没有数据发送。

针对上面的 3 类超时异常，Netty 提供了 3 类`ChannelHandler`来进行监测。

- IdleStateHandler ： 当 Channel 一段时间未执行读取、写入或者两者都未执行时，触发 IdleStateEvent 事件。
- ReadTimeoutHandler ：在一定时间内未读取任何数据时，引发 ReadTimeoutEvent 事件。
- WriteTimeoutHandler ：当写操作在一定时间内无法完成时，引发 WriteTimeoutEvent 事件。

```java
public IdleStateHandler(int readerIdleTimeSeconds, int writerIdleTimeSeconds, int allIdleTimeSeconds) {

    this((long)readerIdleTimeSeconds, (long)writerIdleTimeSeconds, (long)allIdleTimeSeconds, TimeUnit.SECONDS);

}

```

构造函数可以接收以下参数：

- readerIdleTimeSecond：指定读超时时间，指定 0 表明为禁用。
- writerIdleTimeSecond：指定写超时时间，指定 0 表明为禁用。
- allIdleTimeSecond：在指定读写超时时间，指定 0 表明为禁用。

参考连接：https://juejin.cn/post/7016956245835776037



#### Q：TCP参数设置 ChannelOption参数

**SO_KEEPALIVE**：是否使用TCP的心跳机制。
当设置该选项以后，如果在两小时内没有数据的通信时，TCP会自动发送一个活动探测数据报文。
建议：心跳机制由应用层自己实现；

**SO_BACKLOG**：此为TCP传输选项，表示服务端接收连接的队列长度，如果队列已满，客户端连接将被拒绝。

> 服务端在处理客户端新连接请求时（三次握手）是顺序处理的，所以同一时间只能处理一个客户端连接，多个客户端到来的时候，服务端将不能处理的客户端连接请求放在队列中等待处理，队列的大小通过SO_BACKLOG指定。





策略：BIO、NIO、AIO三者区别

1、TCP 的粘包的概念是对的吗



2、简述AIO、BIO、NIO的具体使用、区别及原理



3、BIO，NIO，AIO的痛点，怎么优化?



4、IO/NIO/AIO区别？介绍Reactor，介绍Proactor？

为什么BIO比NIO性能差？简单讲讲区别？

假设有100个连接，采用NIO的方式要服务端要分配几个线程，采用BIO的方式呢？

为啥要用异步IO不用多线程，不是一样可以加速吗？



5、说说你对Netty的认识？



6、NIO中Channel的作用



7、NIO的设计架构？JDK中NIO有哪些重要组件？



8、为什么选用Netty来做通信框架？还知道其他网络通信框架？



9、Netty怎么实现高性能的？Netty高性能主要依赖了哪些特性？Netty为什么快（基于NIO+零拷贝）Netty为啥效率高（零拷贝，线程模型）



10、netty bytebuf工作原理，和NIO里buffer区别？



11、除了Netty还知道哪些网络传输框架吗？



12、为什么大多数rpc框架都用netty（聊了下Netty的特点）？你为什么会用到Netty?



13、同步、异步调用方式的具体实现



14、netty使用场景



15、netty的线程模型



16、RPC过程网络上发生了什么



17、RPC多个请求是在一个连接完成的吗



18、Netty服务调用如何变成同步的？（不知道）（回答netty中的Reactor模型）

Netty异步编程怎么做的？



19、基于Netty实现通信，使用了哪些TCP优化参数？

你说网络通信使用的Netty，你都通过那些设置对Netty进行过调优（我表示Netty的bootstrap的option设置基本都是模仿Netty官方案例搞的，然后他问了我backlog是什么意思）



20、tcp粘包

粘包半包怎么解决的（LineBased和LengthBased,我是用的是LineBased）

为什么要使用LineBased，怎么分割的（/r/n，当时没有考虑太多，觉得这个比较简单）



21、Netty解决粘包的几种方式

Netty拆包粘包的实质，Netty线程池中的线程建立连接之后，这条连接是不是始终于这个请求，对于Netty来说是不是只占用服务端的一个套接字，了解zero copy嘛

项目中如何解决粘包、拆包的问题（基于字符或者基于长度）

你这个报文传输的时候会不会遇到报文粘连的情况？如何解决？



22、Netty底层原理



23、Netty中的select过程



24、零拷贝讲讲（mmap优化，sendfile）



25、Netty的两个线程池，为什么两个，有什么区别，具体说来。Netty初始化的时候需要初始化两个线程池，你能简单说一说吗？



26、怎么实现保持长连接的（Netty保证的，应该是使用了TCP的长连接特性）



27、如何实现心跳保持（IDLE编解码器监听事件）



28、多少个线程，为什么这么设置？（netty自带的，默认CPU*2）



### 四、负载均衡

负载均衡其实就是任务分发，使得任务均摊到各个节点，避免单点失效场景。

负载均衡算法常见的分类有：软件负载均衡、硬件负载均衡和DNS负载均衡。

一般来说DNS是地理级别的，硬件负载均衡是集群级别的，软件负载均衡是机器级别的。

- 随机：按随机数来分配。


- 轮询：按请求时间逐一分配到不同服务器。


- ip_hash：每个请求按访问IP的hash结果来分配，每个访客固定访问一个后端服务器，可以解决动态网页中存在的session共享问题。

- 加权轮询法：不同的后端服务器可能机器的配置和当前系统的负载并不相同，因此它们的抗压能力也不相同。给配置高、负载低的机器配置更高的权重，让其处理更多的请；而配置低、负载高的机器，给其分配较低的权重，降低其系统负载，加权轮询能很好地处理这一问题，并将请求顺序且按照权重分配到后端

- 最小连接数法：根据后端服务器当前的连接情况，动态地选取其中当前积压连接数最少的一台服务器来处理当前的请求，尽可能地提高后端服务的利用效率


> 正向代理其实是客户端的代理，帮助客户端访问其无法访问的服务器资源。反向代理则是服务器的代理，帮助服务器做负载均衡，安全防护等

#### Q：Dubbo的四种负载均衡策略

（1）RandomLoadBalance:随机负载均衡。按权重设置随机概率。是Dubbo的**默认**负载均衡策略(Dubbo 中的随机负载是按照权重设置随机概率)。

（2）RoundRobinLoadBalance:轮询负载均衡。轮询选择一个(Dubbo中有权重的概念，按公约后的权重设置轮询比率)。

> 问题：存在慢的提供者请求的问题，比如：第二胎机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上

（3）LeastActiveLoadBalance:最少活跃调用数，活跃数最小的机器，相同活跃数的随机。活跃数指调用前后计数差。

> 好处：使慢的 Provider 收到更少请求，因为越慢的 Provider 的调用前后计数差会越大。

> 举个例子.每个服务有一个活跃计数器那么我们假如有A,B两个提供者.计数初始均为0当A提供者开始处理请求,该计数+1,此时A还没处理完,当处理完后则计数-1而B请求接收到请求处理得很快.B处理完后A还没处理完,所以此时A,B的计数为1,0那么当有新的请求来的时候,就会选择B提供者(B的活跃计数比A小)这就是文档说的,使慢的提供者收到更少请求

（4）ConsistentHashLoadBalance:一致性哈希负载均衡。一致性hash：添加删除机器前后映射关系一致，当然，不是严格一致。实现的关键是环形Hash空间。将数据和机器都hash到环上，数据映射到顺时针离自己最近的机器中。

> 好处：当某一台提供者挂时，原本该发往该提供者的请求，基于虚拟节点，平摊到其他提供者，不会引起剧烈变动



### 五、RPC和HTTP

#### 问：HTTP 和 RPC 的关系? RPC 和 HTTP 的区别?为什么要用 RPC？

首先HTTP它是一种应用层通信协议，而RPC 远程过程调用，它是一个方案，指允许像调用本地服务一样调用远程服务。有很多的可选实现方案。两者不是一个并行的概念。

成熟的RPC库相对HTTP，还封装了“服务发现”，"负载均衡"，“熔断降级”一类面向服务的高级特性。如果在HTTP封装一层服务发现和函数代理调用，那它就已经可以做一个RPC框架。

之所以，会有HTTP实现RPC框架，是因为现在大部分的系统都是给浏览器使用的，而且HTTP2也有了很大改进了，grpc就是使用HTTP2作为传输协议。

因为良好的rpc调用是面向服务的封装，针对服务的可用性和效率等都做了优化。单纯使用http调用则缺少了这些特性。



#### Q：RPC 和 HTTP的对比？

首先，RPC是远程调用，通常包括传输协议和序列化协议。

1、传输协议：

　　RPC：基于HTTP协议，TCP协议

　　HTTP：基于HTTP协议

2、传输效率：

　　RPC：(1)使用自定义的TCP协议，请求报文体积更小，

 (2)使用HTTP2协议，也可以很好的减小报文体积，提高传输效率

　　HTTP：(1)基于http1.1的协议，请求中会包含很多无用的内容，

 (2)基于HTTP2.0，那么简单的封装下可以作为一个RPC来使用，这时标准的RPC框架更多的是服务治理。

3、性能消耗：

　　RPC：可以基于thrift实现高效的二进制传输

　　HTTP：大部分是基于JSON实现的，字节大小和序列化耗时都比thrift要更消耗性能

4、负载均衡：

　　RPC：基本自带了负载均衡策略

　　HTTP：需要配置Nginx、HAProxy配置

5、服务治理：（下游服务新增，重启，下线时如何不影响上游调用者）

　　RPC：能做到自动通知，不影响上游

　　HTTP：需要事先通知，如修改NGINX配置。



#### Q：为什么spring cloud用的是http

HTTP Restful本身轻量，易用，适用性强，可以很容易的跨语言，跨平台，或者与已有系统交互，

目前很多大型项目多语言共存，http是最通用的协议，可以很好地解决跨语言跨平台兼容性









# 项目相关

## RPC项目

1、NIO 和 Netty的技术选型

- 开发出一个高质量的NIO程序不是一件简单的事情，除去NIO固有的复杂性，作为一个NIO服务端，需要能够处理客户端重复接入、客户端安全认证、消息的编解码、半包读写等情况。而如果选择Netty的话，这些情况都有一个稳定的解决方案。
- 其次，同样一个简单的两点网络通信，Netty只需要30行不到的业务逻辑代码，即完成了NIO服务端的开发，相比于JDK NIO原生库类的服务端，代码量大大减少，难度也降低很多。









#### OPPO面试































